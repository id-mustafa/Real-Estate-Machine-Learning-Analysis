{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FIRST SECTION: CATEGORIZING CITIES AS HOT, MEDIUM, OR COLD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 416,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Key for changing state names to their abbreviations\n",
    "us_state_abbrev = {\n",
    "    'Alabama': 'AL', 'Alaska': 'AK', 'Arizona': 'AZ', 'Arkansas': 'AR', 'California': 'CA',\n",
    "    'Colorado': 'CO', 'Connecticut': 'CT', 'Delaware': 'DE', 'Florida': 'FL', 'Georgia': 'GA',\n",
    "    'Hawaii': 'HI', 'Idaho': 'ID', 'Illinois': 'IL', 'Indiana': 'IN', 'Iowa': 'IA', 'Kansas': 'KS',\n",
    "    'Kentucky': 'KY', 'Louisiana': 'LA', 'Maine': 'ME', 'Maryland': 'MD', 'Massachusetts': 'MA',\n",
    "    'Michigan': 'MI', 'Minnesota': 'MN', 'Mississippi': 'MS', 'Missouri': 'MO', 'Montana': 'MT',\n",
    "    'Nebraska': 'NE', 'Nevada': 'NV', 'New Hampshire': 'NH', 'New Jersey': 'NJ', 'New Mexico': 'NM',\n",
    "    'New York': 'NY', 'North Carolina': 'NC', 'North Dakota': 'ND', 'Ohio': 'OH', 'Oklahoma': 'OK',\n",
    "    'Oregon': 'OR', 'Pennsylvania': 'PA', 'Rhode Island': 'RI', 'South Carolina': 'SC',\n",
    "    'South Dakota': 'SD', 'Tennessee': 'TN', 'Texas': 'TX', 'Utah': 'UT', 'Vermont': 'VT',\n",
    "    'Virginia': 'VA', 'Washington': 'WA', 'West Virginia': 'WV', 'Wisconsin': 'WI', 'Wyoming': 'WY',\n",
    "    'Puerto Rico': 'PR', 'Virgin Islands': 'VI', 'District of Columbia':'DI', 'New Brunswick': 'NB',\n",
    "    'Guam': 'GU'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FIRST SECTION: CLASSIFYING TEMPERTATURES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 417,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>City</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BIRMINGHAM,AL</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>HUNTSVILLE,AL</td>\n",
       "      <td>Medium</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>MOBILE,AL</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MONTGOMERY,AL</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>ANCHORAGE,AK</td>\n",
       "      <td>Cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>POHNPEI- CAROLINE IS.,PC</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>CHUUK- E. CAROLINE IS.,PC</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>YAP- W CAROLINE IS.,PC</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>SAN JUAN,PR</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>263</th>\n",
       "      <td>CITY</td>\n",
       "      <td>Cold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>264 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                          City Temperature\n",
       "0                BIRMINGHAM,AL         Hot\n",
       "1                HUNTSVILLE,AL      Medium\n",
       "2                    MOBILE,AL         Hot\n",
       "3                MONTGOMERY,AL         Hot\n",
       "4                 ANCHORAGE,AK        Cold\n",
       "..                         ...         ...\n",
       "259   POHNPEI- CAROLINE IS.,PC         Hot\n",
       "260  CHUUK- E. CAROLINE IS.,PC         Hot\n",
       "261     YAP- W CAROLINE IS.,PC         Hot\n",
       "262                SAN JUAN,PR         Hot\n",
       "263                       CITY        Cold\n",
       "\n",
       "[264 rows x 2 columns]"
      ]
     },
     "execution_count": 417,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Read data\n",
    "tdf = pd.read_csv(\"citytemperatures.csv\")\n",
    "\n",
    "# Rename CITY to City\n",
    "tdf = tdf.rename(columns={\"CITY\": \"City\"})\n",
    "\n",
    "# Convert month columns (2 to 13) to numeric\n",
    "tdf.iloc[:, 2:14] = tdf.iloc[:, 2:14].apply(pd.to_numeric, errors=\"coerce\")\n",
    "\n",
    "# Calculate the average temperature across months\n",
    "tdf[\"Temperature\"] = tdf.iloc[:, 2:14].mean(axis=1)\n",
    "\n",
    "# Compute the average temperature across multiple instances of the same city if there are multiple inputs (multiple rows for each city)\n",
    "tdf_avg_temp = tdf.groupby('City', as_index=False)['Temperature'].mean()\n",
    "\n",
    "# Drop duplicates\n",
    "tdf = tdf.drop_duplicates(subset=['City'], keep='first')\n",
    "\n",
    "# Calculate quantiles for splitting data into 3 parts\n",
    "quantiles = tdf[\"Temperature\"].quantile([0.33, 0.66])\n",
    "\n",
    "# Function to classify temperature based on quantiles\n",
    "def classify_temp(temp):\n",
    "    if temp >= quantiles[0.66]:\n",
    "        return \"Hot\"\n",
    "    elif temp >= quantiles[0.33]:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Cold\"\n",
    "\n",
    "# Apply classification\n",
    "tdf[\"Category\"] = tdf[\"Temperature\"].apply(classify_temp)\n",
    "\n",
    "# Select only 'City' and 'Category' columns for final (tdf final = tdff)\n",
    "tdff = tdf[[\"City\", \"Category\"]]\n",
    "\n",
    "# Rename Category to Temperature\n",
    "tdff = tdff.rename(columns={\"Category\": \"Temperature\"})\n",
    "\n",
    "tdff"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECOND SECTION: COMBINING THIS WITH HOUSING DATA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 418,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Bedroom</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Area</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>MarketEstimate</th>\n",
       "      <th>RentEstimate</th>\n",
       "      <th>Price</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AL</td>\n",
       "      <td>saraland,al</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>0.38050</td>\n",
       "      <td>240600.0</td>\n",
       "      <td>1599.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AL</td>\n",
       "      <td>southside,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>0.67034</td>\n",
       "      <td>186700.0</td>\n",
       "      <td>1381.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AL</td>\n",
       "      <td>robertsdale,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>3.20000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AL</td>\n",
       "      <td>gulf shores,al</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>342500.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AL</td>\n",
       "      <td>chelsea,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>336200.0</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>335000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249097</th>\n",
       "      <td>WA</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.33000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>359900.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249098</th>\n",
       "      <td>WA</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1616.0</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249099</th>\n",
       "      <td>WA</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249100</th>\n",
       "      <td>WA</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.09000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249101</th>\n",
       "      <td>WA</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3615.0</td>\n",
       "      <td>0.31000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2249102 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        State            City  Bedroom  Bathroom    Area  LotArea  \\\n",
       "0          AL     saraland,al      4.0       2.0  1614.0  0.38050   \n",
       "1          AL    southside,al      3.0       2.0  1474.0  0.67034   \n",
       "2          AL  robertsdale,al      3.0       2.0  1800.0  3.20000   \n",
       "3          AL  gulf shores,al      2.0       2.0  1250.0      NaN   \n",
       "4          AL      chelsea,al      3.0       3.0  2224.0  0.26000   \n",
       "...       ...             ...      ...       ...     ...      ...   \n",
       "2249097    WA     richland,wa      4.0       2.0  3600.0  0.33000   \n",
       "2249098    WA     richland,wa      3.0       2.0  1616.0  0.10000   \n",
       "2249099    WA     richland,wa      6.0       3.0  3200.0  0.50000   \n",
       "2249100    WA     richland,wa      2.0       1.0   933.0  0.09000   \n",
       "2249101    WA     richland,wa      5.0       3.0  3615.0  0.31000   \n",
       "\n",
       "         MarketEstimate  RentEstimate     Price Temperature  \n",
       "0              240600.0        1599.0  239900.0         NaN  \n",
       "1              186700.0        1381.0       1.0         NaN  \n",
       "2                   NaN           NaN  259900.0         NaN  \n",
       "3                   NaN           NaN  342500.0         NaN  \n",
       "4              336200.0        1932.0  335000.0         NaN  \n",
       "...                 ...           ...       ...         ...  \n",
       "2249097             NaN           NaN  359900.0         NaN  \n",
       "2249098             NaN           NaN  350000.0         NaN  \n",
       "2249099             NaN           NaN  440000.0         NaN  \n",
       "2249100             NaN           NaN  179900.0         NaN  \n",
       "2249101             NaN           NaN  580000.0         NaN  \n",
       "\n",
       "[2249102 rows x 10 columns]"
      ]
     },
     "execution_count": 418,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read first houying data\n",
    "hdf = pd.read_csv(\"housingdata1.csv\")\n",
    "\n",
    "# Drop rows where 'City' or 'State' is NaN\n",
    "hdf = hdf.dropna(subset=['City', 'State'])\n",
    "\n",
    "#revised housing dataframe\n",
    "hdf = hdf[[\"State\", \"City\", \"Bedroom\", \"Bathroom\", \"Area\", \"LotArea\", \"MarketEstimate\", \"RentEstimate\", \"Price\"]]\n",
    "\n",
    "# Read second housing dataset\n",
    "hdf2 = pd.read_csv(\"housingdata2.csv\")\n",
    "\n",
    "# Simplify second dataset\n",
    "hdf2 = hdf2[[\"state\", \"city\", \"bed\", \"bath\", \"house_size\", \"acre_lot\", \"price\"]]\n",
    "\n",
    "# Rename columns in second dataset to match the first one\n",
    "hdf2.columns = ['State', 'City', 'Bedroom', 'Bathroom', 'Area', 'LotArea', 'Price']\n",
    "\n",
    "# Drop rows where 'City' or 'State' is NaN in hd2\n",
    "hdf2 = hdf2.dropna(subset=['City', 'State'])\n",
    "\n",
    "# Rename states to their abbreviations\n",
    "hdf2['State'] = hdf2['State'].map(us_state_abbrev).fillna(hdf2['State'])\n",
    "\n",
    "# Filter out all non-abbreviated places\n",
    "hdf2 = hdf2[hdf2['State'].str.match(r'^[A-Z]{2}$', na=False)]\n",
    "\n",
    "# Combining the two hdfs\n",
    "hdf = pd.concat([hdf, hdf2], ignore_index=True)\n",
    "\n",
    "#reformat housing data city to CITY,SI  (SI = STATE INITIAL)\n",
    "hdf[\"City\"] = hdf[\"City\"] + \",\" + hdf[\"State\"]\n",
    "\n",
    "# Ensure key column (city) is of the same type and normalize it so they are all in the same format\n",
    "hdf.loc[:, \"City\"] = hdf[\"City\"].str.strip().str.lower()\n",
    "tdff.loc[:, \"City\"] = tdff[\"City\"].str.strip().str.lower()\n",
    "\n",
    "# Merge datasets on 'city'\n",
    "merged_df = hdf.merge(tdff[[\"City\", \"Temperature\"]], on=\"City\", how=\"left\")\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 2.5: IMPORTING IN OTHER STATEWISE TEMPERATURE DATA TO REPLACE NAN VALUES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 419,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Bedroom</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Area</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>MarketEstimate</th>\n",
       "      <th>RentEstimate</th>\n",
       "      <th>Price</th>\n",
       "      <th>Temperature</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>al</td>\n",
       "      <td>saraland,al</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>0.38050</td>\n",
       "      <td>240600.0</td>\n",
       "      <td>1599.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>al</td>\n",
       "      <td>southside,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>0.67034</td>\n",
       "      <td>186700.0</td>\n",
       "      <td>1381.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>al</td>\n",
       "      <td>robertsdale,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>3.20000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>al</td>\n",
       "      <td>gulf shores,al</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>342500.0</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>al</td>\n",
       "      <td>chelsea,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>336200.0</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>335000.0</td>\n",
       "      <td>Hot</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249097</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.33000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>359900.0</td>\n",
       "      <td>Cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249098</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1616.0</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>Cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249099</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>Cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249100</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.09000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>Cold</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249101</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3615.0</td>\n",
       "      <td>0.31000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>Cold</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2249102 rows Ã— 10 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        State            City  Bedroom  Bathroom    Area  LotArea  \\\n",
       "0          al     saraland,al      4.0       2.0  1614.0  0.38050   \n",
       "1          al    southside,al      3.0       2.0  1474.0  0.67034   \n",
       "2          al  robertsdale,al      3.0       2.0  1800.0  3.20000   \n",
       "3          al  gulf shores,al      2.0       2.0  1250.0      NaN   \n",
       "4          al      chelsea,al      3.0       3.0  2224.0  0.26000   \n",
       "...       ...             ...      ...       ...     ...      ...   \n",
       "2249097    wa     richland,wa      4.0       2.0  3600.0  0.33000   \n",
       "2249098    wa     richland,wa      3.0       2.0  1616.0  0.10000   \n",
       "2249099    wa     richland,wa      6.0       3.0  3200.0  0.50000   \n",
       "2249100    wa     richland,wa      2.0       1.0   933.0  0.09000   \n",
       "2249101    wa     richland,wa      5.0       3.0  3615.0  0.31000   \n",
       "\n",
       "         MarketEstimate  RentEstimate     Price Temperature  \n",
       "0              240600.0        1599.0  239900.0         Hot  \n",
       "1              186700.0        1381.0       1.0         Hot  \n",
       "2                   NaN           NaN  259900.0         Hot  \n",
       "3                   NaN           NaN  342500.0         Hot  \n",
       "4              336200.0        1932.0  335000.0         Hot  \n",
       "...                 ...           ...       ...         ...  \n",
       "2249097             NaN           NaN  359900.0        Cold  \n",
       "2249098             NaN           NaN  350000.0        Cold  \n",
       "2249099             NaN           NaN  440000.0        Cold  \n",
       "2249100             NaN           NaN  179900.0        Cold  \n",
       "2249101             NaN           NaN  580000.0        Cold  \n",
       "\n",
       "[2249102 rows x 10 columns]"
      ]
     },
     "execution_count": 419,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read state temperature data\n",
    "stdf = pd.read_csv(\"averagestatetemperatures.csv\")[[\"state\", \"average_temp\"]]\n",
    "\n",
    "# Rename state to State\n",
    "stdf = stdf.rename(columns={\"state\": \"State\"})\n",
    "\n",
    "# Convert full state names to abbreviations\n",
    "stdf['State'] = stdf['State'].map(us_state_abbrev)\n",
    "\n",
    "# Compute the average temperature for each state if there are multiple rows with the same state\n",
    "stdf_avg_temp = stdf.groupby('State', as_index=False)['average_temp'].mean()\n",
    "\n",
    "# Drop duplicates\n",
    "stdf = stdf.drop_duplicates(subset=['State'], keep='first')\n",
    "\n",
    "# Calculate quantiles for splitting data into 3 parts\n",
    "quantiles = stdf[\"average_temp\"].quantile([0.33, 0.66])\n",
    "\n",
    "# Function to classify temperature based on quantiles\n",
    "def classify_temp(temp):\n",
    "    if temp >= quantiles[0.66]:\n",
    "        return \"Hot\"\n",
    "    elif temp >= quantiles[0.33]:\n",
    "        return \"Medium\"\n",
    "    else:\n",
    "        return \"Cold\"\n",
    "\n",
    "# Apply classification\n",
    "stdf[\"Temperature\"] = stdf[\"average_temp\"].apply(classify_temp)\n",
    "\n",
    "# Remove avg temp\n",
    "stdf = stdf.drop(columns=['average_temp'])\n",
    "\n",
    "# Manually input values for states which were not in the state dataset\n",
    "stdf.loc[len(stdf)] = ['ak', 'Cold']\n",
    "stdf.loc[len(stdf)] = ['pr', 'Hot']\n",
    "stdf.loc[len(stdf)] = ['di', 'Medium']\n",
    "stdf.loc[len(stdf)] = ['vi', 'Hot']\n",
    "stdf.loc[len(stdf)] = ['nb', 'Cold']\n",
    "stdf.loc[len(stdf)] = ['hi', 'Hot']\n",
    "stdf.loc[len(stdf)] = ['gu', 'Hot']\n",
    "\n",
    "# Ensure data in State column is all in the same format\n",
    "stdf.loc[:, \"State\"] = stdf[\"State\"].str.strip().str.lower()\n",
    "merged_df.loc[:, \"State\"] = merged_df[\"State\"].str.strip().str.lower()\n",
    "\n",
    "# Merge state temps into merged df, keeping BOTH temperature columns rather than combining state temps into the merged_df temps. This is because we only want to update\n",
    "# merged_df temps if there was a NaN entry there \n",
    "merged_df = merged_df.merge(stdf, on='State', how='left', suffixes=('_old', '_new'))\n",
    "\n",
    "# Fill NaN values in the original Temperature column with values from the second dataset\n",
    "merged_df['Temperature_old'] = merged_df['Temperature_old'].fillna(merged_df['Temperature_new'])\n",
    "\n",
    "merged_df = merged_df.drop(columns=['Temperature_new'])\n",
    "\n",
    "merged_df = merged_df.rename(columns={\"Temperature_old\": \"Temperature\"})\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# THIRD SECTION: DOING THE SAME THING WITH QOL RATING"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 420,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/vr/57kjgf610p972_0992rx8k700000gn/T/ipykernel_20109/516748852.py:2: DtypeWarning: Columns (0,1,2,3,4,5,6,9,11,12,13,14,15,18,20,21,22,23,24,25,26,27,28,29,30,31,32,33,37,38,39,40,44,45,46,47) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  qoldf = pd.read_csv(\"qolcitydata.csv\", encoding=\"ISO-8859-1\")\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Bedroom</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Area</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>MarketEstimate</th>\n",
       "      <th>RentEstimate</th>\n",
       "      <th>Price</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>2016 Crime Rate</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>AQI%Good</th>\n",
       "      <th>WaterQualityVPV</th>\n",
       "      <th>%CvgCityPark</th>\n",
       "      <th>Cost of Living</th>\n",
       "      <th>2022 Median Income</th>\n",
       "      <th>AVG C2I</th>\n",
       "      <th>Diversity Rank (Race)</th>\n",
       "      <th>Diversity Rank (Gender)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>al</td>\n",
       "      <td>saraland,al</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>0.38050</td>\n",
       "      <td>240600.0</td>\n",
       "      <td>1599.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>47/1000</td>\n",
       "      <td>3.35%</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$71,947.38</td>\n",
       "      <td>$62,409.46</td>\n",
       "      <td>115.28%</td>\n",
       "      <td>26459.0</td>\n",
       "      <td>63210.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>al</td>\n",
       "      <td>southside,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>0.67034</td>\n",
       "      <td>186700.0</td>\n",
       "      <td>1381.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>43/1000</td>\n",
       "      <td>3.12%</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$67,812.73</td>\n",
       "      <td>$58,943.92</td>\n",
       "      <td>115.05%</td>\n",
       "      <td>69642.0</td>\n",
       "      <td>79134.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>al</td>\n",
       "      <td>robertsdale,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>3.20000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>18/1000</td>\n",
       "      <td>2.41%</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$79,155.41</td>\n",
       "      <td>$77,884.76</td>\n",
       "      <td>101.63%</td>\n",
       "      <td>29479.0</td>\n",
       "      <td>36363.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>al</td>\n",
       "      <td>gulf shores,al</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>342500.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>18/1000</td>\n",
       "      <td>2.41%</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$79,155.41</td>\n",
       "      <td>$77,884.76</td>\n",
       "      <td>101.63%</td>\n",
       "      <td>56013.0</td>\n",
       "      <td>31948.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>al</td>\n",
       "      <td>chelsea,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>336200.0</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>335000.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>16/1000</td>\n",
       "      <td>1.87%</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$85,691.03</td>\n",
       "      <td>$98,419.23</td>\n",
       "      <td>87.07%</td>\n",
       "      <td>44179.0</td>\n",
       "      <td>41526.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249097</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.33000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>359900.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>23/1000</td>\n",
       "      <td>5.31%</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249098</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1616.0</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>23/1000</td>\n",
       "      <td>5.31%</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249099</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>23/1000</td>\n",
       "      <td>5.31%</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249100</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.09000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>23/1000</td>\n",
       "      <td>5.31%</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249101</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3615.0</td>\n",
       "      <td>0.31000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>23/1000</td>\n",
       "      <td>5.31%</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2249102 rows Ã— 20 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        State            City  Bedroom  Bathroom    Area  LotArea  \\\n",
       "0          al     saraland,al      4.0       2.0  1614.0  0.38050   \n",
       "1          al    southside,al      3.0       2.0  1474.0  0.67034   \n",
       "2          al  robertsdale,al      3.0       2.0  1800.0  3.20000   \n",
       "3          al  gulf shores,al      2.0       2.0  1250.0      NaN   \n",
       "4          al      chelsea,al      3.0       3.0  2224.0  0.26000   \n",
       "...       ...             ...      ...       ...     ...      ...   \n",
       "2249097    wa     richland,wa      4.0       2.0  3600.0  0.33000   \n",
       "2249098    wa     richland,wa      3.0       2.0  1616.0  0.10000   \n",
       "2249099    wa     richland,wa      6.0       3.0  3200.0  0.50000   \n",
       "2249100    wa     richland,wa      2.0       1.0   933.0  0.09000   \n",
       "2249101    wa     richland,wa      5.0       3.0  3615.0  0.31000   \n",
       "\n",
       "         MarketEstimate  RentEstimate     Price Temperature 2016 Crime Rate  \\\n",
       "0              240600.0        1599.0  239900.0         Hot         47/1000   \n",
       "1              186700.0        1381.0       1.0         Hot         43/1000   \n",
       "2                   NaN           NaN  259900.0         Hot         18/1000   \n",
       "3                   NaN           NaN  342500.0         Hot         18/1000   \n",
       "4              336200.0        1932.0  335000.0         Hot         16/1000   \n",
       "...                 ...           ...       ...         ...             ...   \n",
       "2249097             NaN           NaN  359900.0        Cold         23/1000   \n",
       "2249098             NaN           NaN  350000.0        Cold         23/1000   \n",
       "2249099             NaN           NaN  440000.0        Cold         23/1000   \n",
       "2249100             NaN           NaN  179900.0        Cold         23/1000   \n",
       "2249101             NaN           NaN  580000.0        Cold         23/1000   \n",
       "\n",
       "        Unemployment AQI%Good  WaterQualityVPV %CvgCityPark Cost of Living  \\\n",
       "0              3.35%   80.94%              1.0           -1     $71,947.38   \n",
       "1              3.12%   80.94%              0.0           -1     $67,812.73   \n",
       "2              2.41%   80.94%              1.0           -1     $79,155.41   \n",
       "3              2.41%   80.94%              1.0           -1     $79,155.41   \n",
       "4              1.87%   80.94%             -1.0           -1     $85,691.03   \n",
       "...              ...      ...              ...          ...            ...   \n",
       "2249097        5.31%   92.89%              0.0           -1     $72,571.13   \n",
       "2249098        5.31%   92.89%              0.0           -1     $72,571.13   \n",
       "2249099        5.31%   92.89%              0.0           -1     $72,571.13   \n",
       "2249100        5.31%   92.89%              0.0           -1     $72,571.13   \n",
       "2249101        5.31%   92.89%              0.0           -1     $72,571.13   \n",
       "\n",
       "        2022 Median Income  AVG C2I  Diversity Rank (Race)  \\\n",
       "0               $62,409.46  115.28%                26459.0   \n",
       "1               $58,943.92  115.05%                69642.0   \n",
       "2               $77,884.76  101.63%                29479.0   \n",
       "3               $77,884.76  101.63%                56013.0   \n",
       "4               $98,419.23   87.07%                44179.0   \n",
       "...                    ...      ...                    ...   \n",
       "2249097         $83,393.68   87.02%                35964.0   \n",
       "2249098         $83,393.68   87.02%                35964.0   \n",
       "2249099         $83,393.68   87.02%                35964.0   \n",
       "2249100         $83,393.68   87.02%                35964.0   \n",
       "2249101         $83,393.68   87.02%                35964.0   \n",
       "\n",
       "         Diversity Rank (Gender)  \n",
       "0                        63210.0  \n",
       "1                        79134.0  \n",
       "2                        36363.0  \n",
       "3                        31948.0  \n",
       "4                        41526.0  \n",
       "...                          ...  \n",
       "2249097                  87680.0  \n",
       "2249098                  87680.0  \n",
       "2249099                  87680.0  \n",
       "2249100                  87680.0  \n",
       "2249101                  87680.0  \n",
       "\n",
       "[2249102 rows x 20 columns]"
      ]
     },
     "execution_count": 420,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read qol data\n",
    "qoldf = pd.read_csv(\"qolcitydata.csv\", encoding=\"ISO-8859-1\")\n",
    "\n",
    "# Exract only the columns we want\n",
    "qoldf = qoldf[[\"LCITY\", \"LSTATE\", \"2016 Crime Rate\", \"Unemployment\", \"AQI%Good\", \"WaterQualityVPV\", \"%CvgCityPark\", \"Cost of Living\", \"2022 Median Income\", \"AVG C2I\", \"Diversity Rank (Race)\", \"Diversity Rank (Gender)\"]]\n",
    "\n",
    "# Correct format of qoldf cities to match that of our merged_df\n",
    "qoldf.loc[:, \"LCITY\"] = qoldf[\"LCITY\"].str.strip().str.lower()\n",
    "qoldf.loc[:, \"LSTATE\"] = qoldf[\"LSTATE\"].str.strip().str.lower()\n",
    "qoldf[\"LCITY\"] = qoldf[\"LCITY\"] + \",\" + qoldf[\"LSTATE\"]\n",
    "\n",
    "# Drop rows where 'City' or 'State' is NaN\n",
    "qoldf = qoldf.dropna(subset=['LCITY', 'LSTATE'])\n",
    "\n",
    "# Drop duplicate entries of a city\n",
    "qoldf = qoldf.drop_duplicates(subset=\"LCITY\", keep=\"first\")\n",
    "\n",
    "# Rename LCITY to City for merging\n",
    "qoldf = qoldf.rename(columns={\"LCITY\": \"City\"})\n",
    "\n",
    "merged_df = merged_df.merge(qoldf[[\"City\", \"2016 Crime Rate\", \"Unemployment\", \"AQI%Good\", \"WaterQualityVPV\", \"%CvgCityPark\", \"Cost of Living\", \"2022 Median Income\", \"AVG C2I\", \"Diversity Rank (Race)\", \"Diversity Rank (Gender)\"]], on=\"City\", how=\"left\")\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FOURTH SECTION: FINALLY, ADD IN MEAN INCOME FOR EACH CITY"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 421,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "306795\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Bedroom</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Area</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>MarketEstimate</th>\n",
       "      <th>RentEstimate</th>\n",
       "      <th>Price</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>...</th>\n",
       "      <th>AQI%Good</th>\n",
       "      <th>WaterQualityVPV</th>\n",
       "      <th>%CvgCityPark</th>\n",
       "      <th>Cost of Living</th>\n",
       "      <th>2022 Median Income</th>\n",
       "      <th>AVG C2I</th>\n",
       "      <th>Diversity Rank (Race)</th>\n",
       "      <th>Diversity Rank (Gender)</th>\n",
       "      <th>MeanIncome</th>\n",
       "      <th>MedianIncome</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>al</td>\n",
       "      <td>saraland,al</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>0.38050</td>\n",
       "      <td>240600.0</td>\n",
       "      <td>1599.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>...</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$71,947.38</td>\n",
       "      <td>$62,409.46</td>\n",
       "      <td>115.28%</td>\n",
       "      <td>26459.0</td>\n",
       "      <td>63210.0</td>\n",
       "      <td>43803.0</td>\n",
       "      <td>300000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>al</td>\n",
       "      <td>southside,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>0.67034</td>\n",
       "      <td>186700.0</td>\n",
       "      <td>1381.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>...</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$67,812.73</td>\n",
       "      <td>$58,943.92</td>\n",
       "      <td>115.05%</td>\n",
       "      <td>69642.0</td>\n",
       "      <td>79134.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>al</td>\n",
       "      <td>robertsdale,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>3.20000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>...</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$79,155.41</td>\n",
       "      <td>$77,884.76</td>\n",
       "      <td>101.63%</td>\n",
       "      <td>29479.0</td>\n",
       "      <td>36363.0</td>\n",
       "      <td>67084.0</td>\n",
       "      <td>172640.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>al</td>\n",
       "      <td>gulf shores,al</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>342500.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>...</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$79,155.41</td>\n",
       "      <td>$77,884.76</td>\n",
       "      <td>101.63%</td>\n",
       "      <td>56013.0</td>\n",
       "      <td>31948.0</td>\n",
       "      <td>65583.0</td>\n",
       "      <td>300000.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>al</td>\n",
       "      <td>chelsea,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>336200.0</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>335000.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>...</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$85,691.03</td>\n",
       "      <td>$98,419.23</td>\n",
       "      <td>87.07%</td>\n",
       "      <td>44179.0</td>\n",
       "      <td>41526.0</td>\n",
       "      <td>78399.0</td>\n",
       "      <td>71839.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249097</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.33000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>359900.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>...</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "      <td>115261.0</td>\n",
       "      <td>235998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249098</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1616.0</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>...</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "      <td>115261.0</td>\n",
       "      <td>235998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249099</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>...</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "      <td>115261.0</td>\n",
       "      <td>235998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249100</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.09000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>...</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "      <td>115261.0</td>\n",
       "      <td>235998.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249101</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3615.0</td>\n",
       "      <td>0.31000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>...</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "      <td>115261.0</td>\n",
       "      <td>235998.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2249102 rows Ã— 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        State            City  Bedroom  Bathroom    Area  LotArea  \\\n",
       "0          al     saraland,al      4.0       2.0  1614.0  0.38050   \n",
       "1          al    southside,al      3.0       2.0  1474.0  0.67034   \n",
       "2          al  robertsdale,al      3.0       2.0  1800.0  3.20000   \n",
       "3          al  gulf shores,al      2.0       2.0  1250.0      NaN   \n",
       "4          al      chelsea,al      3.0       3.0  2224.0  0.26000   \n",
       "...       ...             ...      ...       ...     ...      ...   \n",
       "2249097    wa     richland,wa      4.0       2.0  3600.0  0.33000   \n",
       "2249098    wa     richland,wa      3.0       2.0  1616.0  0.10000   \n",
       "2249099    wa     richland,wa      6.0       3.0  3200.0  0.50000   \n",
       "2249100    wa     richland,wa      2.0       1.0   933.0  0.09000   \n",
       "2249101    wa     richland,wa      5.0       3.0  3615.0  0.31000   \n",
       "\n",
       "         MarketEstimate  RentEstimate     Price Temperature  ... AQI%Good  \\\n",
       "0              240600.0        1599.0  239900.0         Hot  ...   80.94%   \n",
       "1              186700.0        1381.0       1.0         Hot  ...   80.94%   \n",
       "2                   NaN           NaN  259900.0         Hot  ...   80.94%   \n",
       "3                   NaN           NaN  342500.0         Hot  ...   80.94%   \n",
       "4              336200.0        1932.0  335000.0         Hot  ...   80.94%   \n",
       "...                 ...           ...       ...         ...  ...      ...   \n",
       "2249097             NaN           NaN  359900.0        Cold  ...   92.89%   \n",
       "2249098             NaN           NaN  350000.0        Cold  ...   92.89%   \n",
       "2249099             NaN           NaN  440000.0        Cold  ...   92.89%   \n",
       "2249100             NaN           NaN  179900.0        Cold  ...   92.89%   \n",
       "2249101             NaN           NaN  580000.0        Cold  ...   92.89%   \n",
       "\n",
       "        WaterQualityVPV %CvgCityPark  Cost of Living 2022 Median Income  \\\n",
       "0                   1.0           -1      $71,947.38         $62,409.46   \n",
       "1                   0.0           -1      $67,812.73         $58,943.92   \n",
       "2                   1.0           -1      $79,155.41         $77,884.76   \n",
       "3                   1.0           -1      $79,155.41         $77,884.76   \n",
       "4                  -1.0           -1      $85,691.03         $98,419.23   \n",
       "...                 ...          ...             ...                ...   \n",
       "2249097             0.0           -1      $72,571.13         $83,393.68   \n",
       "2249098             0.0           -1      $72,571.13         $83,393.68   \n",
       "2249099             0.0           -1      $72,571.13         $83,393.68   \n",
       "2249100             0.0           -1      $72,571.13         $83,393.68   \n",
       "2249101             0.0           -1      $72,571.13         $83,393.68   \n",
       "\n",
       "         AVG C2I Diversity Rank (Race) Diversity Rank (Gender)  MeanIncome  \\\n",
       "0        115.28%               26459.0                 63210.0     43803.0   \n",
       "1        115.05%               69642.0                 79134.0         NaN   \n",
       "2        101.63%               29479.0                 36363.0     67084.0   \n",
       "3        101.63%               56013.0                 31948.0     65583.0   \n",
       "4         87.07%               44179.0                 41526.0     78399.0   \n",
       "...          ...                   ...                     ...         ...   \n",
       "2249097   87.02%               35964.0                 87680.0    115261.0   \n",
       "2249098   87.02%               35964.0                 87680.0    115261.0   \n",
       "2249099   87.02%               35964.0                 87680.0    115261.0   \n",
       "2249100   87.02%               35964.0                 87680.0    115261.0   \n",
       "2249101   87.02%               35964.0                 87680.0    115261.0   \n",
       "\n",
       "         MedianIncome  \n",
       "0            300000.0  \n",
       "1                 NaN  \n",
       "2            172640.0  \n",
       "3            300000.0  \n",
       "4             71839.0  \n",
       "...               ...  \n",
       "2249097      235998.0  \n",
       "2249098      235998.0  \n",
       "2249099      235998.0  \n",
       "2249100      235998.0  \n",
       "2249101      235998.0  \n",
       "\n",
       "[2249102 rows x 22 columns]"
      ]
     },
     "execution_count": 421,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read data\n",
    "idf = pd.read_csv(\"meancityincome.csv\", encoding=\"ISO-8859-1\")[[\"State_ab\", \"City\", \"Mean\", \"Median\"]]\n",
    "\n",
    "# Rename columns\n",
    "idf = idf.rename(columns={\"State_ab\": \"State\"})\n",
    "idf = idf.rename(columns={\"Mean\": \"MeanIncome\"})\n",
    "idf = idf.rename(columns={\"Median\": \"MedianIncome\"})\n",
    "\n",
    "# Correct format of idf cities to match that of our merged_df\n",
    "idf.loc[:, \"City\"] = idf[\"City\"].str.strip().str.lower()\n",
    "idf.loc[:, \"State\"] = idf[\"State\"].str.strip().str.lower()\n",
    "idf[\"City\"] = idf[\"City\"] + \",\" + idf[\"State\"]\n",
    "\n",
    "# Average out the mean and median entries for cases where one city has multiple entries\n",
    "idf = idf.groupby('City')[['MeanIncome', 'MedianIncome']].mean().round().reset_index()\n",
    "idf = idf.drop_duplicates(subset='City')\n",
    "\n",
    "# Merge with merged_df\n",
    "merged_df = merged_df.merge(idf[[\"City\", \"MeanIncome\", \"MedianIncome\"]], on=\"City\", how=\"left\")\n",
    "\n",
    "nan_count = merged_df['Cost of Living'].isna().sum()\n",
    "print(f\"{nan_count}\")\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SEC 4.5: ADDING POPULATIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Bedroom</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Area</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>MarketEstimate</th>\n",
       "      <th>RentEstimate</th>\n",
       "      <th>Price</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>...</th>\n",
       "      <th>WaterQualityVPV</th>\n",
       "      <th>%CvgCityPark</th>\n",
       "      <th>Cost of Living</th>\n",
       "      <th>2022 Median Income</th>\n",
       "      <th>AVG C2I</th>\n",
       "      <th>Diversity Rank (Race)</th>\n",
       "      <th>Diversity Rank (Gender)</th>\n",
       "      <th>MeanIncome</th>\n",
       "      <th>MedianIncome</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>al</td>\n",
       "      <td>saraland,al</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>0.38050</td>\n",
       "      <td>240600.0</td>\n",
       "      <td>1599.0</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$71,947.38</td>\n",
       "      <td>$62,409.46</td>\n",
       "      <td>115.28%</td>\n",
       "      <td>26459.0</td>\n",
       "      <td>63210.0</td>\n",
       "      <td>43803.0</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>16358.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>al</td>\n",
       "      <td>southside,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>0.67034</td>\n",
       "      <td>186700.0</td>\n",
       "      <td>1381.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$67,812.73</td>\n",
       "      <td>$58,943.92</td>\n",
       "      <td>115.05%</td>\n",
       "      <td>69642.0</td>\n",
       "      <td>79134.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9554.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>al</td>\n",
       "      <td>robertsdale,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>3.20000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$79,155.41</td>\n",
       "      <td>$77,884.76</td>\n",
       "      <td>101.63%</td>\n",
       "      <td>29479.0</td>\n",
       "      <td>36363.0</td>\n",
       "      <td>67084.0</td>\n",
       "      <td>172640.0</td>\n",
       "      <td>7189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>al</td>\n",
       "      <td>gulf shores,al</td>\n",
       "      <td>2.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1250.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>342500.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$79,155.41</td>\n",
       "      <td>$77,884.76</td>\n",
       "      <td>101.63%</td>\n",
       "      <td>56013.0</td>\n",
       "      <td>31948.0</td>\n",
       "      <td>65583.0</td>\n",
       "      <td>300000.0</td>\n",
       "      <td>16193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>al</td>\n",
       "      <td>chelsea,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>336200.0</td>\n",
       "      <td>1932.0</td>\n",
       "      <td>335000.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>...</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$85,691.03</td>\n",
       "      <td>$98,419.23</td>\n",
       "      <td>87.07%</td>\n",
       "      <td>44179.0</td>\n",
       "      <td>41526.0</td>\n",
       "      <td>78399.0</td>\n",
       "      <td>71839.0</td>\n",
       "      <td>16193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249310</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.33000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>359900.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "      <td>115261.0</td>\n",
       "      <td>235998.0</td>\n",
       "      <td>62821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249311</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1616.0</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "      <td>115261.0</td>\n",
       "      <td>235998.0</td>\n",
       "      <td>62821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249312</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "      <td>115261.0</td>\n",
       "      <td>235998.0</td>\n",
       "      <td>62821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249313</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.09000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "      <td>115261.0</td>\n",
       "      <td>235998.0</td>\n",
       "      <td>62821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249314</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3615.0</td>\n",
       "      <td>0.31000</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>-1</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>87.02%</td>\n",
       "      <td>35964.0</td>\n",
       "      <td>87680.0</td>\n",
       "      <td>115261.0</td>\n",
       "      <td>235998.0</td>\n",
       "      <td>62821.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2249315 rows Ã— 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        State            City  Bedroom  Bathroom    Area  LotArea  \\\n",
       "0          al     saraland,al      4.0       2.0  1614.0  0.38050   \n",
       "1          al    southside,al      3.0       2.0  1474.0  0.67034   \n",
       "2          al  robertsdale,al      3.0       2.0  1800.0  3.20000   \n",
       "3          al  gulf shores,al      2.0       2.0  1250.0      NaN   \n",
       "4          al      chelsea,al      3.0       3.0  2224.0  0.26000   \n",
       "...       ...             ...      ...       ...     ...      ...   \n",
       "2249310    wa     richland,wa      4.0       2.0  3600.0  0.33000   \n",
       "2249311    wa     richland,wa      3.0       2.0  1616.0  0.10000   \n",
       "2249312    wa     richland,wa      6.0       3.0  3200.0  0.50000   \n",
       "2249313    wa     richland,wa      2.0       1.0   933.0  0.09000   \n",
       "2249314    wa     richland,wa      5.0       3.0  3615.0  0.31000   \n",
       "\n",
       "         MarketEstimate  RentEstimate     Price Temperature  ...  \\\n",
       "0              240600.0        1599.0  239900.0         Hot  ...   \n",
       "1              186700.0        1381.0       1.0         Hot  ...   \n",
       "2                   NaN           NaN  259900.0         Hot  ...   \n",
       "3                   NaN           NaN  342500.0         Hot  ...   \n",
       "4              336200.0        1932.0  335000.0         Hot  ...   \n",
       "...                 ...           ...       ...         ...  ...   \n",
       "2249310             NaN           NaN  359900.0        Cold  ...   \n",
       "2249311             NaN           NaN  350000.0        Cold  ...   \n",
       "2249312             NaN           NaN  440000.0        Cold  ...   \n",
       "2249313             NaN           NaN  179900.0        Cold  ...   \n",
       "2249314             NaN           NaN  580000.0        Cold  ...   \n",
       "\n",
       "        WaterQualityVPV %CvgCityPark Cost of Living  2022 Median Income  \\\n",
       "0                   1.0           -1     $71,947.38          $62,409.46   \n",
       "1                   0.0           -1     $67,812.73          $58,943.92   \n",
       "2                   1.0           -1     $79,155.41          $77,884.76   \n",
       "3                   1.0           -1     $79,155.41          $77,884.76   \n",
       "4                  -1.0           -1     $85,691.03          $98,419.23   \n",
       "...                 ...          ...            ...                 ...   \n",
       "2249310             0.0           -1     $72,571.13          $83,393.68   \n",
       "2249311             0.0           -1     $72,571.13          $83,393.68   \n",
       "2249312             0.0           -1     $72,571.13          $83,393.68   \n",
       "2249313             0.0           -1     $72,571.13          $83,393.68   \n",
       "2249314             0.0           -1     $72,571.13          $83,393.68   \n",
       "\n",
       "         AVG C2I Diversity Rank (Race) Diversity Rank (Gender) MeanIncome  \\\n",
       "0        115.28%               26459.0                 63210.0    43803.0   \n",
       "1        115.05%               69642.0                 79134.0        NaN   \n",
       "2        101.63%               29479.0                 36363.0    67084.0   \n",
       "3        101.63%               56013.0                 31948.0    65583.0   \n",
       "4         87.07%               44179.0                 41526.0    78399.0   \n",
       "...          ...                   ...                     ...        ...   \n",
       "2249310   87.02%               35964.0                 87680.0   115261.0   \n",
       "2249311   87.02%               35964.0                 87680.0   115261.0   \n",
       "2249312   87.02%               35964.0                 87680.0   115261.0   \n",
       "2249313   87.02%               35964.0                 87680.0   115261.0   \n",
       "2249314   87.02%               35964.0                 87680.0   115261.0   \n",
       "\n",
       "         MedianIncome  Population  \n",
       "0            300000.0     16358.0  \n",
       "1                 NaN      9554.0  \n",
       "2            172640.0      7189.0  \n",
       "3            300000.0     16193.0  \n",
       "4             71839.0     16193.0  \n",
       "...               ...         ...  \n",
       "2249310      235998.0     62821.0  \n",
       "2249311      235998.0     62821.0  \n",
       "2249312      235998.0     62821.0  \n",
       "2249313      235998.0     62821.0  \n",
       "2249314      235998.0     62821.0  \n",
       "\n",
       "[2249315 rows x 23 columns]"
      ]
     },
     "execution_count": 422,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf = pd.read_csv('uscitypopulations.csv')[['CITY', 'STATE', '2022_POPULATION']]\n",
    "\n",
    "# Rename columns\n",
    "pdf.columns = ['City', 'State', 'Population']\n",
    "\n",
    "# Convert full state names to abbreviations\n",
    "pdf['State'] = pdf['State'].map(us_state_abbrev)\n",
    "\n",
    "# Correct formatting of cities and towns\n",
    "pdf['City'] = pdf['City'].str.replace(r'\\b( city| town)\\b', '', case=False, regex=True).str.strip()\n",
    "pdf.loc[:, \"City\"] = pdf[\"City\"].str.strip().str.lower()\n",
    "pdf.loc[:, \"State\"] = pdf[\"State\"].str.strip().str.lower()\n",
    "pdf[\"City\"] = pdf[\"City\"] + \",\" + pdf[\"State\"]\n",
    "\n",
    "pdf\n",
    "\n",
    "# Merge with merged_df\n",
    "merged_df = merged_df.merge(pdf[[\"City\", \"Population\"]], on=\"City\", how=\"left\")\n",
    "\n",
    "merged_df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop Nan Values as well as other unnecessary columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 423,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>State</th>\n",
       "      <th>City</th>\n",
       "      <th>Bedroom</th>\n",
       "      <th>Bathroom</th>\n",
       "      <th>Area</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Price</th>\n",
       "      <th>Temperature</th>\n",
       "      <th>2016 Crime Rate</th>\n",
       "      <th>Unemployment</th>\n",
       "      <th>AQI%Good</th>\n",
       "      <th>WaterQualityVPV</th>\n",
       "      <th>Cost of Living</th>\n",
       "      <th>2022 Median Income</th>\n",
       "      <th>Population</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>al</td>\n",
       "      <td>saraland,al</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1614.0</td>\n",
       "      <td>0.38050</td>\n",
       "      <td>239900.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>47/1000</td>\n",
       "      <td>3.35%</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>$71,947.38</td>\n",
       "      <td>$62,409.46</td>\n",
       "      <td>16358.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>al</td>\n",
       "      <td>southside,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1474.0</td>\n",
       "      <td>0.67034</td>\n",
       "      <td>1.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>43/1000</td>\n",
       "      <td>3.12%</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$67,812.73</td>\n",
       "      <td>$58,943.92</td>\n",
       "      <td>9554.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>al</td>\n",
       "      <td>robertsdale,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1800.0</td>\n",
       "      <td>3.20000</td>\n",
       "      <td>259900.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>18/1000</td>\n",
       "      <td>2.41%</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>$79,155.41</td>\n",
       "      <td>$77,884.76</td>\n",
       "      <td>7189.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>al</td>\n",
       "      <td>chelsea,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2224.0</td>\n",
       "      <td>0.26000</td>\n",
       "      <td>335000.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>16/1000</td>\n",
       "      <td>1.87%</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>-1.0</td>\n",
       "      <td>$85,691.03</td>\n",
       "      <td>$98,419.23</td>\n",
       "      <td>16193.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>al</td>\n",
       "      <td>montgomery,al</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1564.0</td>\n",
       "      <td>8712.00000</td>\n",
       "      <td>151000.0</td>\n",
       "      <td>Hot</td>\n",
       "      <td>47/1000</td>\n",
       "      <td>3.17%</td>\n",
       "      <td>80.94%</td>\n",
       "      <td>1.0</td>\n",
       "      <td>$74,899.78</td>\n",
       "      <td>$64,886.16</td>\n",
       "      <td>196986.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249310</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3600.0</td>\n",
       "      <td>0.33000</td>\n",
       "      <td>359900.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>23/1000</td>\n",
       "      <td>5.31%</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>62821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249311</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>3.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1616.0</td>\n",
       "      <td>0.10000</td>\n",
       "      <td>350000.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>23/1000</td>\n",
       "      <td>5.31%</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>62821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249312</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3200.0</td>\n",
       "      <td>0.50000</td>\n",
       "      <td>440000.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>23/1000</td>\n",
       "      <td>5.31%</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>62821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249313</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>933.0</td>\n",
       "      <td>0.09000</td>\n",
       "      <td>179900.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>23/1000</td>\n",
       "      <td>5.31%</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>62821.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2249314</th>\n",
       "      <td>wa</td>\n",
       "      <td>richland,wa</td>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3615.0</td>\n",
       "      <td>0.31000</td>\n",
       "      <td>580000.0</td>\n",
       "      <td>Cold</td>\n",
       "      <td>23/1000</td>\n",
       "      <td>5.31%</td>\n",
       "      <td>92.89%</td>\n",
       "      <td>0.0</td>\n",
       "      <td>$72,571.13</td>\n",
       "      <td>$83,393.68</td>\n",
       "      <td>62821.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>944616 rows Ã— 15 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "        State            City  Bedroom  Bathroom    Area     LotArea  \\\n",
       "0          al     saraland,al      4.0       2.0  1614.0     0.38050   \n",
       "1          al    southside,al      3.0       2.0  1474.0     0.67034   \n",
       "2          al  robertsdale,al      3.0       2.0  1800.0     3.20000   \n",
       "4          al      chelsea,al      3.0       3.0  2224.0     0.26000   \n",
       "6          al   montgomery,al      3.0       2.0  1564.0  8712.00000   \n",
       "...       ...             ...      ...       ...     ...         ...   \n",
       "2249310    wa     richland,wa      4.0       2.0  3600.0     0.33000   \n",
       "2249311    wa     richland,wa      3.0       2.0  1616.0     0.10000   \n",
       "2249312    wa     richland,wa      6.0       3.0  3200.0     0.50000   \n",
       "2249313    wa     richland,wa      2.0       1.0   933.0     0.09000   \n",
       "2249314    wa     richland,wa      5.0       3.0  3615.0     0.31000   \n",
       "\n",
       "            Price Temperature 2016 Crime Rate Unemployment AQI%Good  \\\n",
       "0        239900.0         Hot         47/1000        3.35%   80.94%   \n",
       "1             1.0         Hot         43/1000        3.12%   80.94%   \n",
       "2        259900.0         Hot         18/1000        2.41%   80.94%   \n",
       "4        335000.0         Hot         16/1000        1.87%   80.94%   \n",
       "6        151000.0         Hot         47/1000        3.17%   80.94%   \n",
       "...           ...         ...             ...          ...      ...   \n",
       "2249310  359900.0        Cold         23/1000        5.31%   92.89%   \n",
       "2249311  350000.0        Cold         23/1000        5.31%   92.89%   \n",
       "2249312  440000.0        Cold         23/1000        5.31%   92.89%   \n",
       "2249313  179900.0        Cold         23/1000        5.31%   92.89%   \n",
       "2249314  580000.0        Cold         23/1000        5.31%   92.89%   \n",
       "\n",
       "         WaterQualityVPV Cost of Living 2022 Median Income  Population  \n",
       "0                    1.0     $71,947.38         $62,409.46     16358.0  \n",
       "1                    0.0     $67,812.73         $58,943.92      9554.0  \n",
       "2                    1.0     $79,155.41         $77,884.76      7189.0  \n",
       "4                   -1.0     $85,691.03         $98,419.23     16193.0  \n",
       "6                    1.0     $74,899.78         $64,886.16    196986.0  \n",
       "...                  ...            ...                ...         ...  \n",
       "2249310              0.0     $72,571.13         $83,393.68     62821.0  \n",
       "2249311              0.0     $72,571.13         $83,393.68     62821.0  \n",
       "2249312              0.0     $72,571.13         $83,393.68     62821.0  \n",
       "2249313              0.0     $72,571.13         $83,393.68     62821.0  \n",
       "2249314              0.0     $72,571.13         $83,393.68     62821.0  \n",
       "\n",
       "[944616 rows x 15 columns]"
      ]
     },
     "execution_count": 423,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Simplify data by dropping columns we're not interested in\n",
    "merged_df = merged_df.drop(columns=['MarketEstimate', 'RentEstimate', '%CvgCityPark', 'Diversity Rank (Race)', 'Diversity Rank (Gender)', 'AVG C2I', 'MeanIncome', 'MedianIncome'])\n",
    "\n",
    "# Drop NaN Vals\n",
    "merged_df = merged_df.dropna()\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 5: UPLOADING TO SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 424,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connected to the database successfully!\n",
      "Data uploaded successfully!\n",
      "DataFrame successfully uploaded to the SQL database.\n"
     ]
    }
   ],
   "source": [
    "from sqlalchemy import create_engine\n",
    "from dotenv import load_dotenv\n",
    "from supabase import create_client, Client\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Retrieve database credentials from .env file\n",
    "DB_URL = os.getenv(\"DATABASE_URL\")\n",
    "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
    "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
    "DB_PASS = os.getenv(\"DATABASE_PASS\")\n",
    "\n",
    "# Initialize Supabase client\n",
    "supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
    "\n",
    "# Create SQLAlchemy engine\n",
    "engine = create_engine(DB_URL, pool_size=5, max_overflow=10)\n",
    "\n",
    "try:\n",
    "    with engine.connect() as conn:\n",
    "        print(\"Connected to the database successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "\n",
    "\n",
    "try:\n",
    "    # merged_df.to_sql('Housing Data', engine, if_exists='replace', index=False)\n",
    "    print(\"Data uploaded successfully!\")\n",
    "except Exception as e:\n",
    "    print(f\"Error uploading data: {e}\")\n",
    "\n",
    "# Upload DataFrame to SQL table\n",
    "# merged_df.to_sql(\"Housing Data\", engine, if_exists=\"replace\", index=False)\n",
    "\n",
    "print(\"DataFrame successfully uploaded to the SQL database.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 6: KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<>:13: SyntaxWarning: invalid escape sequence '\\$'\n",
      "<>:13: SyntaxWarning: invalid escape sequence '\\$'\n",
      "/var/folders/vr/57kjgf610p972_0992rx8k700000gn/T/ipykernel_20109/448515744.py:13: SyntaxWarning: invalid escape sequence '\\$'\n",
      "  merged_df[col] = merged_df[col].replace('[\\$,]', '', regex=True).astype(float)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended city: fargo,nd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Select features for clustering\n",
    "feature_cols = ['Bedroom', 'Bathroom', 'Area', 'LotArea', 'Price', '2022 Median Income', 'Temperature', 'Population']\n",
    "quality_cols = ['AQI%Good', 'WaterQualityVPV', 'Unemployment', '2016 Crime Rate', 'Cost of Living']\n",
    "# Note: Curious about whether or not we should have Cost of Living be an inputted feature variable or if it should just be something that's generally minimized\n",
    "\n",
    "# Clean currency columns\n",
    "for col in ['Price', '2022 Median Income', 'Cost of Living']:\n",
    "    merged_df[col] = merged_df[col].replace('[\\$,]', '', regex=True).astype(float)\n",
    "\n",
    "# Clean crime rate\n",
    "merged_df['2016 Crime Rate'] = (\n",
    "    merged_df['2016 Crime Rate']\n",
    "    .astype(str)\n",
    "    .str.extract(r'(\\d+)/(\\d+)')\n",
    "    .astype(float)\n",
    "    .apply(lambda row: row[0] / row[1] if pd.notna(row[0]) and pd.notna(row[1]) else np.nan, axis=1)\n",
    ")\n",
    "\n",
    "# Clean quality columns\n",
    "for col in quality_cols:\n",
    "    merged_df[col] = (\n",
    "        merged_df[col]\n",
    "        .astype(str)\n",
    "        .str.replace('%', '', regex=False)\n",
    "        .str.replace(',', '', regex=False)\n",
    "        .replace({'N/A': np.nan, 'unknown': np.nan, 'Missing': np.nan})\n",
    "    )\n",
    "    merged_df[col] = pd.to_numeric(merged_df[col], errors='coerce')\n",
    "\n",
    "# Map temperature to numeric\n",
    "temp_mapping = {'Cold': 0, 'Medium': 1, 'Hot': 2}\n",
    "merged_df['Temperature'] = merged_df['Temperature'].map(temp_mapping)\n",
    "\n",
    "# Save dataframe as it is\n",
    "merged_df2 = merged_df.copy()\n",
    "\n",
    "# Preprocess: scale the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(merged_df[feature_cols])\n",
    "# X = merged_df[feature_cols].values\n",
    "\n",
    "# Get unique cities and map them\n",
    "cities = merged_df['City'].unique()\n",
    "city_to_index = {city: idx for idx, city in enumerate(cities)}\n",
    "index_to_city = {idx: city for city, idx in city_to_index.items()}\n",
    "\n",
    "# Assign city labels\n",
    "y = merged_df['City'].map(city_to_index)\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "def recommend_city(user_input, visualize=True):\n",
    "    # user_input: includes Bedroom, Bathroom, Area, LotArea, Price, Temperature, 2022 Median Income, Population\n",
    "    temp_mapping = {'Cold': 0, 'Medium': 1, 'Hot': 2}\n",
    "    \n",
    "    user_features = np.array([\n",
    "        user_input['Bedroom'],\n",
    "        user_input['Bathroom'],\n",
    "        user_input['Area'],\n",
    "        user_input['LotArea'],\n",
    "        user_input['Price'],\n",
    "        user_input['2022 Median Income'],\n",
    "        temp_mapping[user_input['Temperature']],  # map temp string to number\n",
    "        user_input['Population']\n",
    "    ]).reshape(1, -1)\n",
    "\n",
    "    user_features_scaled = scaler.transform(user_features)\n",
    "\n",
    "    # Calculate Euclidean distances\n",
    "    distances = np.linalg.norm(X - user_features_scaled, axis=1)\n",
    "\n",
    "    # Find k nearest neighbors\n",
    "    k = 5\n",
    "    top_k_indices = np.argsort(distances)[:k]\n",
    "\n",
    "    # Look up corresponding cities\n",
    "    neighbor_cities = y.iloc[top_k_indices].values\n",
    "\n",
    "    # Tally up most common city\n",
    "    unique, counts = np.unique(neighbor_cities, return_counts=True)\n",
    "    candidate_cities = [index_to_city[idx] for idx in unique]\n",
    "\n",
    "    # Filter original df for these candidates\n",
    "    candidates_df = merged_df[merged_df['City'].isin(candidate_cities)].copy()\n",
    "\n",
    "    # Score candidates based on quality metrics (Increasing or decreasing distance based on QoL features)\n",
    "    candidates_df['QualityScore'] = (\n",
    "        candidates_df['AQI%Good'] + candidates_df['WaterQualityVPV']\n",
    "        - candidates_df['Unemployment'] * 2 - candidates_df['2016 Crime Rate'] * 2\n",
    "        - candidates_df['Cost of Living'] * 1.5\n",
    "    )\n",
    "\n",
    "    # Group by city and take the best average score\n",
    "    city_scores = candidates_df.groupby('City')['QualityScore'].mean()\n",
    "\n",
    "    # Best city\n",
    "    best_city = city_scores.idxmax()\n",
    "\n",
    "    visualize = False\n",
    "    if visualize:\n",
    "        # Visualize\n",
    "        user_2d = pca.transform(user_features_scaled)\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sample_size = 5000\n",
    "        if len(merged_df) > sample_size:\n",
    "            sampled_indices = np.random.choice(len(merged_df), size=sample_size, replace=False)\n",
    "            sampled_df = merged_df.iloc[sampled_indices]\n",
    "            X_2d_sampled = X_2d[sampled_indices]\n",
    "        else:\n",
    "            sampled_df = merged_df\n",
    "            X_2d_sampled = X_2d\n",
    "\n",
    "        for city in sampled_df['City'].unique():\n",
    "            mask = sampled_df['City'] == city\n",
    "            plt.scatter(X_2d_sampled[mask, 0], X_2d_sampled[mask, 1], label=city, alpha=0.5)\n",
    "\n",
    "        plt.scatter(user_2d[0, 0], user_2d[0, 1], color='black', marker='X', s=200, label='User Input')\n",
    "\n",
    "        for idx in top_k_indices:\n",
    "            plt.plot([user_2d[0, 0], X_2d[idx, 0]], [user_2d[0, 1], X_2d[idx, 1]], 'k--', alpha=0.7)\n",
    "\n",
    "        plt.title(f'Recommended City: {best_city}')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return best_city\n",
    "\n",
    "# Example user input\n",
    "user_input = {\n",
    "    'Bedroom': 3,\n",
    "    'Bathroom': 2,\n",
    "    'Area': 1500,\n",
    "    'LotArea': 5000,\n",
    "    'Price': 1000000,\n",
    "    '2022 Median Income': 90000,\n",
    "    'Temperature': 'Cold',   # Now passed as text\n",
    "    'Population': 500000     # New: user estimates size of city they want\n",
    "}\n",
    "\n",
    "best_city = recommend_city(user_input, visualize=True)\n",
    "print(f\"Recommended city: {best_city}\")s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 7: TESTING ACCURACY\n",
    "\n",
    "Note: This first trial tests the accuracy of the features for each row in the testing data returning the specific city that is actually assigned to them. Since our model does not take in quality of life features as inputted Features, but instead tries to pick a city that generally minimizes or maximizes them (depending on the specific QoL feature) by default, this will return a relatively low accuracy since certain cities will have lower quality of life ratings and so will not be returned by the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 426,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sampled model accuracy (on 500 points): 0.57\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data (assuming Temperature is already part of X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Important: fix randomness\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample 500 test points\n",
    "sample_size = 500\n",
    "sample_indices = np.random.choice(len(X_test), size=sample_size, replace=False)\n",
    "\n",
    "X_test_sampled = X_test[sample_indices]        # numpy array indexing\n",
    "y_test_sampled = y_test.iloc[sample_indices]   # pandas Series positional indexing\n",
    "\n",
    "# Recommendation function â€” no temperature penalty needed\n",
    "def recommend_city_from_train(user_features_scaled, X_train, y_train, k=5):\n",
    "    distances = np.linalg.norm(X_train - user_features_scaled, axis=1)\n",
    "    \n",
    "    top_k_indices = np.argsort(distances)[:k]\n",
    "    neighbor_cities = y_train.iloc[top_k_indices].values\n",
    "    unique, counts = np.unique(neighbor_cities, return_counts=True)\n",
    "    best_city_idx = unique[np.argmax(counts)]\n",
    "    return best_city_idx\n",
    "\n",
    "# Evaluate accuracy on the sampled test set\n",
    "correct = 0\n",
    "total = len(X_test_sampled)\n",
    "\n",
    "for i in range(total):\n",
    "    user_features_scaled = X_test_sampled[i].reshape(1, -1)  # numpy indexing\n",
    "    predicted_idx = recommend_city_from_train(user_features_scaled, X_train, y_train)\n",
    "    actual_idx = y_test_sampled.iloc[i]  # pandas indexing\n",
    "\n",
    "    if predicted_idx == actual_idx:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Sampled model accuracy (on {sample_size} points): {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 8: TESTING ACCURACY WITHOUT SIMPLY MAXIMIZING QUALITY OF LIFE FEATURES\n",
    "\n",
    "Now, we tinker with constructing our model to recieve QoL features as input, so that we can see how good it is at generally predicting data. If this model can return a high accuracy, we know our KNN Algorithm is working properly to at least predict the cities accurately. However, for the final model, we will change it back to maximizing or minimizing certain QoL values because we are not just trying to predict a city but are trying to give the user the best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 427,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure Matching Model Accuracy: 0.80\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reset dataframe\n",
    "merged_df = merged_df2.copy()\n",
    "\n",
    "# Step 1: Prepare feature set that includes quality of life features + temperature + population\n",
    "full_feature_cols = ['Bedroom', 'Bathroom', 'Area', 'LotArea', 'Price', '2022 Median Income',\n",
    "                     'AQI%Good', 'WaterQualityVPV', 'Unemployment', '2016 Crime Rate', 'Cost of Living', \n",
    "                     'Temperature', 'Population'] \n",
    "\n",
    "# Preprocess: scale the full features\n",
    "scaler_full = StandardScaler()\n",
    "X_full = scaler_full.fit_transform(merged_df[full_feature_cols])\n",
    "# X_full = merged_df[full_feature_cols].values\n",
    "\n",
    "# Labels stay the same\n",
    "y_full = merged_df['City'].map(city_to_index)\n",
    "\n",
    "# Step 2: Train/test split\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Important: fix randomness\n",
    "np.random.seed(42)\n",
    "\n",
    "# Optional: sample 500 points to keep speed reasonable\n",
    "sample_size = 500\n",
    "sample_indices = np.random.choice(len(X_test_full), size=sample_size, replace=False)\n",
    "X_test_sampled_full = X_test_full[sample_indices]\n",
    "y_test_sampled_full = y_test_full.iloc[sample_indices]\n",
    "\n",
    "# Step 3: Define a pure recommend function\n",
    "def pure_recommend_city(user_features_scaled, X_train, y_train, k=5):\n",
    "    distances = np.linalg.norm(X_train - user_features_scaled, axis=1)\n",
    "\n",
    "    top_k_indices = np.argsort(distances)[:k]\n",
    "    neighbor_cities = y_train.iloc[top_k_indices].values\n",
    "    unique, counts = np.unique(neighbor_cities, return_counts=True)\n",
    "    best_city_idx = unique[np.argmax(counts)]\n",
    "    return best_city_idx\n",
    "\n",
    "# Step 4: Evaluate pure matching accuracy\n",
    "correct = 0\n",
    "total = len(X_test_sampled_full)\n",
    "\n",
    "for i in range(total):\n",
    "    user_features_scaled = X_test_sampled_full[i].reshape(1, -1)\n",
    "    predicted_idx = pure_recommend_city(user_features_scaled, X_train_full, y_train_full)\n",
    "    actual_idx = y_test_sampled_full.iloc[i]\n",
    "\n",
    "    if predicted_idx == actual_idx:\n",
    "        correct += 1\n",
    "\n",
    "pure_accuracy = correct / total\n",
    "print(f\"Pure Matching Model Accuracy: {pure_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we try to tune the model for higher accuracy by increasing the weights of important, city-defining features, dropping noisy features, increasing the value of k, and other tweaks\n",
    "\n",
    "Once we find the modifications that maximize the model's accuracy without QoL optimization, we can re-implement these tweaks into the regular model which DOES maximize QoL Values, as we know that these results will be more accurate to the user's inputs while still maximizing better quality of life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 428,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure Matching Tuned Model Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reset dataframe\n",
    "merged_df = merged_df2.copy()\n",
    "\n",
    "# Step 1: Prepare feature set that includes quality of life features + temperature + population\n",
    "full_feature_cols = ['Bedroom', 'Bathroom', 'Area', 'LotArea', 'Price', '2022 Median Income',\n",
    "                     'AQI%Good', 'WaterQualityVPV', 'Unemployment', '2016 Crime Rate', 'Cost of Living', \n",
    "                     'Temperature', 'Population']\n",
    "\n",
    "# merged_df[\"2022 Median Income\"] *= 1\n",
    "# merged_df[\"Cost of Living\"] *= 2.5\n",
    "# merged_df[\"Population\"] *= 2\n",
    "# merged_df[\"Temperature\"] *= 2.5 \n",
    "#merged_df['Bedroom'] *= 1.5\n",
    "#merged_df['Bathroom'] *= 1.5\n",
    "#merged_df['Area'] *= 5\n",
    "# merged_df['LotArea'] *= 1.5\n",
    "\n",
    "# Preprocess: scale the full features\n",
    "scaler_full = StandardScaler()\n",
    "X_full = scaler_full.fit_transform(merged_df[full_feature_cols])\n",
    "# X_full = merged_df[full_feature_cols].values\n",
    "\n",
    "# Weigh features to prioritize more important, impactful ones\n",
    "# feature_weights = np.array([1, 1, 1, 1, 1, 2.5, 1, 1, 1, 1, 2, 3, 2])\n",
    "feature_weights = np.array([1, 1, 1, 1.5, 1, 4, 1, 1, 1, 1, 2.5, 3, 3])\n",
    "\n",
    "\n",
    "# Labels stay the same\n",
    "y_full = merged_df['City'].map(city_to_index)\n",
    "\n",
    "# Step 2: Train/test split\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_full, y_full, test_size=0.2, random_state=42)\n",
    "\n",
    "# Important: fix randomness\n",
    "np.random.seed(42)\n",
    "\n",
    "# Optional: sample 500 points to keep speed reasonable\n",
    "sample_size = 500\n",
    "sample_indices = np.random.choice(len(X_test_full), size=sample_size, replace=False)\n",
    "X_test_sampled_full = X_test_full[sample_indices]\n",
    "y_test_sampled_full = y_test_full.iloc[sample_indices]\n",
    "\n",
    "# Step 3: Define a pure recommend function (NO temperature penalty anymore)\n",
    "def pure_recommend_city(user_features_scaled, X_train, y_train, k=5):\n",
    "    weighted_diff = (X_train - user_features_scaled) * feature_weights\n",
    "    distances = np.linalg.norm(weighted_diff, axis=1)\n",
    "\n",
    "    top_k_indices = np.argsort(distances)[:k]\n",
    "    neighbor_cities = y_train.iloc[top_k_indices].values\n",
    "    unique, counts = np.unique(neighbor_cities, return_counts=True)\n",
    "    best_city_idx = unique[np.argmax(counts)]\n",
    "    return best_city_idx\n",
    "\n",
    "# Step 4: Evaluate pure matching accuracy\n",
    "correct = 0\n",
    "total = len(X_test_sampled_full)\n",
    "\n",
    "for i in range(total):\n",
    "    user_features_scaled = X_test_sampled_full[i].reshape(1, -1)\n",
    "    predicted_idx = pure_recommend_city(user_features_scaled, X_train_full, y_train_full)\n",
    "    actual_idx = y_test_sampled_full.iloc[i]\n",
    "\n",
    "    if predicted_idx == actual_idx:\n",
    "        correct += 1\n",
    "\n",
    "pure_accuracy = correct / total\n",
    "print(f\"Pure Matching Tuned Model Accuracy: {pure_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now, we reconstruct the KNN with these modifications to get our final, most accurate KNN which prioritizes Quality of Life values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 430,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended city: natchez,ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reset dataframe\n",
    "merged_df = merged_df2.copy()\n",
    "\n",
    "# Select features for clustering\n",
    "feature_cols = ['Bedroom', 'Bathroom', 'Area', 'LotArea', 'Price', '2022 Median Income', 'Temperature', 'Population']\n",
    "quality_cols = ['AQI%Good', 'WaterQualityVPV', 'Unemployment', '2016 Crime Rate', 'Cost of Living']\n",
    "# Note: Curious about whether or not we should have Cost of Living be an inputted feature variable or if it should just be something that's generally minimized\n",
    "\n",
    "# Preprocess: scale the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(merged_df[feature_cols])\n",
    "# X = merged_df[feature_cols].values\n",
    "\n",
    "# Weigh features to prioritize more important, impactful ones\n",
    "feature_weights = np.array([1, 1, 1, 1.5, 1, 4, 3, 3])\n",
    "\n",
    "# Get unique cities and map them\n",
    "cities = merged_df['City'].unique()\n",
    "city_to_index = {city: idx for idx, city in enumerate(cities)}\n",
    "index_to_city = {idx: city for city, idx in city_to_index.items()}\n",
    "\n",
    "# Assign city labels\n",
    "y = merged_df['City'].map(city_to_index)\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "def recommend_city(user_input, visualize=True):\n",
    "    # user_input: includes Bedroom, Bathroom, Area, LotArea, Price, Temperature, 2022 Median Income, Population\n",
    "    temp_mapping = {'Cold': 0, 'Medium': 1, 'Hot': 2}\n",
    "    \n",
    "    user_features = np.array([\n",
    "        user_input['Bedroom'],\n",
    "        user_input['Bathroom'],\n",
    "        user_input['Area'],\n",
    "        user_input['LotArea'],\n",
    "        user_input['Price'],\n",
    "        user_input['2022 Median Income'],\n",
    "        temp_mapping[user_input['Temperature']],  # map temp string to number\n",
    "        user_input['Population']\n",
    "    ]).reshape(1, -1)\n",
    "\n",
    "    user_features_scaled = scaler.transform(user_features)\n",
    "\n",
    "    # Calculate Euclidean distances\n",
    "    weighted_diff = (X_train - user_features_scaled) * feature_weights\n",
    "    distances = np.linalg.norm(weighted_diff, axis=1)\n",
    "\n",
    "    # Find k nearest neighbors\n",
    "    k = 5\n",
    "    top_k_indices = np.argsort(distances)[:k]\n",
    "\n",
    "    # Look up corresponding cities\n",
    "    neighbor_cities = y.iloc[top_k_indices].values\n",
    "\n",
    "    # Tally up most common city\n",
    "    unique, counts = np.unique(neighbor_cities, return_counts=True)\n",
    "    candidate_cities = [index_to_city[idx] for idx in unique]\n",
    "\n",
    "    # Filter original df for these candidates\n",
    "    candidates_df = merged_df[merged_df['City'].isin(candidate_cities)].copy()\n",
    "\n",
    "    # Score candidates based on quality metrics (Increasing or decreasing distance based on QoL features)\n",
    "    candidates_df['QualityScore'] = (\n",
    "        candidates_df['AQI%Good'] + candidates_df['WaterQualityVPV']\n",
    "        - candidates_df['Unemployment'] * 2\n",
    "        - candidates_df['2016 Crime Rate'] * 2\n",
    "        - candidates_df['Cost of Living'] * 1.5\n",
    "    )\n",
    "\n",
    "    # Group by city and take the best average score\n",
    "    city_scores = candidates_df.groupby('City')['QualityScore'].mean()\n",
    "\n",
    "    # Best city\n",
    "    best_city = city_scores.idxmax()\n",
    "\n",
    "    visualize = False\n",
    "    if visualize:\n",
    "        # Visualize\n",
    "        user_2d = pca.transform(user_features_scaled)\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sample_size = 5000\n",
    "        if len(merged_df) > sample_size:\n",
    "            sampled_indices = np.random.choice(len(merged_df), size=sample_size, replace=False)\n",
    "            sampled_df = merged_df.iloc[sampled_indices]\n",
    "            X_2d_sampled = X_2d[sampled_indices]\n",
    "        else:\n",
    "            sampled_df = merged_df\n",
    "            X_2d_sampled = X_2d\n",
    "\n",
    "        for city in sampled_df['City'].unique():\n",
    "            mask = sampled_df['City'] == city\n",
    "            plt.scatter(X_2d_sampled[mask, 0], X_2d_sampled[mask, 1], label=city, alpha=0.5)\n",
    "\n",
    "        plt.scatter(user_2d[0, 0], user_2d[0, 1], color='black', marker='X', s=200, label='User Input')\n",
    "\n",
    "        for idx in top_k_indices:\n",
    "            plt.plot([user_2d[0, 0], X_2d[idx, 0]], [user_2d[0, 1], X_2d[idx, 1]], 'k--', alpha=0.7)\n",
    "\n",
    "        plt.title(f'Recommended City: {best_city}')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return best_city\n",
    "\n",
    "# Example user input\n",
    "user_input = {\n",
    "    'Bedroom': 3,\n",
    "    'Bathroom': 2,\n",
    "    'Area': 2500,\n",
    "    'LotArea': 5000,\n",
    "    'Price': 1000000,\n",
    "    '2022 Median Income': 160000,\n",
    "    'Temperature': 'Cold',   # Now passed as text\n",
    "    'Population': 500000     # New: user estimates size of city they want\n",
    "}\n",
    "\n",
    "best_city = recommend_city(user_input, visualize=True)\n",
    "print(f\"Recommended city: {best_city}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 9: CREATING A KNN FOR EVERY POSSIBLE COMBINATION OF USER INPUTTED FEATURES\n",
    "\n",
    "## INFO FOR MIDWAY REPORT:\n",
    "\n",
    "- Essentially what we're gonna do here is, after deciding which features we want to make Mandatory for the user to input, we will create a model for each different combination of user inputted features, so that the result that our entire machine/program gives them is accurate to what they inputted. For example, if they only specified temperature, price, and square footage, we should give them their best city based only on those fields, rather than simply choosing a default value for their unfilled fields and running that on our first model. This would greatly limit the possible outcomes of the model to be cities that match the default fields we would select for when a user leaves something blank. So we're not gonna do this. We probably want less than 30 or 40 models, cuz otherwise it gets redundant, so maybe we'll only leave 5 or 6 features optional.\n",
    "\n",
    "- This is imporant because this specific implementation is how we are overcoming the problem of a MODEL RUNNING WHEN INPUT FEATURES ARE MISSING, which is a large part of our project and is one of the main questions we sought to answer. We also were interested in removing outliers, which we could propose involves modifying the data to look at things like the average house size/num bedrooms found in a city, etc, because there may be outliers in each city that are massively large mansions even though the city is generally poor. So the solution would be essentially changing the data to represent averages and medians across cities, specifically when it involves the house-features (since the city features are already the same for all house entries which are in the same city, since we filled out the city-related columns just based off of the city that the house was in). This is something we can suggest we will look more into going forward.\n",
    "\n",
    "- Finally, we should also talk about what else we do to expand past KNNs, since we are almost entirely done with the KNNs. We have already proposed that we look into other types of models that could essentially create the same tool we're trying to build based off of our data, and then investigate how to deal with missing features as well as outliers in both of then. I think that we should say we're interested in doing a Linear Regression model, as well as trying random forest. I think Linear Regression would be stronger to have on our Midway report, even if we're not gonna do it. I also think we should say we're interested in trying a neural network.\n",
    "\n",
    "- OOH - BOOM. I THINK I GOT IT. WE SHOULD SAY THAT, ONCE WE MAKE A FEW MODELS WHICH DO THE SAME THING, USING DIFFERENT ML TACTICS, WE SHOULD PLAN TO COMPARE THEM BY SEEING WHAT DIFFERENT CITIES THEY COME UP WITH WHEN GIVEN THE SAME INPUT AFTER BEING TRAINED ON THE SAME DATA. THIS WOULD BE REALLY INTERESTING, BECAUSE THEN WE COULD CREATE A SIMPLE TESTING ALGORITHM WHICH FINDS OUT WHICH MODEL GAVE A CITY MORE ACCURATE TO THE USER'S PREFERENCES. THEN WE CAN THEORIZE WHY ONE IS BETTER THAN THE OTHER, AND BOOM THAT'S A GOOD PROJECT.\n",
    "\n",
    "- Also, in the midway report, make sure to specify how we've divided our data into input features from the user and just general QoL features that the model either tries to maximize or minimize in the cities it outputs, regardless of the user inputted data. This feature makes our approach a bit more unique compared to just any other KNN or ML model which provides a predicted output based on features. Otherwisem, just look through all the comments and headers in this document and try to summarize what has been done. This document pretty much contains everything that has been done for the project LMAO.\n",
    "\n",
    "- For the literature section, we need to do more work (Adhi was also right during that meeting, we essentially do have to BS it since most of my info has come from chatgpt), so we need to find some resources that we can pretend we got inspiration and insight from in terms of coding our project. I used freecodecamp a bit to learn about this stuff, so i can add that in.\n",
    "\n",
    "- Dividing up the work section should just be like one sentence, and our future plans should involve what i already specified here pretty much."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
