{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Packages"
      ],
      "metadata": {
        "id": "hSSh9iaisaHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supabase python-dotenv pandas tensorflow scikit-learn joblib"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XtRBySbysWMi",
        "outputId": "be3b81bf-981e-4083-d059-abc8ceb94e80",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting supabase\n",
            "  Downloading supabase-2.15.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Collecting gotrue<3.0.0,>=2.11.0 (from supabase)\n",
            "  Downloading gotrue-2.12.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.11/dist-packages (from supabase) (0.28.1)\n",
            "Collecting postgrest<1.1,>0.19 (from supabase)\n",
            "  Downloading postgrest-1.0.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting realtime<2.5.0,>=2.4.0 (from supabase)\n",
            "  Downloading realtime-2.4.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting storage3<0.12,>=0.10 (from supabase)\n",
            "  Downloading storage3-0.11.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting supafunc<0.10,>=0.9 (from supabase)\n",
            "  Downloading supafunc-0.9.4-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.10 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.11.3)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.10.1)\n",
            "Collecting pytest-mock<4.0.0,>=3.14.0 (from gotrue<3.0.0,>=2.11.0->supabase)\n",
            "  Downloading pytest_mock-3.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.14.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Collecting deprecation<3.0.0,>=2.1.0 (from postgrest<1.1,>0.19->supabase)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (3.11.15)\n",
            "Collecting websockets<15,>=11 (from realtime<2.5.0,>=2.4.0->supabase)\n",
            "  Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Collecting strenum<0.5.0,>=0.4.15 (from supafunc<0.10,>=0.9->supabase)\n",
            "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (1.19.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (0.4.0)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.11/dist-packages (from pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (8.3.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (1.5.0)\n",
            "Downloading supabase-2.15.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading gotrue-2.12.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading postgrest-1.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading realtime-2.4.2-py3-none-any.whl (22 kB)\n",
            "Downloading storage3-0.11.3-py3-none-any.whl (17 kB)\n",
            "Downloading supafunc-0.9.4-py3-none-any.whl (7.8 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pytest_mock-3.14.0-py3-none-any.whl (9.9 kB)\n",
            "Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
            "Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: strenum, websockets, python-dotenv, deprecation, pytest-mock, realtime, supafunc, storage3, postgrest, gotrue, supabase\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "Successfully installed deprecation-2.1.0 gotrue-2.12.0 postgrest-1.0.1 pytest-mock-3.14.0 python-dotenv-1.1.0 realtime-2.4.2 storage3-0.11.3 strenum-0.4.15 supabase-2.15.0 supafunc-0.9.4 websockets-14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bring in data"
      ],
      "metadata": {
        "id": "9pnAwq4ZsxeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['SUPABASE_URL'] = 'https://lgcrogvgnqphznuwdopu.supabase.co'\n",
        "os.environ['SUPABASE_KEY'] = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImxnY3JvZ3ZnbnFwaHpudXdkb3B1Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDQ0MTQwMzcsImV4cCI6MjA1OTk5MDAzN30.2lozGgOq70UbrCm1_7Y1p38WbCqOMTjQ8Cs_ZSvNhSs'"
      ],
      "metadata": {
        "id": "v3HYrpXNs8Iz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import PsiKit Learn"
      ],
      "metadata": {
        "id": "RhPcDOg54oGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ],
      "metadata": {
        "id": "Rz2Dq4tZ4xOY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database Setup"
      ],
      "metadata": {
        "id": "qIgz1FFwtlJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile db.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from supabase import create_client, Client\n",
        "import pandas as pd\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
        "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
        "\n",
        "def get_supabase_client() -> Client:\n",
        "    return create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "def fetch_housing_data() -> pd.DataFrame:\n",
        "    client = get_supabase_client()\n",
        "    # Replace \"House\" with your actual table name if different\n",
        "    response = client.table(\"House\").select(\"*\").execute()\n",
        "    data = response.data  # a list of dictionaries\n",
        "    return pd.DataFrame(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "J0EFFUcjtrU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "18b88508-65af-4282-da19-7db46fe18001"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing db.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database link"
      ],
      "metadata": {
        "id": "aZp22fF6uaC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile db.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from supabase import create_client, Client\n",
        "import pandas as pd\n",
        "\n",
        "# Load environment variables from a .env file or your environment\n",
        "load_dotenv()\n",
        "\n",
        "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
        "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
        "\n",
        "def get_supabase_client() -> Client:\n",
        "    return create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "def fetch_housing_data() -> pd.DataFrame:\n",
        "    client = get_supabase_client()\n",
        "    # Replace \"House\" with your actual table name if needed.\n",
        "    response = client.table(\"House\").select(\"*\").execute()\n",
        "    data = response.data  # a list of dictionaries\n",
        "    return pd.DataFrame(data)\n"
      ],
      "metadata": {
        "id": "LQUwr_JyucoT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4242c090-5d9f-4d50-9bbe-86870cb97016"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overwriting db.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Script"
      ],
      "metadata": {
        "id": "qICHF1HUtuD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile train_model.py\n",
        "import os\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from db import fetch_housing_data\n",
        "import joblib\n",
        "\n",
        "def train_model():\n",
        "    # 1. Fetch data from Supabase.\n",
        "    df = fetch_housing_data()\n",
        "    print(\"Data loaded from Supabase:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # 2. Create a binary target from \"QualityOfLifeTotalScore\".\n",
        "    # Here, a score above the median is set to 1 (high quality) and otherwise 0.\n",
        "    threshold = df['QualityOfLifeTotalScore'].median()\n",
        "    df['target'] = (df['QualityOfLifeTotalScore'] > threshold).astype(int)\n",
        "\n",
        "    # Input feature selection.\n",
        "    input_features = ['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']\n",
        "    target_feature = 'target'\n",
        "\n",
        "    # Impute missing values.\n",
        "    X = df[input_features].fillna(df[input_features].median())\n",
        "    y = df[target_feature]\n",
        "\n",
        "    # 3. Scale the input features.\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # 4. Split the data into training and testing sets.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 5. Build the logistic regression model.\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=(X_train.shape[1],)),\n",
        "        tf.keras.layers.Dense(1, activation='sigmoid')\n",
        "    ])\n",
        "\n",
        "    # 6. Compile the model with binary_crossentropy and accuracy as metric.\n",
        "    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "    # 7. Train the model. (Using 10% of training data for validation.)\n",
        "    model.fit(X_train, y_train, epochs=50, validation_split=0.1)\n",
        "\n",
        "    # 8. Evaluate the model.\n",
        "    loss, accuracy = model.evaluate(X_test, y_test)\n",
        "    print(\"Test loss:\", loss)\n",
        "    print(\"Test accuracy:\", accuracy)\n",
        "\n",
        "    # 9. Save the model and scaler.\n",
        "    model.save(\"trained_model.h5\")\n",
        "    joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()\n"
      ],
      "metadata": {
        "id": "xQ1fNRDXtxFG",
        "outputId": "84769b99-99fe-4601-c6cd-016145f7974b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing train_model.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add-on"
      ],
      "metadata": {
        "id": "cNN2QR1GxsLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbdq2CSnxuVZ",
        "outputId": "8a5cbd1f-4af5-477d-a576-e7bee5c85ece"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions on Model"
      ],
      "metadata": {
        "id": "_oDdWTFRxbLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile prediction.py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "from db import fetch_housing_data\n",
        "\n",
        "# Load the trained logistic regression model and scaler.\n",
        "model = tf.keras.models.load_model(\"trained_model.h5\")\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "\n",
        "def predict_house_quality(user_input):\n",
        "    \"\"\"\n",
        "    Predicts the binary house quality class.\n",
        "\n",
        "    Expected keys in user_input:\n",
        "      - house_price: corresponds to 'ListedPrice'\n",
        "      - income: corresponds to 'MeanIncome'\n",
        "      - beds: corresponds to 'Bedroom'\n",
        "      - baths: corresponds to 'Bathroom'\n",
        "      - sq_ft: corresponds to 'Area'\n",
        "      - population: corresponds to '2022 Population'\n",
        "\n",
        "    Returns:\n",
        "      - predicted_class: 0 (low quality) or 1 (high quality)\n",
        "    \"\"\"\n",
        "    data = np.array([[\n",
        "        user_input[\"house_price\"],\n",
        "        user_input[\"income\"],\n",
        "        user_input[\"beds\"],\n",
        "        user_input[\"baths\"],\n",
        "        user_input[\"sq_ft\"],\n",
        "        user_input[\"population\"]\n",
        "    ]])\n",
        "    scaled_data = scaler.transform(data)\n",
        "    probability = model.predict(scaled_data)\n",
        "    predicted_class = (probability > 0.5).astype(\"int32\")\n",
        "    return predicted_class[0][0]\n",
        "\n",
        "def find_closest_match(user_input, df, scaler):\n",
        "    \"\"\"\n",
        "    Finds the closest matching record in the DataFrame to the user inputs based on the training features.\n",
        "\n",
        "    Parameters:\n",
        "      - user_input: dictionary with keys \"house_price\", \"income\", \"beds\", \"baths\", \"sq_ft\", \"population\"\n",
        "      - df: DataFrame containing the housing records with at least the following columns:\n",
        "            ['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']\n",
        "      - scaler: a fitted StandardScaler instance used on the training data.\n",
        "\n",
        "    Returns:\n",
        "      - A Pandas Series corresponding to the closest matching record.\n",
        "    \"\"\"\n",
        "    input_features = ['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']\n",
        "    user_vector = np.array([[\n",
        "        user_input[\"house_price\"],\n",
        "        user_input[\"income\"],\n",
        "        user_input[\"beds\"],\n",
        "        user_input[\"baths\"],\n",
        "        user_input[\"sq_ft\"],\n",
        "        user_input[\"population\"]\n",
        "    ]])\n",
        "\n",
        "    user_vector_scaled = scaler.transform(user_vector)\n",
        "    data_features = df[input_features].fillna(df[input_features].median())\n",
        "    data_features_scaled = scaler.transform(data_features)\n",
        "    distances = np.linalg.norm(data_features_scaled - user_vector_scaled, axis=1)\n",
        "    idx = np.argmin(distances)\n",
        "    closest_match = df.iloc[idx]\n",
        "    return closest_match\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7oPQav1xhn7",
        "outputId": "d721045a-a3c5-4840-aa8e-9415cb90aa6e"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing prediction.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find closest match"
      ],
      "metadata": {
        "id": "edo3YCq50_ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def find_closest_match(user_input, df, scaler):\n",
        "    \"\"\"\n",
        "    Given a user_input dictionary, a DataFrame df with the housing records,\n",
        "    and a scaler used for the training data, this function finds the record\n",
        "    in df that is closest to the user's desired features.\n",
        "\n",
        "    Parameters:\n",
        "      - user_input: dict with keys \"house_price\", \"income\", \"beds\",\n",
        "                    \"baths\", \"sq_ft\", \"population\"\n",
        "      - df: DataFrame of housing records containing at least the following columns:\n",
        "            ['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']\n",
        "            plus location columns like \"State\", \"City\".\n",
        "      - scaler: A StandardScaler fitted on the training data.\n",
        "\n",
        "    Returns:\n",
        "      - closest_match: A Pandas Series that represents the record closest to the input.\n",
        "    \"\"\"\n",
        "    # Define the features used for matching (same as used in training)\n",
        "    input_features = ['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']\n",
        "\n",
        "    # Construct the user vector from the dictionary\n",
        "    user_vector = np.array([[user_input[\"house_price\"],\n",
        "                              user_input[\"income\"],\n",
        "                              user_input[\"beds\"],\n",
        "                              user_input[\"baths\"],\n",
        "                              user_input[\"sq_ft\"],\n",
        "                              user_input[\"population\"]]])\n",
        "\n",
        "    # Scale the user input using the same scaler\n",
        "    user_vector_scaled = scaler.transform(user_vector)\n",
        "\n",
        "    # Extract the features from df and handle missing values if needed\n",
        "    data_features = df[input_features].fillna(df[input_features].median())\n",
        "\n",
        "    # Scale all these feature rows\n",
        "    data_features_scaled = scaler.transform(data_features)\n",
        "\n",
        "    # Compute Euclidean distances between the user vector and each row in data_features_scaled\n",
        "    distances = np.linalg.norm(data_features_scaled - user_vector_scaled, axis=1)\n",
        "\n",
        "    # Find the index of the closest match\n",
        "    idx = np.argmin(distances)\n",
        "    closest_match = df.iloc[idx]\n",
        "\n",
        "    return closest_match\n"
      ],
      "metadata": {
        "id": "A3CCjRWR1C9Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Training Protocol"
      ],
      "metadata": {
        "id": "IBZcLKkxa73i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!python train_model.py"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wVqpunz2a5Dd",
        "outputId": "6e742bff-5195-40bf-8392-8eb3829c404a"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2025-04-24 07:28:51.477290: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1745479731.502809    1343 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
            "E0000 00:00:1745479731.510481    1343 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
            "2025-04-24 07:28:51.536371: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
            "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
            "Data loaded from Supabase:\n",
            "  State        City  ...  QualityOfLifeSafety      id\n",
            "0    az  phoenix,az  ...                   40  289585\n",
            "1    az  phoenix,az  ...                   40  289586\n",
            "2    az  phoenix,az  ...                   40  289587\n",
            "3    az  phoenix,az  ...                   40  289588\n",
            "4    az  phoenix,az  ...                   40  289589\n",
            "\n",
            "[5 rows x 22 columns]\n",
            "2025-04-24 07:28:58.960703: E external/local_xla/xla/stream_executor/cuda/cuda_driver.cc:152] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: UNKNOWN ERROR (303)\n",
            "Epoch 1/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 17ms/step - accuracy: 0.4280 - loss: 0.8179 - val_accuracy: 0.4375 - val_loss: 0.7825\n",
            "Epoch 2/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.4270 - loss: 0.7850 - val_accuracy: 0.4500 - val_loss: 0.7628\n",
            "Epoch 3/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4622 - loss: 0.7807 - val_accuracy: 0.4875 - val_loss: 0.7441\n",
            "Epoch 4/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - accuracy: 0.4719 - loss: 0.7665 - val_accuracy: 0.5375 - val_loss: 0.7275\n",
            "Epoch 5/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5246 - loss: 0.7654 - val_accuracy: 0.5875 - val_loss: 0.7107\n",
            "Epoch 6/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - accuracy: 0.5865 - loss: 0.7359 - val_accuracy: 0.6250 - val_loss: 0.6949\n",
            "Epoch 7/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.5964 - loss: 0.7213 - val_accuracy: 0.6250 - val_loss: 0.6793\n",
            "Epoch 8/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6536 - loss: 0.6919 - val_accuracy: 0.6250 - val_loss: 0.6650\n",
            "Epoch 9/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6616 - loss: 0.6777 - val_accuracy: 0.6500 - val_loss: 0.6514\n",
            "Epoch 10/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7075 - loss: 0.6594 - val_accuracy: 0.6625 - val_loss: 0.6379\n",
            "Epoch 11/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6764 - loss: 0.6705 - val_accuracy: 0.6625 - val_loss: 0.6247\n",
            "Epoch 12/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.6814 - loss: 0.6566 - val_accuracy: 0.6875 - val_loss: 0.6127\n",
            "Epoch 13/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7321 - loss: 0.6223 - val_accuracy: 0.7000 - val_loss: 0.6008\n",
            "Epoch 14/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7075 - loss: 0.6269 - val_accuracy: 0.7000 - val_loss: 0.5894\n",
            "Epoch 15/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6921 - loss: 0.6195 - val_accuracy: 0.7000 - val_loss: 0.5777\n",
            "Epoch 16/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7194 - loss: 0.6051 - val_accuracy: 0.7125 - val_loss: 0.5670\n",
            "Epoch 17/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.6921 - loss: 0.6032 - val_accuracy: 0.7375 - val_loss: 0.5563\n",
            "Epoch 18/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7318 - loss: 0.5733 - val_accuracy: 0.7375 - val_loss: 0.5458\n",
            "Epoch 19/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.7228 - loss: 0.5707 - val_accuracy: 0.7375 - val_loss: 0.5358\n",
            "Epoch 20/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.7203 - loss: 0.5604 - val_accuracy: 0.7500 - val_loss: 0.5259\n",
            "Epoch 21/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7746 - loss: 0.5280 - val_accuracy: 0.7500 - val_loss: 0.5164\n",
            "Epoch 22/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7280 - loss: 0.5333 - val_accuracy: 0.7750 - val_loss: 0.5067\n",
            "Epoch 23/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7526 - loss: 0.5250 - val_accuracy: 0.8125 - val_loss: 0.4977\n",
            "Epoch 24/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7657 - loss: 0.5174 - val_accuracy: 0.8375 - val_loss: 0.4887\n",
            "Epoch 25/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.7927 - loss: 0.5107 - val_accuracy: 0.8500 - val_loss: 0.4803\n",
            "Epoch 26/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8261 - loss: 0.4993 - val_accuracy: 0.8875 - val_loss: 0.4720\n",
            "Epoch 27/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.8558 - loss: 0.4885 - val_accuracy: 0.9125 - val_loss: 0.4634\n",
            "Epoch 28/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.8849 - loss: 0.4771 - val_accuracy: 0.9375 - val_loss: 0.4554\n",
            "Epoch 29/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - accuracy: 0.9380 - loss: 0.4598 - val_accuracy: 0.9750 - val_loss: 0.4475\n",
            "Epoch 30/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9247 - loss: 0.4700 - val_accuracy: 0.9875 - val_loss: 0.4400\n",
            "Epoch 31/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9302 - loss: 0.4567 - val_accuracy: 0.9875 - val_loss: 0.4322\n",
            "Epoch 32/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9489 - loss: 0.4466 - val_accuracy: 0.9875 - val_loss: 0.4249\n",
            "Epoch 33/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9408 - loss: 0.4383 - val_accuracy: 0.9875 - val_loss: 0.4178\n",
            "Epoch 34/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9530 - loss: 0.4311 - val_accuracy: 0.9875 - val_loss: 0.4108\n",
            "Epoch 35/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9603 - loss: 0.4224 - val_accuracy: 0.9875 - val_loss: 0.4042\n",
            "Epoch 36/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9617 - loss: 0.4144 - val_accuracy: 1.0000 - val_loss: 0.3973\n",
            "Epoch 37/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9667 - loss: 0.4122 - val_accuracy: 1.0000 - val_loss: 0.3905\n",
            "Epoch 38/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9684 - loss: 0.4031 - val_accuracy: 1.0000 - val_loss: 0.3841\n",
            "Epoch 39/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9730 - loss: 0.3938 - val_accuracy: 1.0000 - val_loss: 0.3778\n",
            "Epoch 40/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9715 - loss: 0.3882 - val_accuracy: 1.0000 - val_loss: 0.3717\n",
            "Epoch 41/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9803 - loss: 0.3802 - val_accuracy: 1.0000 - val_loss: 0.3656\n",
            "Epoch 42/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9884 - loss: 0.3757 - val_accuracy: 1.0000 - val_loss: 0.3595\n",
            "Epoch 43/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9837 - loss: 0.3727 - val_accuracy: 1.0000 - val_loss: 0.3540\n",
            "Epoch 44/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 0.9870 - loss: 0.3631 - val_accuracy: 1.0000 - val_loss: 0.3481\n",
            "Epoch 45/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9880 - loss: 0.3588 - val_accuracy: 1.0000 - val_loss: 0.3428\n",
            "Epoch 46/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9928 - loss: 0.3540 - val_accuracy: 1.0000 - val_loss: 0.3373\n",
            "Epoch 47/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9879 - loss: 0.3473 - val_accuracy: 1.0000 - val_loss: 0.3320\n",
            "Epoch 48/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9901 - loss: 0.3382 - val_accuracy: 1.0000 - val_loss: 0.3268\n",
            "Epoch 49/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9874 - loss: 0.3332 - val_accuracy: 1.0000 - val_loss: 0.3218\n",
            "Epoch 50/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - accuracy: 0.9908 - loss: 0.3269 - val_accuracy: 1.0000 - val_loss: 0.3168\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - accuracy: 1.0000 - loss: 0.3151 \n",
            "Test loss: 0.3185444176197052\n",
            "Test accuracy: 1.0\n",
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "User Input and Output"
      ],
      "metadata": {
        "id": "H2VLUTZW1FIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from db import fetch_housing_data\n",
        "from prediction import predict_house_quality, find_closest_match, scaler\n",
        "\n",
        "# Load the full dataset from Supabase.\n",
        "df = fetch_housing_data()  # Ensure that df is defined.\n",
        "print(\"Data loaded:\")\n",
        "print(df.head())\n",
        "\n",
        "# 1. Collect user input.\n",
        "house_price = float(input(\"Enter your desired house price: \"))\n",
        "income = float(input(\"Enter your income: \"))\n",
        "beds = int(input(\"Enter number of beds: \"))\n",
        "baths = int(input(\"Enter number of baths: \"))\n",
        "sq_ft = float(input(\"Enter square footage: \"))\n",
        "population = float(input(\"Enter desired population: \"))\n",
        "\n",
        "user_input = {\n",
        "    \"house_price\": house_price,\n",
        "    \"income\": income,\n",
        "    \"beds\": beds,\n",
        "    \"baths\": baths,\n",
        "    \"sq_ft\": sq_ft,\n",
        "    \"population\": population\n",
        "}\n",
        "\n",
        "# 2. Get the model prediction.\n",
        "prediction = predict_house_quality(user_input)\n",
        "print(\"Predicted house quality class (0 = low, 1 = high):\", prediction)\n",
        "\n",
        "# 3. Reload the full dataset if needed.\n",
        "df = fetch_housing_data()  # or load from a CSV if applicable.\n",
        "print(\"Full dataset loaded (first 5 rows):\")\n",
        "print(df.head())\n",
        "\n",
        "# 4. Find the closest matching record.\n",
        "closest_match = find_closest_match(user_input, df, scaler)\n",
        "\n",
        "# 5. Print out details from the best match.\n",
        "print(\"\\nClosest Matching House Record:\")\n",
        "print(\"State:\", closest_match.get(\"State\", \"N/A\"))\n",
        "print(\"City:\", closest_match.get(\"City\", \"N/A\"))\n",
        "print(\"Listed Price:\", closest_match.get(\"ListedPrice\", \"N/A\"))\n",
        "print(\"Mean Income:\", closest_match.get(\"MeanIncome\", \"N/A\"))\n",
        "print(\"Bedrooms:\", closest_match.get(\"Bedroom\", \"N/A\"))\n",
        "print(\"Bathrooms:\", closest_match.get(\"Bathroom\", \"N/A\"))\n",
        "print(\"Area:\", closest_match.get(\"Area\", \"N/A\"))\n",
        "print(\"2022 Population:\", closest_match.get(\"2022 Population\", \"N/A\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD6g5KIgz5VA",
        "outputId": "a70f4187-cb6d-4388-e21d-955b4a74d785"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded:\n",
            "  State        City  Bedroom  Bathroom    Area  ListedPrice Temperature  \\\n",
            "0    az  phoenix,az      4.0       3.0  2465.0       980000         Hot   \n",
            "1    az  phoenix,az      4.0       4.0  3695.0       975000         Hot   \n",
            "2    az  phoenix,az      4.0       2.0  2581.0       890000         Hot   \n",
            "3    az  phoenix,az      4.0       2.0  2217.0      1144000         Hot   \n",
            "4    az  phoenix,az      4.0       3.0  2578.0       850000         Hot   \n",
            "\n",
            "   2022 Population  2016 Crime Rate  Unemployment  ...  Cost of Living  \\\n",
            "0          4551524            0.032          3.46  ...        82847.38   \n",
            "1          4551524            0.032          3.46  ...        82847.38   \n",
            "2          4551524            0.032          3.46  ...        82847.38   \n",
            "3          4551524            0.032          3.46  ...        82847.38   \n",
            "4          4551524            0.032          3.46  ...        82847.38   \n",
            "\n",
            "   AVG C2I  MeanIncome  QualityOfLifeTotalScore  QualityOfLifeQualityOfLife  \\\n",
            "0    105.1       57559                    48.31                          21   \n",
            "1    105.1       57559                    48.31                          21   \n",
            "2    105.1       57559                    48.31                          21   \n",
            "3    105.1       57559                    48.31                          21   \n",
            "4    105.1       57559                    48.31                          21   \n",
            "\n",
            "   QualityOfLifeAffordability  QualityOfLifeEconomy  \\\n",
            "0                          25                    14   \n",
            "1                          25                    14   \n",
            "2                          25                    14   \n",
            "3                          25                    14   \n",
            "4                          25                    14   \n",
            "\n",
            "   QualityOfLifeEducationAndHealth  QualityOfLifeSafety      id  \n",
            "0                               39                   40  290225  \n",
            "1                               39                   40  290226  \n",
            "2                               39                   40  290227  \n",
            "3                               39                   40  290228  \n",
            "4                               39                   40  290229  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "Enter your desired house price: 1\n",
            "Enter your income: 1\n",
            "Enter number of beds: 1\n",
            "Enter number of baths: 1\n",
            "Enter square footage: 1\n",
            "Enter desired population: 1\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Predicted house quality class (0 = low, 1 = high): 1\n",
            "Full dataset loaded (first 5 rows):\n",
            "  State        City  Bedroom  Bathroom    Area  ListedPrice Temperature  \\\n",
            "0    az  phoenix,az      3.0       1.0  1095.0       300000         Hot   \n",
            "1    az   tucson,az      NaN       NaN     NaN        65000         Hot   \n",
            "2    az   tucson,az      3.0       3.0  2645.0       825000         Hot   \n",
            "3    az   tucson,az      NaN       NaN     NaN        80000         Hot   \n",
            "4    az   tucson,az      3.0       2.0  1344.0       214500         Hot   \n",
            "\n",
            "   2022 Population  2016 Crime Rate  Unemployment  ...  Cost of Living  \\\n",
            "0          4551524            0.032          3.46  ...        82847.38   \n",
            "1          1057597            0.046          3.95  ...        70794.96   \n",
            "2          1057597            0.046          3.95  ...        70794.96   \n",
            "3          1057597            0.046          3.95  ...        70794.96   \n",
            "4          1057597            0.046          3.95  ...        70794.96   \n",
            "\n",
            "   AVG C2I  MeanIncome  QualityOfLifeTotalScore  QualityOfLifeQualityOfLife  \\\n",
            "0   105.10       57559                    48.31                          21   \n",
            "1   103.32       56039                    48.31                          21   \n",
            "2   103.32       56039                    48.31                          21   \n",
            "3   103.32       56039                    48.31                          21   \n",
            "4   103.32       56039                    48.31                          21   \n",
            "\n",
            "   QualityOfLifeAffordability  QualityOfLifeEconomy  \\\n",
            "0                          25                    14   \n",
            "1                          25                    14   \n",
            "2                          25                    14   \n",
            "3                          25                    14   \n",
            "4                          25                    14   \n",
            "\n",
            "   QualityOfLifeEducationAndHealth  QualityOfLifeSafety      id  \n",
            "0                               39                   40  290865  \n",
            "1                               39                   40  290866  \n",
            "2                               39                   40  290867  \n",
            "3                               39                   40  290868  \n",
            "4                               39                   40  290869  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "\n",
            "Closest Matching House Record:\n",
            "State: az\n",
            "City: tucson,az\n",
            "Listed Price: 74900\n",
            "Mean Income: 56039\n",
            "Bedrooms: 1.0\n",
            "Bathrooms: 1.0\n",
            "Area: 560.0\n",
            "2022 Population: 1057597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model"
      ],
      "metadata": {
        "id": "Jtd9pPrH6VMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from supabase import create_client, Client\n",
        "\n",
        "def fetch_housing_data() -> pd.DataFrame:\n",
        "    load_dotenv()  # Make sure your environment variables are set\n",
        "    SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
        "    SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
        "    client: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "    response = client.table(\"House\").select(\"*\").execute()\n",
        "    data = response.data\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# Import additional metrics from scikit-learn\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# scripts/train_model.py\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "def train_model():\n",
        "    # 1. Fetch data\n",
        "    df = fetch_housing_data()\n",
        "\n",
        "    # 2. Define features/targets\n",
        "    input_features = ['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']\n",
        "    target_features = ['QualityOfLifeTotalScore', 'Cost of Living', '2016 Crime Rate']\n",
        "\n",
        "    X = df[input_features].fillna(df[input_features].median())\n",
        "    y = df[target_features].fillna(df[target_features].median())\n",
        "\n",
        "    # 3. Scale\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # 4. Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 5. Build model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(len(target_features), input_shape=(X_train.shape[1],))\n",
        "    ])\n",
        "\n",
        "    # Change: Since there is only one Dense layer (output),\n",
        "    # use a single loss function and remove loss_weights\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    # 6. Train\n",
        "    model.fit(X_train, y_train, epochs=50, validation_split=0.1)\n",
        "\n",
        "    # 7. Evaluate\n",
        "    loss = model.evaluate(X_test, y_test)\n",
        "    print(\"Test loss:\", loss)\n",
        "\n",
        "    # 8. Save model & scaler if needed\n",
        "    model.save(\"trained_model.h5\")\n",
        "    # Optionally pickle the scaler for predictions\n",
        "    import joblib\n",
        "    joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8f0c8263-9ffa-4536-9808-4d04540ad07a",
        "id": "UohCPn7e6Snb"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 1670638976.0000 - val_loss: 1670643840.0000\n",
            "Epoch 2/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670642560.0000 - val_loss: 1670642688.0000\n",
            "Epoch 3/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670637568.0000 - val_loss: 1670641408.0000\n",
            "Epoch 4/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1670641152.0000 - val_loss: 1670640384.0000\n",
            "Epoch 5/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670638080.0000 - val_loss: 1670639360.0000\n",
            "Epoch 6/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670636288.0000 - val_loss: 1670638336.0000\n",
            "Epoch 7/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670633216.0000 - val_loss: 1670637312.0000\n",
            "Epoch 8/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670634368.0000 - val_loss: 1670636288.0000\n",
            "Epoch 9/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670632960.0000 - val_loss: 1670635136.0000\n",
            "Epoch 10/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670629888.0000 - val_loss: 1670634112.0000\n",
            "Epoch 11/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670632704.0000 - val_loss: 1670633088.0000\n",
            "Epoch 12/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670629760.0000 - val_loss: 1670631808.0000\n",
            "Epoch 13/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670628864.0000 - val_loss: 1670630784.0000\n",
            "Epoch 14/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1670630144.0000 - val_loss: 1670629632.0000\n",
            "Epoch 15/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670625920.0000 - val_loss: 1670628608.0000\n",
            "Epoch 16/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1670625024.0000 - val_loss: 1670627584.0000\n",
            "Epoch 17/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1670623104.0000 - val_loss: 1670626432.0000\n",
            "Epoch 18/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1670622080.0000 - val_loss: 1670625408.0000\n",
            "Epoch 19/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670624768.0000 - val_loss: 1670624256.0000\n",
            "Epoch 20/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670620544.0000 - val_loss: 1670623232.0000\n",
            "Epoch 21/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670622464.0000 - val_loss: 1670622208.0000\n",
            "Epoch 22/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670617984.0000 - val_loss: 1670620928.0000\n",
            "Epoch 23/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670615168.0000 - val_loss: 1670619904.0000\n",
            "Epoch 24/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670618368.0000 - val_loss: 1670618880.0000\n",
            "Epoch 25/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670613504.0000 - val_loss: 1670617856.0000\n",
            "Epoch 26/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1670616704.0000 - val_loss: 1670616832.0000\n",
            "Epoch 27/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670611200.0000 - val_loss: 1670615680.0000\n",
            "Epoch 28/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670613504.0000 - val_loss: 1670614656.0000\n",
            "Epoch 29/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670610560.0000 - val_loss: 1670613632.0000\n",
            "Epoch 30/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670610816.0000 - val_loss: 1670612352.0000\n",
            "Epoch 31/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670608640.0000 - val_loss: 1670611328.0000\n",
            "Epoch 32/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670606080.0000 - val_loss: 1670610176.0000\n",
            "Epoch 33/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 1670605184.0000 - val_loss: 1670609152.0000\n",
            "Epoch 34/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670604160.0000 - val_loss: 1670608128.0000\n",
            "Epoch 35/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670610816.0000 - val_loss: 1670607104.0000\n",
            "Epoch 36/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670603136.0000 - val_loss: 1670605952.0000\n",
            "Epoch 37/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670603776.0000 - val_loss: 1670604800.0000\n",
            "Epoch 38/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670602880.0000 - val_loss: 1670603776.0000\n",
            "Epoch 39/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670602880.0000 - val_loss: 1670602752.0000\n",
            "Epoch 40/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670597632.0000 - val_loss: 1670601472.0000\n",
            "Epoch 41/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 1670596992.0000 - val_loss: 1670600576.0000\n",
            "Epoch 42/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670595840.0000 - val_loss: 1670599424.0000\n",
            "Epoch 43/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670595072.0000 - val_loss: 1670598400.0000\n",
            "Epoch 44/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670594816.0000 - val_loss: 1670597376.0000\n",
            "Epoch 45/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670593024.0000 - val_loss: 1670596224.0000\n",
            "Epoch 46/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670597888.0000 - val_loss: 1670595200.0000\n",
            "Epoch 47/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 1670590720.0000 - val_loss: 1670594176.0000\n",
            "Epoch 48/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 11ms/step - loss: 1670592640.0000 - val_loss: 1670593152.0000\n",
            "Epoch 49/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1670590592.0000 - val_loss: 1670591872.0000\n",
            "Epoch 50/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 14ms/step - loss: 1670590592.0000 - val_loss: 1670590848.0000\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 10ms/step - loss: 1670589952.0000\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 1670590976.0\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}