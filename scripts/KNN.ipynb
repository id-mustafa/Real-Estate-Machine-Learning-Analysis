{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 1: DOWNLOADING DATA FROM SUPABASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloaded 1000 rows so far...\n",
      "Downloaded 2000 rows so far...\n",
      "Downloaded 3000 rows so far...\n",
      "Downloaded 4000 rows so far...\n",
      "Downloaded 5000 rows so far...\n",
      "Downloaded 6000 rows so far...\n",
      "Downloaded 7000 rows so far...\n",
      "Downloaded 8000 rows so far...\n",
      "Downloaded 9000 rows so far...\n",
      "Downloaded 10000 rows so far...\n",
      "Downloaded 11000 rows so far...\n",
      "Downloaded 12000 rows so far...\n",
      "Downloaded 13000 rows so far...\n",
      "Downloaded 14000 rows so far...\n",
      "Downloaded 15000 rows so far...\n",
      "Downloaded 16000 rows so far...\n",
      "Downloaded 17000 rows so far...\n",
      "Downloaded 18000 rows so far...\n",
      "Downloaded 19000 rows so far...\n",
      "Downloaded 20000 rows so far...\n",
      "Downloaded 21000 rows so far...\n",
      "Downloaded 22000 rows so far...\n",
      "Downloaded 23000 rows so far...\n",
      "Downloaded 24000 rows so far...\n",
      "Downloaded 25000 rows so far...\n",
      "Downloaded 26000 rows so far...\n",
      "Downloaded 27000 rows so far...\n",
      "Downloaded 28000 rows so far...\n",
      "Downloaded 29000 rows so far...\n",
      "Downloaded 30000 rows so far...\n",
      "Downloaded 31000 rows so far...\n",
      "Downloaded 32000 rows so far...\n",
      "Downloaded 33000 rows so far...\n",
      "Downloaded 34000 rows so far...\n",
      "Downloaded 35000 rows so far...\n",
      "Downloaded 36000 rows so far...\n",
      "Downloaded 37000 rows so far...\n",
      "Downloaded 38000 rows so far...\n",
      "Downloaded 39000 rows so far...\n",
      "Downloaded 40000 rows so far...\n",
      "Downloaded 41000 rows so far...\n",
      "Downloaded 42000 rows so far...\n",
      "Downloaded 43000 rows so far...\n",
      "Downloaded 44000 rows so far...\n",
      "Downloaded 45000 rows so far...\n",
      "Downloaded 46000 rows so far...\n",
      "Downloaded 47000 rows so far...\n",
      "Downloaded 48000 rows so far...\n",
      "Downloaded 49000 rows so far...\n",
      "Downloaded 50000 rows so far...\n",
      "Downloaded 51000 rows so far...\n",
      "Downloaded 52000 rows so far...\n",
      "Downloaded 53000 rows so far...\n",
      "Downloaded 54000 rows so far...\n",
      "Downloaded 55000 rows so far...\n",
      "Downloaded 56000 rows so far...\n",
      "Downloaded 57000 rows so far...\n",
      "Downloaded 58000 rows so far...\n",
      "Downloaded 59000 rows so far...\n",
      "Downloaded 60000 rows so far...\n",
      "Downloaded 61000 rows so far...\n",
      "Downloaded 62000 rows so far...\n",
      "Downloaded 63000 rows so far...\n",
      "Downloaded 64000 rows so far...\n",
      "Downloaded 65000 rows so far...\n",
      "Downloaded 66000 rows so far...\n",
      "Downloaded 67000 rows so far...\n",
      "Downloaded 68000 rows so far...\n",
      "Downloaded 69000 rows so far...\n",
      "Downloaded 70000 rows so far...\n",
      "Downloaded 71000 rows so far...\n",
      "Downloaded 72000 rows so far...\n",
      "Downloaded 73000 rows so far...\n",
      "Downloaded 74000 rows so far...\n",
      "Downloaded 75000 rows so far...\n",
      "Downloaded 76000 rows so far...\n",
      "Downloaded 77000 rows so far...\n",
      "Downloaded 78000 rows so far...\n",
      "Downloaded 79000 rows so far...\n",
      "Downloaded 80000 rows so far...\n",
      "Downloaded 81000 rows so far...\n",
      "Downloaded 82000 rows so far...\n",
      "Downloaded 83000 rows so far...\n",
      "Downloaded 84000 rows so far...\n",
      "Downloaded 85000 rows so far...\n",
      "Downloaded 86000 rows so far...\n",
      "Downloaded 87000 rows so far...\n",
      "Downloaded 88000 rows so far...\n",
      "Downloaded 89000 rows so far...\n",
      "Downloaded 90000 rows so far...\n",
      "Downloaded 91000 rows so far...\n",
      "Downloaded 92000 rows so far...\n",
      "Downloaded 93000 rows so far...\n",
      "Downloaded 94000 rows so far...\n",
      "Downloaded 95000 rows so far...\n",
      "Downloaded 96000 rows so far...\n",
      "Downloaded 97000 rows so far...\n",
      "Downloaded 98000 rows so far...\n",
      "Downloaded 99000 rows so far...\n",
      "Downloaded 100000 rows so far...\n",
      "Downloaded 101000 rows so far...\n",
      "Downloaded 102000 rows so far...\n",
      "Downloaded 103000 rows so far...\n",
      "Downloaded 104000 rows so far...\n",
      "Downloaded 105000 rows so far...\n",
      "Downloaded 106000 rows so far...\n",
      "Downloaded 107000 rows so far...\n",
      "Downloaded 108000 rows so far...\n",
      "Downloaded 109000 rows so far...\n",
      "Downloaded 110000 rows so far...\n",
      "Downloaded 111000 rows so far...\n",
      "Downloaded 112000 rows so far...\n",
      "Downloaded 113000 rows so far...\n",
      "Downloaded 114000 rows so far...\n",
      "Downloaded 115000 rows so far...\n",
      "Downloaded 116000 rows so far...\n",
      "Downloaded 117000 rows so far...\n",
      "Downloaded 118000 rows so far...\n",
      "Downloaded 119000 rows so far...\n",
      "Downloaded 120000 rows so far...\n",
      "Downloaded 121000 rows so far...\n",
      "Downloaded 122000 rows so far...\n",
      "Downloaded 123000 rows so far...\n",
      "Downloaded 124000 rows so far...\n",
      "Downloaded 125000 rows so far...\n",
      "Downloaded 126000 rows so far...\n",
      "Downloaded 127000 rows so far...\n",
      "Downloaded 128000 rows so far...\n",
      "Downloaded 129000 rows so far...\n",
      "Downloaded 130000 rows so far...\n",
      "Downloaded 131000 rows so far...\n",
      "Downloaded 132000 rows so far...\n",
      "Downloaded 133000 rows so far...\n",
      "Downloaded 134000 rows so far...\n",
      "Downloaded 135000 rows so far...\n",
      "Downloaded 136000 rows so far...\n",
      "Downloaded 137000 rows so far...\n",
      "Downloaded 138000 rows so far...\n",
      "Downloaded 139000 rows so far...\n",
      "Downloaded 140000 rows so far...\n",
      "Downloaded 141000 rows so far...\n",
      "Downloaded 142000 rows so far...\n",
      "Downloaded 143000 rows so far...\n",
      "Downloaded 144000 rows so far...\n",
      "Downloaded 145000 rows so far...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 34\u001b[39m\n\u001b[32m     27\u001b[39m \u001b[38;5;66;03m# Step 3: Pull batches\u001b[39;00m\n\u001b[32m     28\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m     29\u001b[39m     response = (\n\u001b[32m     30\u001b[39m         \u001b[43msupabase\u001b[49m\n\u001b[32m     31\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mtable\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mHousing Data 2\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m     32\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mrange\u001b[49m\u001b[43m(\u001b[49m\u001b[43moffset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moffset\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m \u001b[49m\u001b[43m-\u001b[49m\u001b[43m \u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m---> \u001b[39m\u001b[32m34\u001b[39m \u001b[43m        \u001b[49m\u001b[43m.\u001b[49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     35\u001b[39m     )\n\u001b[32m     36\u001b[39m     batch = response.data\n\u001b[32m     38\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m batch:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/postgrest/_sync/request_builder.py:58\u001b[39m, in \u001b[36mSyncQueryRequestBuilder.execute\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m     46\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mexecute\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> APIResponse[_ReturnT]:\n\u001b[32m     47\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Execute the query.\u001b[39;00m\n\u001b[32m     48\u001b[39m \n\u001b[32m     49\u001b[39m \u001b[33;03m    .. tip::\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     56\u001b[39m \u001b[33;03m        :class:`APIError` If the API raised an error.\u001b[39;00m\n\u001b[32m     57\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m58\u001b[39m     r = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msession\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mhttp_method\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m        \u001b[49m\u001b[43mjson\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mjson\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[43m        \u001b[49m\u001b[43mparams\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[43m        \u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mheaders\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     65\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     66\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m r.is_success:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpx/_client.py:825\u001b[39m, in \u001b[36mClient.request\u001b[39m\u001b[34m(self, method, url, content, data, files, json, params, headers, cookies, auth, follow_redirects, timeout, extensions)\u001b[39m\n\u001b[32m    810\u001b[39m     warnings.warn(message, \u001b[38;5;167;01mDeprecationWarning\u001b[39;00m, stacklevel=\u001b[32m2\u001b[39m)\n\u001b[32m    812\u001b[39m request = \u001b[38;5;28mself\u001b[39m.build_request(\n\u001b[32m    813\u001b[39m     method=method,\n\u001b[32m    814\u001b[39m     url=url,\n\u001b[32m   (...)\u001b[39m\u001b[32m    823\u001b[39m     extensions=extensions,\n\u001b[32m    824\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m825\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43msend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpx/_client.py:914\u001b[39m, in \u001b[36mClient.send\u001b[39m\u001b[34m(self, request, stream, auth, follow_redirects)\u001b[39m\n\u001b[32m    910\u001b[39m \u001b[38;5;28mself\u001b[39m._set_timeout(request)\n\u001b[32m    912\u001b[39m auth = \u001b[38;5;28mself\u001b[39m._build_request_auth(request, auth)\n\u001b[32m--> \u001b[39m\u001b[32m914\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_auth\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    915\u001b[39m \u001b[43m    \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    916\u001b[39m \u001b[43m    \u001b[49m\u001b[43mauth\u001b[49m\u001b[43m=\u001b[49m\u001b[43mauth\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    917\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    918\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    919\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    920\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    921\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m stream:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpx/_client.py:942\u001b[39m, in \u001b[36mClient._send_handling_auth\u001b[39m\u001b[34m(self, request, auth, follow_redirects, history)\u001b[39m\n\u001b[32m    939\u001b[39m request = \u001b[38;5;28mnext\u001b[39m(auth_flow)\n\u001b[32m    941\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m942\u001b[39m     response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_handling_redirects\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    943\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    944\u001b[39m \u001b[43m        \u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfollow_redirects\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    945\u001b[39m \u001b[43m        \u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhistory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    946\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    947\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    948\u001b[39m         \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpx/_client.py:979\u001b[39m, in \u001b[36mClient._send_handling_redirects\u001b[39m\u001b[34m(self, request, follow_redirects, history)\u001b[39m\n\u001b[32m    976\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mrequest\u001b[39m\u001b[33m\"\u001b[39m]:\n\u001b[32m    977\u001b[39m     hook(request)\n\u001b[32m--> \u001b[39m\u001b[32m979\u001b[39m response = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_send_single_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    980\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    981\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._event_hooks[\u001b[33m\"\u001b[39m\u001b[33mresponse\u001b[39m\u001b[33m\"\u001b[39m]:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpx/_client.py:1014\u001b[39m, in \u001b[36mClient._send_single_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m   1009\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\n\u001b[32m   1010\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mAttempted to send an async request with a sync Client instance.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1011\u001b[39m     )\n\u001b[32m   1013\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m request_context(request=request):\n\u001b[32m-> \u001b[39m\u001b[32m1014\u001b[39m     response = \u001b[43mtransport\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1016\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, SyncByteStream)\n\u001b[32m   1018\u001b[39m response.request = request\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpx/_transports/default.py:250\u001b[39m, in \u001b[36mHTTPTransport.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    237\u001b[39m req = httpcore.Request(\n\u001b[32m    238\u001b[39m     method=request.method,\n\u001b[32m    239\u001b[39m     url=httpcore.URL(\n\u001b[32m   (...)\u001b[39m\u001b[32m    247\u001b[39m     extensions=request.extensions,\n\u001b[32m    248\u001b[39m )\n\u001b[32m    249\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_httpcore_exceptions():\n\u001b[32m--> \u001b[39m\u001b[32m250\u001b[39m     resp = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_pool\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreq\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    252\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(resp.stream, typing.Iterable)\n\u001b[32m    254\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    255\u001b[39m     status_code=resp.status,\n\u001b[32m    256\u001b[39m     headers=resp.headers,\n\u001b[32m    257\u001b[39m     stream=ResponseStream(resp.stream),\n\u001b[32m    258\u001b[39m     extensions=resp.extensions,\n\u001b[32m    259\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:256\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    253\u001b[39m         closing = \u001b[38;5;28mself\u001b[39m._assign_requests_to_connections()\n\u001b[32m    255\u001b[39m     \u001b[38;5;28mself\u001b[39m._close_connections(closing)\n\u001b[32m--> \u001b[39m\u001b[32m256\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    258\u001b[39m \u001b[38;5;66;03m# Return the response. Note that in this case we still have to manage\u001b[39;00m\n\u001b[32m    259\u001b[39m \u001b[38;5;66;03m# the point at which the response is closed.\u001b[39;00m\n\u001b[32m    260\u001b[39m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(response.stream, typing.Iterable)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/connection_pool.py:236\u001b[39m, in \u001b[36mConnectionPool.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    232\u001b[39m connection = pool_request.wait_for_connection(timeout=timeout)\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;66;03m# Send the request on the assigned connection.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m236\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    237\u001b[39m \u001b[43m        \u001b[49m\u001b[43mpool_request\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrequest\u001b[49m\n\u001b[32m    238\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    239\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m ConnectionNotAvailable:\n\u001b[32m    240\u001b[39m     \u001b[38;5;66;03m# In some cases a connection may initially be available to\u001b[39;00m\n\u001b[32m    241\u001b[39m     \u001b[38;5;66;03m# handle a request, but then become unavailable.\u001b[39;00m\n\u001b[32m    242\u001b[39m     \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m    243\u001b[39m     \u001b[38;5;66;03m# In this case we clear the connection and try again.\u001b[39;00m\n\u001b[32m    244\u001b[39m     pool_request.clear_connection()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/connection.py:103\u001b[39m, in \u001b[36mHTTPConnection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    100\u001b[39m     \u001b[38;5;28mself\u001b[39m._connect_failed = \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[32m    101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m exc\n\u001b[32m--> \u001b[39m\u001b[32m103\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_connection\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhandle_request\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/http2.py:183\u001b[39m, in \u001b[36mHTTP2Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    179\u001b[39m     \u001b[38;5;66;03m# If h2 raises a protocol error in some other state then we\u001b[39;00m\n\u001b[32m    180\u001b[39m     \u001b[38;5;66;03m# must somehow have made a protocol violation.\u001b[39;00m\n\u001b[32m    181\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m LocalProtocolError(exc)  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m183\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m exc\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/http2.py:146\u001b[39m, in \u001b[36mHTTP2Connection.handle_request\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    142\u001b[39m     \u001b[38;5;28mself\u001b[39m._send_request_body(request=request, stream_id=stream_id)\n\u001b[32m    143\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m Trace(\n\u001b[32m    144\u001b[39m     \u001b[33m\"\u001b[39m\u001b[33mreceive_response_headers\u001b[39m\u001b[33m\"\u001b[39m, logger, request, kwargs\n\u001b[32m    145\u001b[39m ) \u001b[38;5;28;01mas\u001b[39;00m trace:\n\u001b[32m--> \u001b[39m\u001b[32m146\u001b[39m     status, headers = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_response\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    147\u001b[39m \u001b[43m        \u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m=\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_id\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstream_id\u001b[49m\n\u001b[32m    148\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    149\u001b[39m     trace.return_value = (status, headers)\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m Response(\n\u001b[32m    152\u001b[39m     status=status,\n\u001b[32m    153\u001b[39m     headers=headers,\n\u001b[32m   (...)\u001b[39m\u001b[32m    159\u001b[39m     },\n\u001b[32m    160\u001b[39m )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/http2.py:290\u001b[39m, in \u001b[36mHTTP2Connection._receive_response\u001b[39m\u001b[34m(self, request, stream_id)\u001b[39m\n\u001b[32m    286\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    287\u001b[39m \u001b[33;03mReturn the response status code and headers for a given stream ID.\u001b[39;00m\n\u001b[32m    288\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    289\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m290\u001b[39m     event = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_stream_event\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    291\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h2.events.ResponseReceived):\n\u001b[32m    292\u001b[39m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/http2.py:329\u001b[39m, in \u001b[36mHTTP2Connection._receive_stream_event\u001b[39m\u001b[34m(self, request, stream_id)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    324\u001b[39m \u001b[33;03mReturn the next available event for a given stream ID.\u001b[39;00m\n\u001b[32m    325\u001b[39m \n\u001b[32m    326\u001b[39m \u001b[33;03mWill read more data from the network if required.\u001b[39;00m\n\u001b[32m    327\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    328\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._events.get(stream_id):\n\u001b[32m--> \u001b[39m\u001b[32m329\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_receive_events\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream_id\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    330\u001b[39m event = \u001b[38;5;28mself\u001b[39m._events[stream_id].pop(\u001b[32m0\u001b[39m)\n\u001b[32m    331\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h2.events.StreamReset):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/http2.py:357\u001b[39m, in \u001b[36mHTTP2Connection._receive_events\u001b[39m\u001b[34m(self, request, stream_id)\u001b[39m\n\u001b[32m    350\u001b[39m \u001b[38;5;66;03m# This conditional is a bit icky. We don't want to block reading if we've\u001b[39;00m\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# actually got an event to return for a given stream. We need to do that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# check *within* the atomic read lock. Though it also need to be optional,\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# because when we call it from `_wait_for_outgoing_flow` we *do* want to\u001b[39;00m\n\u001b[32m    354\u001b[39m \u001b[38;5;66;03m# block until we've available flow control, event when we have events\u001b[39;00m\n\u001b[32m    355\u001b[39m \u001b[38;5;66;03m# pending for the stream ID we're attempting to send on.\u001b[39;00m\n\u001b[32m    356\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m stream_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._events.get(stream_id):\n\u001b[32m--> \u001b[39m\u001b[32m357\u001b[39m     events = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_read_incoming_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrequest\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    358\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m event \u001b[38;5;129;01min\u001b[39;00m events:\n\u001b[32m    359\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(event, h2.events.RemoteSettingsChanged):\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_sync/http2.py:432\u001b[39m, in \u001b[36mHTTP2Connection._read_incoming_data\u001b[39m\u001b[34m(self, request)\u001b[39m\n\u001b[32m    429\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m._read_exception  \u001b[38;5;66;03m# pragma: nocover\u001b[39;00m\n\u001b[32m    431\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m432\u001b[39m     data = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_network_stream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mREAD_NUM_BYTES\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    433\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m data == \u001b[33mb\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m    434\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m RemoteProtocolError(\u001b[33m\"\u001b[39m\u001b[33mServer disconnected\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/httpcore/_backends/sync.py:128\u001b[39m, in \u001b[36mSyncStream.read\u001b[39m\u001b[34m(self, max_bytes, timeout)\u001b[39m\n\u001b[32m    126\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m map_exceptions(exc_map):\n\u001b[32m    127\u001b[39m     \u001b[38;5;28mself\u001b[39m._sock.settimeout(timeout)\n\u001b[32m--> \u001b[39m\u001b[32m128\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1285\u001b[39m, in \u001b[36mSSLSocket.recv\u001b[39m\u001b[34m(self, buflen, flags)\u001b[39m\n\u001b[32m   1281\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m flags != \u001b[32m0\u001b[39m:\n\u001b[32m   1282\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1283\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mnon-zero flags not allowed in calls to recv() on \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m\"\u001b[39m %\n\u001b[32m   1284\u001b[39m             \u001b[38;5;28mself\u001b[39m.\u001b[34m__class__\u001b[39m)\n\u001b[32m-> \u001b[39m\u001b[32m1285\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbuflen\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1286\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1287\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().recv(buflen, flags)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ssl.py:1140\u001b[39m, in \u001b[36mSSLSocket.read\u001b[39m\u001b[34m(self, len, buffer)\u001b[39m\n\u001b[32m   1138\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._sslobj.read(\u001b[38;5;28mlen\u001b[39m, buffer)\n\u001b[32m   1139\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1140\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sslobj\u001b[49m\u001b[43m.\u001b[49m\u001b[43mread\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1141\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m SSLError \u001b[38;5;28;01mas\u001b[39;00m x:\n\u001b[32m   1142\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m x.args[\u001b[32m0\u001b[39m] == SSL_ERROR_EOF \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.suppress_ragged_eofs:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from supabase import create_client, Client\n",
    "from dotenv import load_dotenv\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "# Step 1: Connect to Supabase\n",
    "url = os.getenv(\"SUPABASE_URL\")\n",
    "key = os.getenv(\"SUPABASE_KEY\")\n",
    "supabase: Client = create_client(url, key)\n",
    "\n",
    "# Step 2: Set up variables\n",
    "batch_size = 5000\n",
    "offset = 0\n",
    "all_rows = []\n",
    "total_rows_downloaded = 0\n",
    "\n",
    "columns = ','.join([\n",
    "    'Bedroom', 'Bathroom', 'Area', 'LotArea', 'Price',\n",
    "    '\"2022 Median Income\"', 'Temperature', 'Population',\n",
    "    'City', '\"AQI%Good\"', 'WaterQualityVPV', 'Unemployment',\n",
    "    '\"2016 Crime Rate\"', '\"Cost of Living\"'\n",
    "])\n",
    "\n",
    "# Step 3: Pull batches\n",
    "while True:\n",
    "    response = (\n",
    "        supabase\n",
    "        .table('Housing Data 2')\n",
    "        .select(columns)\n",
    "        .range(offset, offset + batch_size - 1)\n",
    "        .execute()\n",
    "    )\n",
    "    batch = response.data\n",
    "\n",
    "    if not batch:\n",
    "        break\n",
    "\n",
    "    all_rows.extend(batch)\n",
    "    batch_size_real = len(batch)\n",
    "    total_rows_downloaded += batch_size_real\n",
    "    offset += batch_size_real\n",
    "\n",
    "    print(f\"Downloaded {total_rows_downloaded} rows so far...\")   # Real progress\n",
    "\n",
    "# Step 4: Build DataFrame\n",
    "merged_df = pd.DataFrame(all_rows)\n",
    "\n",
    "# Step 5: Save\n",
    "merged_df.to_csv('merged_data.csv', index=False)\n",
    "\n",
    "print(f\"Finished! Total rows pulled: {len(merged_df)}\")\n",
    "\n",
    "merged_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1.5: Storing fresh state of merged_df in merged_df2 so that we can refresh the dataframe after potentially modifying it in accuracy tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "merged_df2 = merged_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: CREATING FIRST KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended city: albuquerque,nm\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reset dataframe\n",
    "merged_df = merged_df2.copy()\n",
    "\n",
    "# Select features for clustering\n",
    "feature_cols = ['Bedroom', 'Bathroom', 'Area', 'LotArea', 'Price', '2022 Median Income', 'Temperature', 'Population']\n",
    "quality_cols = ['AQI%Good', 'WaterQualityVPV', 'Unemployment', '2016 Crime Rate', 'Cost of Living']\n",
    "# Note: Curious about whether or not we should have Cost of Living be an inputted feature variable or if it should just be something that's generally minimized\n",
    "\n",
    "# Preprocess: scale the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(merged_df[feature_cols])\n",
    "# X = merged_df[feature_cols].values\n",
    "\n",
    "# Get unique cities and map them\n",
    "cities = merged_df['City'].unique()\n",
    "city_to_index = {city: idx for idx, city in enumerate(cities)}\n",
    "index_to_city = {idx: city for city, idx in city_to_index.items()}\n",
    "\n",
    "# Assign city labels\n",
    "y = merged_df['City'].map(city_to_index)\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "def recommend_city(user_input, visualize=True):\n",
    "    # user_input: includes Bedroom, Bathroom, Area, LotArea, Price, Temperature, 2022 Median Income, Population\n",
    "    temp_mapping = {'Cold': 0, 'Medium': 1, 'Hot': 2}\n",
    "    \n",
    "    user_features = np.array([\n",
    "        user_input['Bedroom'],\n",
    "        user_input['Bathroom'],\n",
    "        user_input['Area'],\n",
    "        user_input['LotArea'],\n",
    "        user_input['Price'],\n",
    "        user_input['2022 Median Income'],\n",
    "        temp_mapping[user_input['Temperature']],  # map temp string to number\n",
    "        user_input['Population']\n",
    "    ]).reshape(1, -1)\n",
    "\n",
    "    user_features_scaled = scaler.transform(user_features)\n",
    "\n",
    "    # Calculate Euclidean distances\n",
    "    distances = np.linalg.norm(X - user_features_scaled, axis=1)\n",
    "\n",
    "    # Find k nearest neighbors\n",
    "    k = 5\n",
    "    top_k_indices = np.argsort(distances)[:k]\n",
    "\n",
    "    # Look up corresponding cities\n",
    "    neighbor_cities = y.iloc[top_k_indices].values\n",
    "\n",
    "    # Tally up most common city\n",
    "    unique, counts = np.unique(neighbor_cities, return_counts=True)\n",
    "    candidate_cities = [index_to_city[idx] for idx in unique]\n",
    "\n",
    "    # Filter original df for these candidates\n",
    "    candidates_df = merged_df[merged_df['City'].isin(candidate_cities)].copy()\n",
    "\n",
    "    # Score candidates based on quality metrics (Increasing or decreasing distance based on QoL features)\n",
    "    candidates_df['QualityScore'] = (\n",
    "        candidates_df['AQI%Good'] + candidates_df['WaterQualityVPV']\n",
    "        - candidates_df['Unemployment'] * 2 - candidates_df['2016 Crime Rate'] * 2\n",
    "        - candidates_df['Cost of Living'] * 1.5\n",
    "    )\n",
    "\n",
    "    # Group by city and take the best average score\n",
    "    city_scores = candidates_df.groupby('City')['QualityScore'].mean()\n",
    "\n",
    "    # Best city\n",
    "    best_city = city_scores.idxmax()\n",
    "\n",
    "    visualize = False\n",
    "    if visualize:\n",
    "        # Visualize\n",
    "        user_2d = pca.transform(user_features_scaled)\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sample_size = 5000\n",
    "        if len(merged_df) > sample_size:\n",
    "            sampled_indices = np.random.choice(len(merged_df), size=sample_size, replace=False)\n",
    "            sampled_df = merged_df.iloc[sampled_indices]\n",
    "            X_2d_sampled = X_2d[sampled_indices]\n",
    "        else:\n",
    "            sampled_df = merged_df\n",
    "            X_2d_sampled = X_2d\n",
    "\n",
    "        for city in sampled_df['City'].unique():\n",
    "            mask = sampled_df['City'] == city\n",
    "            plt.scatter(X_2d_sampled[mask, 0], X_2d_sampled[mask, 1], label=city, alpha=0.5)\n",
    "\n",
    "        plt.scatter(user_2d[0, 0], user_2d[0, 1], color='black', marker='X', s=200, label='User Input')\n",
    "\n",
    "        for idx in top_k_indices:\n",
    "            plt.plot([user_2d[0, 0], X_2d[idx, 0]], [user_2d[0, 1], X_2d[idx, 1]], 'k--', alpha=0.7)\n",
    "\n",
    "        plt.title(f'Recommended City: {best_city}')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return best_city\n",
    "\n",
    "# Example user input\n",
    "user_input = {\n",
    "    'Bedroom': 4,\n",
    "    'Bathroom': 4,\n",
    "    'Area': 1500,\n",
    "    'LotArea': 5000,\n",
    "    'Price': 100000,\n",
    "    '2022 Median Income': 70000,\n",
    "    'Temperature': 'Medium',   # Now passed as text\n",
    "    'Population': 150000     # New: user estimates size of city they want\n",
    "}\n",
    "\n",
    "best_city = recommend_city(user_input, visualize=True)\n",
    "print(f\"Recommended city: {best_city}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended city: colorado springs,co\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "user_input = {\n",
    "    'Bedroom': 4,\n",
    "    'Bathroom': 4,\n",
    "    'Area': 1500,\n",
    "    'LotArea': 5000,\n",
    "    'Price': 50000,\n",
    "    '2022 Median Income': 100000,\n",
    "    'Temperature': 'Cold',   # Now passed as text\n",
    "    'Population': 1000000     # New: user estimates size of city they want\n",
    "}\n",
    "\n",
    "best_city = recommend_city(user_input, visualize=True)\n",
    "print(f\"Recommended city: {best_city}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 3: TESTING ACCURACY\n",
    "\n",
    "Note: This first trial tests the accuracy of the features for each row in the testing data returning the specific city that is actually assigned to them. Since our model does not take in quality of life features as inputted Features, but instead tries to pick a city that generally minimizes or maximizes them (depending on the specific QoL feature) by default, this will return a relatively low accuracy since certain cities will have lower quality of life ratings and so will not be returned by the Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 32\u001b[39m\n\u001b[32m     30\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(total):\n\u001b[32m     31\u001b[39m     user_features_scaled = X_test_sampled[i].reshape(\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)  \u001b[38;5;66;03m# numpy indexing\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m32\u001b[39m     predicted_idx = \u001b[43mrecommend_city_from_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43muser_features_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     33\u001b[39m     actual_idx = y_test_sampled.iloc[i]  \u001b[38;5;66;03m# pandas indexing\u001b[39;00m\n\u001b[32m     35\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m predicted_idx == actual_idx:\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 20\u001b[39m, in \u001b[36mrecommend_city_from_train\u001b[39m\u001b[34m(user_features_scaled, X_train, y_train, k)\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mrecommend_city_from_train\u001b[39m(user_features_scaled, X_train, y_train, k=\u001b[32m5\u001b[39m):\n\u001b[32m     18\u001b[39m     distances = np.linalg.norm(X_train - user_features_scaled, axis=\u001b[32m1\u001b[39m)\n\u001b[32m---> \u001b[39m\u001b[32m20\u001b[39m     top_k_indices = \u001b[43mnp\u001b[49m\u001b[43m.\u001b[49m\u001b[43margsort\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdistances\u001b[49m\u001b[43m)\u001b[49m[:k]\n\u001b[32m     21\u001b[39m     neighbor_cities = y_train.iloc[top_k_indices].values\n\u001b[32m     22\u001b[39m     unique, counts = np.unique(neighbor_cities, return_counts=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:1243\u001b[39m, in \u001b[36margsort\u001b[39m\u001b[34m(a, axis, kind, order, stable)\u001b[39m\n\u001b[32m   1130\u001b[39m \u001b[38;5;129m@array_function_dispatch\u001b[39m(_argsort_dispatcher)\n\u001b[32m   1131\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34margsort\u001b[39m(a, axis=-\u001b[32m1\u001b[39m, kind=\u001b[38;5;28;01mNone\u001b[39;00m, order=\u001b[38;5;28;01mNone\u001b[39;00m, *, stable=\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[32m   1132\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   1133\u001b[39m \u001b[33;03m    Returns the indices that would sort an array.\u001b[39;00m\n\u001b[32m   1134\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   1241\u001b[39m \n\u001b[32m   1242\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1243\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m        \u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43margsort\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m=\u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkind\u001b[49m\u001b[43m=\u001b[49m\u001b[43mkind\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mstable\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/numpy/_core/fromnumeric.py:57\u001b[39m, in \u001b[36m_wrapfunc\u001b[39m\u001b[34m(obj, method, *args, **kwds)\u001b[39m\n\u001b[32m     54\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n\u001b[32m     56\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m57\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[32m     59\u001b[39m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[32m     60\u001b[39m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m     64\u001b[39m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[32m     65\u001b[39m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[32m     66\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, *args, **kwds)\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split data (assuming Temperature is already part of X)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.05, random_state=42)\n",
    "\n",
    "# Important: fix randomness\n",
    "np.random.seed(42)\n",
    "\n",
    "# Sample 500 test points\n",
    "sample_size = 500\n",
    "sample_indices = np.random.choice(len(X_test), size=sample_size, replace=False)\n",
    "\n",
    "X_test_sampled = X_test[sample_indices]        # numpy array indexing\n",
    "y_test_sampled = y_test.iloc[sample_indices]   # pandas Series positional indexing\n",
    "\n",
    "# Recommendation function â€” no temperature penalty needed\n",
    "def recommend_city_from_train(user_features_scaled, X_train, y_train, k=5):\n",
    "    distances = np.linalg.norm(X_train - user_features_scaled, axis=1)\n",
    "    \n",
    "    top_k_indices = np.argsort(distances)[:k]\n",
    "    neighbor_cities = y_train.iloc[top_k_indices].values\n",
    "    unique, counts = np.unique(neighbor_cities, return_counts=True)\n",
    "    best_city_idx = unique[np.argmax(counts)]\n",
    "    return best_city_idx\n",
    "\n",
    "# Evaluate accuracy on the sampled test set\n",
    "correct = 0\n",
    "total = len(X_test_sampled)\n",
    "\n",
    "for i in range(total):\n",
    "    user_features_scaled = X_test_sampled[i].reshape(1, -1)  # numpy indexing\n",
    "    predicted_idx = recommend_city_from_train(user_features_scaled, X_train, y_train)\n",
    "    actual_idx = y_test_sampled.iloc[i]  # pandas indexing\n",
    "\n",
    "    if predicted_idx == actual_idx:\n",
    "        correct += 1\n",
    "\n",
    "accuracy = correct / total\n",
    "print(f\"Sampled model accuracy (on {sample_size} points): {accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 4: TESTING ACCURACY WITHOUT SIMPLY MAXIMIZING QUALITY OF LIFE FEATURES\n",
    "\n",
    "Now, we tinker with constructing our model to recieve QoL features as input, so that we can see how good it is at generally predicting data. If this model can return a high accuracy, we know our KNN Algorithm is working properly to at least predict the cities accurately. However, for the final model, we will change it back to maximizing or minimizing certain QoL values because we are not just trying to predict a city but are trying to give the user the best option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure Matching Model Accuracy: 0.85\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reset dataframe\n",
    "merged_df = merged_df2.copy()\n",
    "\n",
    "# Step 1: Prepare feature set that includes quality of life features + temperature + population\n",
    "full_feature_cols = ['Bedroom', 'Bathroom', 'Area', 'LotArea', 'Price', '2022 Median Income',\n",
    "                     'AQI%Good', 'WaterQualityVPV', 'Unemployment', '2016 Crime Rate', 'Cost of Living', \n",
    "                     'Temperature', 'Population'] \n",
    "\n",
    "# Preprocess: scale the full features\n",
    "scaler_full = StandardScaler()\n",
    "X_full = scaler_full.fit_transform(merged_df[full_feature_cols])\n",
    "# X_full = merged_df[full_feature_cols].values\n",
    "\n",
    "# Labels stay the same\n",
    "y_full = merged_df['City'].map(city_to_index)\n",
    "\n",
    "# Step 2: Train/test split\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_full, y_full, test_size=0.05, random_state=42)\n",
    "\n",
    "# Important: fix randomness\n",
    "np.random.seed(42)\n",
    "\n",
    "# Optional: sample 500 points to keep speed reasonable\n",
    "sample_size = 500\n",
    "sample_indices = np.random.choice(len(X_test_full), size=sample_size, replace=False)\n",
    "X_test_sampled_full = X_test_full[sample_indices]\n",
    "y_test_sampled_full = y_test_full.iloc[sample_indices]\n",
    "\n",
    "# Step 3: Define a pure recommend function\n",
    "def pure_recommend_city(user_features_scaled, X_train, y_train, k=5):\n",
    "    distances = np.linalg.norm(X_train - user_features_scaled, axis=1)\n",
    "\n",
    "    top_k_indices = np.argsort(distances)[:k]\n",
    "    neighbor_cities = y_train.iloc[top_k_indices].values\n",
    "    unique, counts = np.unique(neighbor_cities, return_counts=True)\n",
    "    best_city_idx = unique[np.argmax(counts)]\n",
    "    return best_city_idx\n",
    "\n",
    "# Step 4: Evaluate pure matching accuracy\n",
    "correct = 0\n",
    "total = len(X_test_sampled_full)\n",
    "\n",
    "for i in range(total):\n",
    "    user_features_scaled = X_test_sampled_full[i].reshape(1, -1)\n",
    "    predicted_idx = pure_recommend_city(user_features_scaled, X_train_full, y_train_full)\n",
    "    actual_idx = y_test_sampled_full.iloc[i]\n",
    "\n",
    "    if predicted_idx == actual_idx:\n",
    "        correct += 1\n",
    "\n",
    "pure_accuracy = correct / total\n",
    "print(f\"Pure Matching Model Accuracy: {pure_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 5: TUNING MODEL PARAMETERS AND SCALARS TO MAXIMIZE ACCURACY\n",
    "\n",
    "- Now, we try to tune the model for higher accuracy by increasing the weights of important, city-defining features, dropping noisy features, increasing the value of k, and other tweaks\n",
    "\n",
    "- Once we find the modifications that maximize the model's accuracy without QoL optimization, we can re-implement these tweaks into the regular model which DOES maximize QoL Values, as we know that these results will be more accurate to the user's inputs while still maximizing better quality of life."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pure Matching Tuned Model Accuracy: 0.89\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reset dataframe\n",
    "merged_df = merged_df2.copy()\n",
    "\n",
    "# Step 1: Prepare feature set that includes quality of life features + temperature + population\n",
    "full_feature_cols = ['Bedroom', 'Bathroom', 'Area', 'LotArea', 'Price', '2022 Median Income',\n",
    "                     'AQI%Good', 'WaterQualityVPV', 'Unemployment', '2016 Crime Rate', 'Cost of Living', \n",
    "                     'Temperature', 'Population']\n",
    "\n",
    "# merged_df[\"2022 Median Income\"] *= 1\n",
    "# merged_df[\"Cost of Living\"] *= 2.5\n",
    "# merged_df[\"Population\"] *= 2\n",
    "# merged_df[\"Temperature\"] *= 2.5 \n",
    "#merged_df['Bedroom'] *= 1.5\n",
    "#merged_df['Bathroom'] *= 1.5\n",
    "#merged_df['Area'] *= 5\n",
    "# merged_df['LotArea'] *= 1.5\n",
    "\n",
    "# Preprocess: scale the full features\n",
    "scaler_full = StandardScaler()\n",
    "X_full = scaler_full.fit_transform(merged_df[full_feature_cols])\n",
    "# X_full = merged_df[full_feature_cols].values\n",
    "\n",
    "# Weigh features to prioritize more important, impactful ones\n",
    "# feature_weights = np.array([1, 1, 1, 1, 1, 2.5, 1, 1, 1, 1, 2, 3, 2])\n",
    "feature_weights = np.array([1, 1, 1, 1.5, 1, 4, 1, 1, 1, 1, 2.5, 3, 3])\n",
    "\n",
    "\n",
    "# Labels stay the same\n",
    "y_full = merged_df['City'].map(city_to_index)\n",
    "\n",
    "# Step 2: Train/test split\n",
    "X_train_full, X_test_full, y_train_full, y_test_full = train_test_split(X_full, y_full, test_size=0.05, random_state=42)\n",
    "\n",
    "# Important: fix randomness\n",
    "np.random.seed(42)\n",
    "\n",
    "# Optional: sample 500 points to keep speed reasonable\n",
    "sample_size = 500\n",
    "sample_indices = np.random.choice(len(X_test_full), size=sample_size, replace=False)\n",
    "X_test_sampled_full = X_test_full[sample_indices]\n",
    "y_test_sampled_full = y_test_full.iloc[sample_indices]\n",
    "\n",
    "# Step 3: Define a pure recommend function (NO temperature penalty anymore)\n",
    "def pure_recommend_city(user_features_scaled, X_train, y_train, k=5):\n",
    "    weighted_diff = (X_train - user_features_scaled) * feature_weights\n",
    "    distances = np.linalg.norm(weighted_diff, axis=1)\n",
    "\n",
    "    top_k_indices = np.argsort(distances)[:k]\n",
    "    neighbor_cities = y_train.iloc[top_k_indices].values\n",
    "    unique, counts = np.unique(neighbor_cities, return_counts=True)\n",
    "    best_city_idx = unique[np.argmax(counts)]\n",
    "    return best_city_idx\n",
    "\n",
    "# Step 4: Evaluate pure matching accuracy\n",
    "correct = 0\n",
    "total = len(X_test_sampled_full)\n",
    "\n",
    "for i in range(total):\n",
    "    user_features_scaled = X_test_sampled_full[i].reshape(1, -1)\n",
    "    predicted_idx = pure_recommend_city(user_features_scaled, X_train_full, y_train_full)\n",
    "    actual_idx = y_test_sampled_full.iloc[i]\n",
    "\n",
    "    if predicted_idx == actual_idx:\n",
    "        correct += 1\n",
    "\n",
    "pure_accuracy = correct / total\n",
    "print(f\"Pure Matching Tuned Model Accuracy: {pure_accuracy:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 6: RECONSTRUCTING A MORE ACCURATE KNN\n",
    "\n",
    "- Now, we reconstruct the KNN with these modifications to get our final, most accurate KNN which prioritizes Quality of Life values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended city: conover,nc\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Reset dataframe\n",
    "merged_df = merged_df2.copy()\n",
    "\n",
    "# Select features for clustering\n",
    "feature_cols = ['Bedroom', 'Bathroom', 'Area', 'LotArea', 'Price', '2022 Median Income', 'Temperature', 'Population']\n",
    "quality_cols = ['AQI%Good', 'WaterQualityVPV', 'Unemployment', '2016 Crime Rate', 'Cost of Living']\n",
    "# Note: Curious about whether or not we should have Cost of Living be an inputted feature variable or if it should just be something that's generally minimized\n",
    "\n",
    "# Preprocess: scale the features\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(merged_df[feature_cols])\n",
    "# X = merged_df[feature_cols].values\n",
    "\n",
    "# Weigh features to prioritize more important, impactful ones\n",
    "feature_weights = np.array([1, 1, 1, 1.5, 1, 4, 3, 3])\n",
    "\n",
    "# Get unique cities and map them\n",
    "cities = merged_df['City'].unique()\n",
    "city_to_index = {city: idx for idx, city in enumerate(cities)}\n",
    "index_to_city = {idx: city for city, idx in city_to_index.items()}\n",
    "\n",
    "# Assign city labels\n",
    "y = merged_df['City'].map(city_to_index)\n",
    "\n",
    "# Reduce dimensions for visualization\n",
    "pca = PCA(n_components=2)\n",
    "X_2d = pca.fit_transform(X)\n",
    "\n",
    "def recommend_city(user_input, visualize=True):\n",
    "    # user_input: includes Bedroom, Bathroom, Area, LotArea, Price, Temperature, 2022 Median Income, Population\n",
    "    temp_mapping = {'Cold': 0, 'Medium': 1, 'Hot': 2}\n",
    "    \n",
    "    user_features = np.array([\n",
    "        user_input['Bedroom'],\n",
    "        user_input['Bathroom'],\n",
    "        user_input['Area'],\n",
    "        user_input['LotArea'],\n",
    "        user_input['Price'],\n",
    "        user_input['2022 Median Income'],\n",
    "        temp_mapping[user_input['Temperature']],  # map temp string to number\n",
    "        user_input['Population']\n",
    "    ]).reshape(1, -1)\n",
    "\n",
    "    user_features_scaled = scaler.transform(user_features)\n",
    "\n",
    "    # Calculate Euclidean distances\n",
    "    weighted_diff = (X_train - user_features_scaled) * feature_weights\n",
    "    distances = np.linalg.norm(weighted_diff, axis=1)\n",
    "\n",
    "    # Find k nearest neighbors\n",
    "    k = 5\n",
    "    top_k_indices = np.argsort(distances)[:k]\n",
    "\n",
    "    # Look up corresponding cities\n",
    "    neighbor_cities = y.iloc[top_k_indices].values\n",
    "\n",
    "    # Tally up most common city\n",
    "    unique, counts = np.unique(neighbor_cities, return_counts=True)\n",
    "    candidate_cities = [index_to_city[idx] for idx in unique]\n",
    "\n",
    "    # Filter original df for these candidates\n",
    "    candidates_df = merged_df[merged_df['City'].isin(candidate_cities)].copy()\n",
    "\n",
    "    # Score candidates based on quality metrics (Increasing or decreasing distance based on QoL features)\n",
    "    candidates_df['QualityScore'] = (\n",
    "        candidates_df['AQI%Good'] + candidates_df['WaterQualityVPV']\n",
    "        - candidates_df['Unemployment'] * 2\n",
    "        - candidates_df['2016 Crime Rate'] * 2\n",
    "        - candidates_df['Cost of Living'] * 1.5\n",
    "    )\n",
    "\n",
    "    # Group by city and take the best average score\n",
    "    city_scores = candidates_df.groupby('City')['QualityScore'].mean()\n",
    "\n",
    "    # Best city\n",
    "    best_city = city_scores.idxmax()\n",
    "\n",
    "    visualize = False\n",
    "    if visualize:\n",
    "        # Visualize\n",
    "        user_2d = pca.transform(user_features_scaled)\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sample_size = 5000\n",
    "        if len(merged_df) > sample_size:\n",
    "            sampled_indices = np.random.choice(len(merged_df), size=sample_size, replace=False)\n",
    "            sampled_df = merged_df.iloc[sampled_indices]\n",
    "            X_2d_sampled = X_2d[sampled_indices]\n",
    "        else:\n",
    "            sampled_df = merged_df\n",
    "            X_2d_sampled = X_2d\n",
    "\n",
    "        for city in sampled_df['City'].unique():\n",
    "            mask = sampled_df['City'] == city\n",
    "            plt.scatter(X_2d_sampled[mask, 0], X_2d_sampled[mask, 1], label=city, alpha=0.5)\n",
    "\n",
    "        plt.scatter(user_2d[0, 0], user_2d[0, 1], color='black', marker='X', s=200, label='User Input')\n",
    "\n",
    "        for idx in top_k_indices:\n",
    "            plt.plot([user_2d[0, 0], X_2d[idx, 0]], [user_2d[0, 1], X_2d[idx, 1]], 'k--', alpha=0.7)\n",
    "\n",
    "        plt.title(f'Recommended City: {best_city}')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return best_city\n",
    "\n",
    "# Example user input\n",
    "user_input = {\n",
    "    'Bedroom': 3,\n",
    "    'Bathroom': 2,\n",
    "    'Area': 2500,\n",
    "    'LotArea': 5000,\n",
    "    'Price': 1000000,\n",
    "    '2022 Median Income': 160000,\n",
    "    'Temperature': 'Cold',   # Now passed as text\n",
    "    'Population': 500000     # New: user estimates size of city they want\n",
    "}\n",
    "\n",
    "best_city = recommend_city(user_input, visualize=True)\n",
    "print(f\"Recommended city: {best_city}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 7: CREATING A KNN WHICH CAN WORK FOR MISSING USER INPUTS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First Trial: KNN will fill missing features with zeroes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_city(user_input, visualize=True):\n",
    "    temp_mapping = {'Cold': 0, 'Medium': 1, 'Hot': 2}\n",
    "\n",
    "    # Mapping feature names to their corresponding order in feature_cols and feature_weights\n",
    "    feature_to_index = {name: idx for idx, name in enumerate(feature_cols)}\n",
    "\n",
    "    # Start with a dummy full 8-feature array (in original feature order)\n",
    "    user_feature_full = np.zeros(len(feature_cols))\n",
    "\n",
    "    provided_features = []\n",
    "    provided_indices = []\n",
    "\n",
    "    for feature in user_input:\n",
    "        if feature not in feature_to_index:\n",
    "            continue\n",
    "        idx = feature_to_index[feature]\n",
    "        value = user_input[feature]\n",
    "        if feature == 'Temperature':\n",
    "            value = temp_mapping[value]\n",
    "        user_feature_full[idx] = value\n",
    "        provided_features.append(feature)\n",
    "        provided_indices.append(idx)\n",
    "\n",
    "    # Scale the full feature vector (8 features)\n",
    "    user_features_scaled_full = scaler.transform(user_feature_full.reshape(1, -1))\n",
    "\n",
    "    # Select only the provided features after scaling\n",
    "    user_features_scaled = user_features_scaled_full[:, provided_indices]\n",
    "\n",
    "    # Also reduce the full X to match only provided features\n",
    "    X_subset = X[:, provided_indices]\n",
    "\n",
    "    # Adjust feature weights\n",
    "    feature_weights_subset = feature_weights[provided_indices]\n",
    "\n",
    "    # Calculate weighted distances\n",
    "    weighted_diff = (X_subset - user_features_scaled) * feature_weights_subset\n",
    "    distances = np.linalg.norm(weighted_diff, axis=1)\n",
    "\n",
    "    # Find k nearest neighbors\n",
    "    k = 5\n",
    "    top_k_indices = np.argsort(distances)[:k]\n",
    "\n",
    "    # Look up corresponding cities\n",
    "    neighbor_cities = y.iloc[top_k_indices].values\n",
    "\n",
    "    unique, counts = np.unique(neighbor_cities, return_counts=True)\n",
    "    candidate_cities = [index_to_city[idx] for idx in unique]\n",
    "\n",
    "    candidates_df = merged_df[merged_df['City'].isin(candidate_cities)].copy()\n",
    "\n",
    "    # Score candidates\n",
    "    candidates_df['QualityScore'] = (\n",
    "        candidates_df['AQI%Good'] + candidates_df['WaterQualityVPV']\n",
    "        - candidates_df['Unemployment'] * 2\n",
    "        - candidates_df['2016 Crime Rate'] * 2\n",
    "        - candidates_df['Cost of Living'] * 1.5\n",
    "    )\n",
    "\n",
    "    city_scores = candidates_df.groupby('City')['QualityScore'].mean()\n",
    "\n",
    "    best_city = city_scores.idxmax()\n",
    "\n",
    "    visualize = False\n",
    "    if visualize:\n",
    "        user_2d = pca.transform(user_features_scaled_full)  # Now safe to transform full\n",
    "\n",
    "        plt.figure(figsize=(10, 7))\n",
    "        sample_size = 5000\n",
    "        if len(merged_df) > sample_size:\n",
    "            sampled_indices = np.random.choice(len(merged_df), size=sample_size, replace=False)\n",
    "            sampled_df = merged_df.iloc[sampled_indices]\n",
    "            X_2d_sampled = X_2d[sampled_indices]\n",
    "        else:\n",
    "            sampled_df = merged_df\n",
    "            X_2d_sampled = X_2d\n",
    "\n",
    "        for city in sampled_df['City'].unique():\n",
    "            mask = sampled_df['City'] == city\n",
    "            plt.scatter(X_2d_sampled[mask, 0], X_2d_sampled[mask, 1], label=city, alpha=0.5)\n",
    "\n",
    "        plt.scatter(user_2d[0, 0], user_2d[0, 1], color='black', marker='X', s=200, label='User Input')\n",
    "\n",
    "        for idx in top_k_indices:\n",
    "            plt.plot([user_2d[0, 0], X_2d[idx, 0]], [user_2d[0, 1], X_2d[idx, 1]], 'k--', alpha=0.7)\n",
    "\n",
    "        plt.title(f'Recommended City: {best_city}')\n",
    "        plt.legend(bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.xlabel('PCA Component 1')\n",
    "        plt.ylabel('PCA Component 2')\n",
    "        plt.grid(True)\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    return best_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended city: minneapolis,mn\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.13/lib/python3.13/site-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "user_input = {\n",
    "    'Bedroom': 4,\n",
    "    'Bathroom': 4,\n",
    "    'Area': 1500,\n",
    "    '2022 Median Income': 100000,\n",
    "    'Temperature': 'Cold',   # Now passed as text\n",
    "    'Population': 1000000     # New: user estimates size of city they want\n",
    "}\n",
    "\n",
    "best_city = recommend_city(user_input, visualize=True)\n",
    "print(f\"Recommended city: {best_city}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second Trial: KNN completely ignores missing features and only matches provided columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recommend_city(user_input, visualize=True):\n",
    "    temp_mapping = {'Cold': 0, 'Medium': 1, 'Hot': 2}\n",
    "\n",
    "    # Mapping feature names to their corresponding order\n",
    "    feature_to_index = {name: idx for idx, name in enumerate(feature_cols)}\n",
    "\n",
    "    # Find which features were provided\n",
    "    provided_features = []\n",
    "    user_feature_values = []\n",
    "\n",
    "    for feature in feature_cols:\n",
    "        if feature in user_input:\n",
    "            value = user_input[feature]\n",
    "            if feature == 'Temperature':\n",
    "                value = temp_mapping[value]\n",
    "            user_feature_values.append(value)\n",
    "            provided_features.append(feature)\n",
    "\n",
    "    # Get indices of the provided features\n",
    "    provided_indices = [feature_to_index[feat] for feat in provided_features]\n",
    "\n",
    "    # Build training subset\n",
    "    X_subset = X[:, provided_indices]\n",
    "\n",
    "    # Scale user features manually (without touching missing ones)\n",
    "    # Important: We slice the scaler mean and var for only provided columns\n",
    "    scaler_means = scaler.mean_[provided_indices]\n",
    "    scaler_scales = np.sqrt(scaler.var_[provided_indices])\n",
    "\n",
    "    user_features = np.array(user_feature_values).reshape(1, -1)\n",
    "    user_features_scaled = (user_features - scaler_means) / scaler_scales\n",
    "\n",
    "    # Weighted distance\n",
    "    feature_weights_subset = feature_weights[provided_indices]\n",
    "\n",
    "    weighted_diff = (X_subset - user_features_scaled) * feature_weights_subset\n",
    "    distances = np.linalg.norm(weighted_diff, axis=1)\n",
    "\n",
    "    # Find k nearest neighbors\n",
    "    k = 5\n",
    "    top_k_indices = np.argsort(distances)[:k]\n",
    "\n",
    "    neighbor_cities = y.iloc[top_k_indices].values\n",
    "\n",
    "    unique, counts = np.unique(neighbor_cities, return_counts=True)\n",
    "    candidate_cities = [index_to_city[idx] for idx in unique]\n",
    "\n",
    "    candidates_df = merged_df[merged_df['City'].isin(candidate_cities)].copy()\n",
    "\n",
    "    # Score candidates\n",
    "    candidates_df['QualityScore'] = (\n",
    "        candidates_df['AQI%Good'] + candidates_df['WaterQualityVPV']\n",
    "        - candidates_df['Unemployment'] * 2\n",
    "        - candidates_df['2016 Crime Rate'] * 2\n",
    "        - candidates_df['Cost of Living'] * 1.5\n",
    "    )\n",
    "\n",
    "    city_scores = candidates_df.groupby('City')['QualityScore'].mean()\n",
    "\n",
    "    best_city = city_scores.idxmax()\n",
    "\n",
    "    # Visualization (optional)\n",
    "    visualize = False\n",
    "    if visualize:\n",
    "        # Caution: PCA was trained on full 8 features, not subset\n",
    "        # So visualization here would either be approximate or skipped\n",
    "        pass\n",
    "\n",
    "    return best_city"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommended city: trinidad,co\n"
     ]
    }
   ],
   "source": [
    "user_input = {\n",
    "    'Temperature': 'Cold',   # Now passed as text\n",
    "    # New: user estimates size of city they want\n",
    "}\n",
    "\n",
    "best_city = recommend_city(user_input, visualize=True)\n",
    "print(f\"Recommended city: {best_city}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
