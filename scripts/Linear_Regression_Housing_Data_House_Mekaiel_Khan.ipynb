{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Install Packages"
      ],
      "metadata": {
        "id": "hSSh9iaisaHF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install supabase python-dotenv pandas tensorflow scikit-learn joblib"
      ],
      "metadata": {
        "collapsed": true,
        "id": "XtRBySbysWMi",
        "outputId": "bdfb25ca-f7d8-4ef6-9e11-14e0ab9c49d4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting supabase\n",
            "  Downloading supabase-2.15.0-py3-none-any.whl.metadata (11 kB)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.1.0-py3-none-any.whl.metadata (24 kB)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (1.4.2)\n",
            "Collecting gotrue<3.0.0,>=2.11.0 (from supabase)\n",
            "  Downloading gotrue-2.12.0-py3-none-any.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: httpx<0.29,>=0.26 in /usr/local/lib/python3.11/dist-packages (from supabase) (0.28.1)\n",
            "Collecting postgrest<1.1,>0.19 (from supabase)\n",
            "  Downloading postgrest-1.0.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Collecting realtime<2.5.0,>=2.4.0 (from supabase)\n",
            "  Downloading realtime-2.4.2-py3-none-any.whl.metadata (6.6 kB)\n",
            "Collecting storage3<0.12,>=0.10 (from supabase)\n",
            "  Downloading storage3-0.11.3-py3-none-any.whl.metadata (1.8 kB)\n",
            "Collecting supafunc<0.10,>=0.9 (from supabase)\n",
            "  Downloading supafunc-0.9.4-py3-none-any.whl.metadata (1.2 kB)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.0.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.14.1)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: pydantic<3,>=1.10 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.11.3)\n",
            "Requirement already satisfied: pyjwt<3.0.0,>=2.10.1 in /usr/local/lib/python3.11/dist-packages (from gotrue<3.0.0,>=2.11.0->supabase) (2.10.1)\n",
            "Collecting pytest-mock<4.0.0,>=3.14.0 (from gotrue<3.0.0,>=2.11.0->supabase)\n",
            "  Downloading pytest_mock-3.14.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (4.9.0)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (2025.1.31)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (1.0.8)\n",
            "Requirement already satisfied: idna in /usr/local/lib/python3.11/dist-packages (from httpx<0.29,>=0.26->supabase) (3.10)\n",
            "Requirement already satisfied: h11<0.15,>=0.13 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx<0.29,>=0.26->supabase) (0.14.0)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.8)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Collecting deprecation<3.0.0,>=2.1.0 (from postgrest<1.1,>0.19->supabase)\n",
            "  Downloading deprecation-2.1.0-py2.py3-none-any.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.11.14 in /usr/local/lib/python3.11/dist-packages (from realtime<2.5.0,>=2.4.0->supabase) (3.11.15)\n",
            "Collecting websockets<15,>=11 (from realtime<2.5.0,>=2.4.0->supabase)\n",
            "  Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.1)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.3.0)\n",
            "Collecting strenum<0.5.0,>=0.4.15 (from supafunc<0.10,>=0.9->supabase)\n",
            "  Downloading StrEnum-0.4.15-py3-none-any.whl.metadata (5.3 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (1.5.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (6.4.3)\n",
            "Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (0.3.1)\n",
            "Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<4.0.0,>=3.11.14->realtime<2.5.0,>=2.4.0->supabase) (1.19.0)\n",
            "Requirement already satisfied: h2<5,>=3 in /usr/local/lib/python3.11/dist-packages (from httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.1 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (2.33.1)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=1.10->gotrue<3.0.0,>=2.11.0->supabase) (0.4.0)\n",
            "Requirement already satisfied: pytest>=6.2.5 in /usr/local/lib/python3.11/dist-packages (from pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (8.3.5)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio->httpx<0.29,>=0.26->supabase) (1.3.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.18.0)\n",
            "Requirement already satisfied: hyperframe<7,>=6.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (6.1.0)\n",
            "Requirement already satisfied: hpack<5,>=4.1 in /usr/local/lib/python3.11/dist-packages (from h2<5,>=3->httpx[http2]<0.29,>=0.26->gotrue<3.0.0,>=2.11.0->supabase) (4.1.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest>=6.2.5->pytest-mock<4.0.0,>=3.14.0->gotrue<3.0.0,>=2.11.0->supabase) (1.5.0)\n",
            "Downloading supabase-2.15.0-py3-none-any.whl (17 kB)\n",
            "Downloading python_dotenv-1.1.0-py3-none-any.whl (20 kB)\n",
            "Downloading gotrue-2.12.0-py3-none-any.whl (43 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.6/43.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading postgrest-1.0.1-py3-none-any.whl (22 kB)\n",
            "Downloading realtime-2.4.2-py3-none-any.whl (22 kB)\n",
            "Downloading storage3-0.11.3-py3-none-any.whl (17 kB)\n",
            "Downloading supafunc-0.9.4-py3-none-any.whl (7.8 kB)\n",
            "Downloading deprecation-2.1.0-py2.py3-none-any.whl (11 kB)\n",
            "Downloading pytest_mock-3.14.0-py3-none-any.whl (9.9 kB)\n",
            "Downloading StrEnum-0.4.15-py3-none-any.whl (8.9 kB)\n",
            "Downloading websockets-14.2-cp311-cp311-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (169 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m169.9/169.9 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: strenum, websockets, python-dotenv, deprecation, pytest-mock, realtime, supafunc, storage3, postgrest, gotrue, supabase\n",
            "  Attempting uninstall: websockets\n",
            "    Found existing installation: websockets 15.0.1\n",
            "    Uninstalling websockets-15.0.1:\n",
            "      Successfully uninstalled websockets-15.0.1\n",
            "Successfully installed deprecation-2.1.0 gotrue-2.12.0 postgrest-1.0.1 pytest-mock-3.14.0 python-dotenv-1.1.0 realtime-2.4.2 storage3-0.11.3 strenum-0.4.15 supabase-2.15.0 supafunc-0.9.4 websockets-14.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Bring in data"
      ],
      "metadata": {
        "id": "9pnAwq4ZsxeQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "os.environ['SUPABASE_URL'] = 'https://lgcrogvgnqphznuwdopu.supabase.co'\n",
        "os.environ['SUPABASE_KEY'] = 'eyJhbGciOiJIUzI1NiIsInR5cCI6IkpXVCJ9.eyJpc3MiOiJzdXBhYmFzZSIsInJlZiI6ImxnY3JvZ3ZnbnFwaHpudXdkb3B1Iiwicm9sZSI6ImFub24iLCJpYXQiOjE3NDQ0MTQwMzcsImV4cCI6MjA1OTk5MDAzN30.2lozGgOq70UbrCm1_7Y1p38WbCqOMTjQ8Cs_ZSvNhSs'"
      ],
      "metadata": {
        "id": "v3HYrpXNs8Iz"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Import PsiKit Learn"
      ],
      "metadata": {
        "id": "RhPcDOg54oGU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
      ],
      "metadata": {
        "id": "Rz2Dq4tZ4xOY"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database Setup"
      ],
      "metadata": {
        "id": "qIgz1FFwtlJk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%%writefile db.py\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from supabase import create_client, Client\n",
        "import pandas as pd\n",
        "\n",
        "# Load environment variables\n",
        "load_dotenv()\n",
        "\n",
        "SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
        "SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
        "\n",
        "def get_supabase_client() -> Client:\n",
        "    return create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "def fetch_housing_data() -> pd.DataFrame:\n",
        "    client = get_supabase_client()\n",
        "    # Replace \"House\" with your actual table name if different\n",
        "    response = client.table(\"House\").select(\"*\").execute()\n",
        "    data = response.data  # a list of dictionaries\n",
        "    return pd.DataFrame(data)\n",
        "\n"
      ],
      "metadata": {
        "id": "J0EFFUcjtrU1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f510cf07-d9df-4da6-8480-4b554acf9aa7"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing db.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Database link"
      ],
      "metadata": {
        "id": "aZp22fF6uaC4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from supabase import create_client, Client\n",
        "\n",
        "def fetch_housing_data() -> pd.DataFrame:\n",
        "    load_dotenv()  # Make sure your .env file is uploaded or environment variables set\n",
        "    SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
        "    SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
        "\n",
        "    # Create the Supabase client\n",
        "    supabase: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "\n",
        "    # Fetch the data from your table (replace 'housing_data' if your table name differs)\n",
        "    response = supabase.table(\"House\").select(\"*\").execute()\n",
        "    data = response.data  # This should be a list of dictionaries\n",
        "    df = pd.DataFrame(data)\n",
        "    return df\n"
      ],
      "metadata": {
        "id": "LQUwr_JyucoT"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Model Script"
      ],
      "metadata": {
        "id": "qICHF1HUtuD6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "from db import fetch_housing_data  # Make sure db.py is in your PYTHONPATH or same folder structure\n",
        "import joblib\n",
        "\n",
        "def train_model():\n",
        "    # 1. Fetch data from your Supabase database\n",
        "    df = fetch_housing_data()\n",
        "    print(\"Data loaded from Supabase:\")\n",
        "    print(df.head())\n",
        "\n",
        "    # 2. Define input and target features based on your updated columns\n",
        "    # You can adjust the columns if you wish to use additional ones.\n",
        "    input_features = ['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']\n",
        "    target_features = ['QualityOfLifeTotalScore', 'Cost of Living', '2016 Crime Rate']\n",
        "\n",
        "    # Impute missing values with the median for both inputs and targets.\n",
        "    X = df[input_features].fillna(df[input_features].median())\n",
        "    y = df[target_features].fillna(df[target_features].median())\n",
        "\n",
        "    # 3. Scale the input features.\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # 4. Split the data into training and testing sets.\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 5. Build the multioutput regression model with an explicit input layer.\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.Input(shape=(X_train.shape[1],)),\n",
        "        tf.keras.layers.Dense(len(target_features))\n",
        "    ])\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    # 6. Train the model. Using 10% of the training data for validation.\n",
        "    model.fit(X_train, y_train, epochs=50, validation_split=0.1)\n",
        "\n",
        "    # 7. Evaluate the model on the test set.\n",
        "    loss = model.evaluate(X_test, y_test)\n",
        "    print(\"Test loss (MSE):\", loss)\n",
        "\n",
        "    # 8. Save the model and scaler for future prediction use.\n",
        "    model.save(\"trained_model.h5\")\n",
        "    joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()\n",
        "\n"
      ],
      "metadata": {
        "id": "xQ1fNRDXtxFG",
        "outputId": "6d0cae00-9473-4717-c705-b7dafcb494e0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded from Supabase:\n",
            "  State        City  Bedroom  Bathroom    Area  ListedPrice Temperature  \\\n",
            "0    az  phoenix,az      3.0       2.0  1776.0       575000         Hot   \n",
            "1    az  phoenix,az      4.0       2.0  1505.0       375000         Hot   \n",
            "2    az  phoenix,az      3.0       2.0  1670.0       370000         Hot   \n",
            "3    az  phoenix,az      3.0       1.0  1855.0       360000         Hot   \n",
            "4    az  phoenix,az      4.0       3.0  1426.0       342000         Hot   \n",
            "\n",
            "   2022 Population  2016 Crime Rate  Unemployment  ...  Cost of Living  \\\n",
            "0          4551524            0.032          3.46  ...        82847.38   \n",
            "1          4551524            0.032          3.46  ...        82847.38   \n",
            "2          4551524            0.032          3.46  ...        82847.38   \n",
            "3          4551524            0.032          3.46  ...        82847.38   \n",
            "4          4551524            0.032          3.46  ...        82847.38   \n",
            "\n",
            "   AVG C2I  MeanIncome  QualityOfLifeTotalScore  QualityOfLifeQualityOfLife  \\\n",
            "0    105.1       57559                    48.31                          21   \n",
            "1    105.1       57559                    48.31                          21   \n",
            "2    105.1       57559                    48.31                          21   \n",
            "3    105.1       57559                    48.31                          21   \n",
            "4    105.1       57559                    48.31                          21   \n",
            "\n",
            "   QualityOfLifeAffordability  QualityOfLifeEconomy  \\\n",
            "0                          25                    14   \n",
            "1                          25                    14   \n",
            "2                          25                    14   \n",
            "3                          25                    14   \n",
            "4                          25                    14   \n",
            "\n",
            "   QualityOfLifeEducationAndHealth  QualityOfLifeSafety      id  \n",
            "0                               39                   40  287025  \n",
            "1                               39                   40  287026  \n",
            "2                               39                   40  287027  \n",
            "3                               39                   40  287028  \n",
            "4                               39                   40  287029  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "Epoch 1/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 14ms/step - loss: 2287893504.0000 - val_loss: 2287895040.0000\n",
            "Epoch 2/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2287893504.0000 - val_loss: 2287894016.0000\n",
            "Epoch 3/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2287892992.0000 - val_loss: 2287892736.0000\n",
            "Epoch 4/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 9ms/step - loss: 2287891456.0000 - val_loss: 2287891200.0000\n",
            "Epoch 5/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2287888384.0000 - val_loss: 2287889920.0000\n",
            "Epoch 6/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287887360.0000 - val_loss: 2287888896.0000\n",
            "Epoch 7/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2287884288.0000 - val_loss: 2287887616.0000\n",
            "Epoch 8/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287886336.0000 - val_loss: 2287886336.0000\n",
            "Epoch 9/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287882240.0000 - val_loss: 2287884800.0000\n",
            "Epoch 10/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287883264.0000 - val_loss: 2287883776.0000\n",
            "Epoch 11/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287881984.0000 - val_loss: 2287882496.0000\n",
            "Epoch 12/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287881984.0000 - val_loss: 2287881472.0000\n",
            "Epoch 13/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287880704.0000 - val_loss: 2287879680.0000\n",
            "Epoch 14/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287876096.0000 - val_loss: 2287878656.0000\n",
            "Epoch 15/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2287876096.0000 - val_loss: 2287877632.0000\n",
            "Epoch 16/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287876352.0000 - val_loss: 2287876096.0000\n",
            "Epoch 17/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287875584.0000 - val_loss: 2287874816.0000\n",
            "Epoch 18/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287870976.0000 - val_loss: 2287873536.0000\n",
            "Epoch 19/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287873536.0000 - val_loss: 2287872512.0000\n",
            "Epoch 20/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287867648.0000 - val_loss: 2287871232.0000\n",
            "Epoch 21/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287872000.0000 - val_loss: 2287869696.0000\n",
            "Epoch 22/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2287868160.0000 - val_loss: 2287868672.0000\n",
            "Epoch 23/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287866624.0000 - val_loss: 2287867136.0000\n",
            "Epoch 24/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287866624.0000 - val_loss: 2287866112.0000\n",
            "Epoch 25/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287859968.0000 - val_loss: 2287864576.0000\n",
            "Epoch 26/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287862528.0000 - val_loss: 2287863296.0000\n",
            "Epoch 27/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287864832.0000 - val_loss: 2287862272.0000\n",
            "Epoch 28/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287860736.0000 - val_loss: 2287860992.0000\n",
            "Epoch 29/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287852800.0000 - val_loss: 2287859712.0000\n",
            "Epoch 30/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287854592.0000 - val_loss: 2287858432.0000\n",
            "Epoch 31/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287856128.0000 - val_loss: 2287857152.0000\n",
            "Epoch 32/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2287855104.0000 - val_loss: 2287855872.0000\n",
            "Epoch 33/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287857408.0000 - val_loss: 2287854592.0000\n",
            "Epoch 34/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287850240.0000 - val_loss: 2287853056.0000\n",
            "Epoch 35/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287853312.0000 - val_loss: 2287852288.0000\n",
            "Epoch 36/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287847680.0000 - val_loss: 2287850752.0000\n",
            "Epoch 37/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2287844864.0000 - val_loss: 2287849472.0000\n",
            "Epoch 38/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2287846912.0000 - val_loss: 2287848192.0000\n",
            "Epoch 39/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287842560.0000 - val_loss: 2287846912.0000\n",
            "Epoch 40/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287846400.0000 - val_loss: 2287845888.0000\n",
            "Epoch 41/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287844864.0000 - val_loss: 2287844352.0000\n",
            "Epoch 42/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287835136.0000 - val_loss: 2287843328.0000\n",
            "Epoch 43/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287845888.0000 - val_loss: 2287841792.0000\n",
            "Epoch 44/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287839488.0000 - val_loss: 2287840768.0000\n",
            "Epoch 45/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287836672.0000 - val_loss: 2287839488.0000\n",
            "Epoch 46/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287834880.0000 - val_loss: 2287838208.0000\n",
            "Epoch 47/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287836672.0000 - val_loss: 2287836928.0000\n",
            "Epoch 48/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2287835648.0000 - val_loss: 2287835648.0000\n",
            "Epoch 49/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287832832.0000 - val_loss: 2287834624.0000\n",
            "Epoch 50/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2287830272.0000 - val_loss: 2287833088.0000\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2287840256.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss (MSE): 2287843584.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Add-on"
      ],
      "metadata": {
        "id": "cNN2QR1GxsLh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bbdq2CSnxuVZ",
        "outputId": "e4fa79e3-1692-404f-806c-8f9bdf2365f2"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Predictions on Model"
      ],
      "metadata": {
        "id": "_oDdWTFRxbLP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# services/prediction.py\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "import joblib\n",
        "\n",
        "# Load your saved model\n",
        "model = tf.keras.models.load_model(\"trained_model.h5\")\n",
        "scaler = joblib.load(\"scaler.pkl\")\n",
        "\n",
        "def predict_house_metrics(user_input):\n",
        "    \"\"\"\n",
        "    user_input: dict or list that includes\n",
        "    [\n",
        "      Desired House Price,\n",
        "      Income,\n",
        "      # Beds,\n",
        "      # Baths,\n",
        "      Sq. ft,\n",
        "      Desired Population\n",
        "    ]\n",
        "    \"\"\"\n",
        "    # Convert to array (assuming user_input is a dictionary or list)\n",
        "    data = np.array([[\n",
        "      user_input[\"house_price\"],\n",
        "      user_input[\"income\"],\n",
        "      user_input[\"beds\"],\n",
        "      user_input[\"baths\"],\n",
        "      user_input[\"sq_ft\"],\n",
        "      user_input[\"population\"]\n",
        "    ]])\n",
        "    # Scale\n",
        "    scaled_data = scaler.transform(data)\n",
        "    # Predict\n",
        "    prediction = model.predict(scaled_data)\n",
        "    return prediction.tolist()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C7oPQav1xhn7",
        "outputId": "35a2d04d-f7d0-4a6f-a304-c4b25e7f235c"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Find closest match"
      ],
      "metadata": {
        "id": "edo3YCq50_ak"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "def find_closest_match(user_input, df, scaler):\n",
        "    \"\"\"\n",
        "    Given a user_input dictionary, a DataFrame df with the housing records,\n",
        "    and a scaler used for the training data, this function finds the record\n",
        "    in df that is closest to the user's desired features.\n",
        "\n",
        "    Parameters:\n",
        "      - user_input: dict with keys \"house_price\", \"income\", \"beds\",\n",
        "                    \"baths\", \"sq_ft\", \"population\"\n",
        "      - df: DataFrame of housing records containing at least the following columns:\n",
        "            ['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']\n",
        "            plus location columns like \"State\", \"City\".\n",
        "      - scaler: A StandardScaler fitted on the training data.\n",
        "\n",
        "    Returns:\n",
        "      - closest_match: A Pandas Series that represents the record closest to the input.\n",
        "    \"\"\"\n",
        "    # Define the features used for matching (same as used in training)\n",
        "    input_features = ['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']\n",
        "\n",
        "    # Construct the user vector from the dictionary\n",
        "    user_vector = np.array([[user_input[\"house_price\"],\n",
        "                              user_input[\"income\"],\n",
        "                              user_input[\"beds\"],\n",
        "                              user_input[\"baths\"],\n",
        "                              user_input[\"sq_ft\"],\n",
        "                              user_input[\"population\"]]])\n",
        "\n",
        "    # Scale the user input using the same scaler\n",
        "    user_vector_scaled = scaler.transform(user_vector)\n",
        "\n",
        "    # Extract the features from df and handle missing values if needed\n",
        "    data_features = df[input_features].fillna(df[input_features].median())\n",
        "\n",
        "    # Scale all these feature rows\n",
        "    data_features_scaled = scaler.transform(data_features)\n",
        "\n",
        "    # Compute Euclidean distances between the user vector and each row in data_features_scaled\n",
        "    distances = np.linalg.norm(data_features_scaled - user_vector_scaled, axis=1)\n",
        "\n",
        "    # Find the index of the closest match\n",
        "    idx = np.argmin(distances)\n",
        "    closest_match = df.iloc[idx]\n",
        "\n",
        "    return closest_match\n"
      ],
      "metadata": {
        "id": "A3CCjRWR1C9Q"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "User Input and Output"
      ],
      "metadata": {
        "id": "H2VLUTZW1FIo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Load the full dataset from Supabase (or your source)\n",
        "df = fetch_housing_data()  # Make sure this cell runs so df is defined\n",
        "print(\"Data loaded:\")\n",
        "print(df.head())\n",
        "\n",
        "# 1. Collect user input\n",
        "house_price = float(input(\"Enter your desired house price: \"))\n",
        "income = float(input(\"Enter your income: \"))\n",
        "beds = int(input(\"Enter number of beds: \"))\n",
        "baths = int(input(\"Enter number of baths: \"))\n",
        "sq_ft = float(input(\"Enter square footage: \"))\n",
        "population = float(input(\"Enter desired population: \"))\n",
        "\n",
        "user_input = {\n",
        "    \"house_price\": house_price,\n",
        "    \"income\": income,\n",
        "    \"beds\": beds,\n",
        "    \"baths\": baths,\n",
        "    \"sq_ft\": sq_ft,\n",
        "    \"population\": population\n",
        "}\n",
        "\n",
        "# 2. Get model prediction (if needed)\n",
        "prediction = predict_house_metrics(user_input)\n",
        "print(\"Prediction outputs (model's numerical predictions):\", prediction)\n",
        "\n",
        "# 3. Load the full dataset so df is defined\n",
        "df = fetch_housing_data()  # Or load it from a CSV if needed\n",
        "print(\"Full dataset loaded (first 5 rows):\")\n",
        "print(df.head())\n",
        "\n",
        "# 4. Find the closest matching record\n",
        "closest_match = find_closest_match(user_input, df, scaler)\n",
        "\n",
        "# 5. Print out the details from the best match\n",
        "print(\"\\nClosest Matching House Record:\")\n",
        "print(\"State:\", closest_match.get(\"State\", \"N/A\"))\n",
        "print(\"City:\", closest_match.get(\"City\", \"N/A\"))\n",
        "print(\"Listed Price:\", closest_match.get(\"ListedPrice\", \"N/A\"))\n",
        "print(\"Mean Income:\", closest_match.get(\"MeanIncome\", \"N/A\"))\n",
        "print(\"Bedrooms:\", closest_match.get(\"Bedroom\", \"N/A\"))\n",
        "print(\"Bathrooms:\", closest_match.get(\"Bathroom\", \"N/A\"))\n",
        "print(\"Area:\", closest_match.get(\"Area\", \"N/A\"))\n",
        "print(\"2022 Population:\", closest_match.get(\"2022 Population\", \"N/A\"))\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DD6g5KIgz5VA",
        "outputId": "541a2896-72ce-4389-e333-55022b6af56e"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data loaded:\n",
            "  State        City  Bedroom  Bathroom    Area  ListedPrice Temperature  \\\n",
            "0    az  phoenix,az      3.0       2.0  1534.0       407150         Hot   \n",
            "1    az  phoenix,az      2.0       2.0   864.0       317000         Hot   \n",
            "2    az  phoenix,az      4.0       2.0  1092.0       395000         Hot   \n",
            "3    az  phoenix,az      3.0       2.0  1123.0       364900         Hot   \n",
            "4    az  phoenix,az      2.0       1.0   922.0       249900         Hot   \n",
            "\n",
            "   2022 Population  2016 Crime Rate  Unemployment  ...  Cost of Living  \\\n",
            "0          4551524            0.032          3.46  ...        82847.38   \n",
            "1          4551524            0.032          3.46  ...        82847.38   \n",
            "2          4551524            0.032          3.46  ...        82847.38   \n",
            "3          4551524            0.032          3.46  ...        82847.38   \n",
            "4          4551524            0.032          3.46  ...        82847.38   \n",
            "\n",
            "   AVG C2I  MeanIncome  QualityOfLifeTotalScore  QualityOfLifeQualityOfLife  \\\n",
            "0    105.1       57559                    48.31                          21   \n",
            "1    105.1       57559                    48.31                          21   \n",
            "2    105.1       57559                    48.31                          21   \n",
            "3    105.1       57559                    48.31                          21   \n",
            "4    105.1       57559                    48.31                          21   \n",
            "\n",
            "   QualityOfLifeAffordability  QualityOfLifeEconomy  \\\n",
            "0                          25                    14   \n",
            "1                          25                    14   \n",
            "2                          25                    14   \n",
            "3                          25                    14   \n",
            "4                          25                    14   \n",
            "\n",
            "   QualityOfLifeEducationAndHealth  QualityOfLifeSafety      id  \n",
            "0                               39                   40  287665  \n",
            "1                               39                   40  287666  \n",
            "2                               39                   40  287667  \n",
            "3                               39                   40  287668  \n",
            "4                               39                   40  287669  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "Enter your desired house price: 1\n",
            "Enter your income: 1\n",
            "Enter number of beds: 1\n",
            "Enter number of baths: 1\n",
            "Enter square footage: 1\n",
            "Enter desired population: 1\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 51ms/step\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Prediction outputs (model's numerical predictions): [[2687318.0, -925570.75, -1607212.5]]\n",
            "Full dataset loaded (first 5 rows):\n",
            "  State        City  Bedroom  Bathroom    Area  ListedPrice Temperature  \\\n",
            "0    az  phoenix,az      3.0       2.0  1166.0       449000         Hot   \n",
            "1    az  phoenix,az      3.0       2.0  1482.0       429900         Hot   \n",
            "2    az  phoenix,az      3.0       2.0  1357.0       379000         Hot   \n",
            "3    az  phoenix,az      5.0       3.0  2931.0       659900         Hot   \n",
            "4    az  phoenix,az      4.0       4.0  3139.0       677000         Hot   \n",
            "\n",
            "   2022 Population  2016 Crime Rate  Unemployment  ...  Cost of Living  \\\n",
            "0          4551524            0.032          3.46  ...        82847.38   \n",
            "1          4551524            0.032          3.46  ...        82847.38   \n",
            "2          4551524            0.032          3.46  ...        82847.38   \n",
            "3          4551524            0.032          3.46  ...        82847.38   \n",
            "4          4551524            0.032          3.46  ...        82847.38   \n",
            "\n",
            "   AVG C2I  MeanIncome  QualityOfLifeTotalScore  QualityOfLifeQualityOfLife  \\\n",
            "0    105.1       57559                    48.31                          21   \n",
            "1    105.1       57559                    48.31                          21   \n",
            "2    105.1       57559                    48.31                          21   \n",
            "3    105.1       57559                    48.31                          21   \n",
            "4    105.1       57559                    48.31                          21   \n",
            "\n",
            "   QualityOfLifeAffordability  QualityOfLifeEconomy  \\\n",
            "0                          25                    14   \n",
            "1                          25                    14   \n",
            "2                          25                    14   \n",
            "3                          25                    14   \n",
            "4                          25                    14   \n",
            "\n",
            "   QualityOfLifeEducationAndHealth  QualityOfLifeSafety      id  \n",
            "0                               39                   40  288305  \n",
            "1                               39                   40  288306  \n",
            "2                               39                   40  288307  \n",
            "3                               39                   40  288308  \n",
            "4                               39                   40  288309  \n",
            "\n",
            "[5 rows x 22 columns]\n",
            "\n",
            "Closest Matching House Record:\n",
            "State: az\n",
            "City: tucson,az\n",
            "Listed Price: 189000\n",
            "Mean Income: 56039\n",
            "Bedrooms: nan\n",
            "Bathrooms: nan\n",
            "Area: nan\n",
            "2022 Population: 1057597\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/utils/validation.py:2739: UserWarning: X does not have valid feature names, but StandardScaler was fitted with feature names\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Evaluate Model"
      ],
      "metadata": {
        "id": "Jtd9pPrH6VMd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "import os\n",
        "import pandas as pd\n",
        "from dotenv import load_dotenv\n",
        "from supabase import create_client, Client\n",
        "\n",
        "def fetch_housing_data() -> pd.DataFrame:\n",
        "    load_dotenv()  # Make sure your environment variables are set\n",
        "    SUPABASE_URL = os.getenv(\"SUPABASE_URL\")\n",
        "    SUPABASE_KEY = os.getenv(\"SUPABASE_KEY\")\n",
        "    client: Client = create_client(SUPABASE_URL, SUPABASE_KEY)\n",
        "    response = client.table(\"House\").select(\"*\").execute()\n",
        "    data = response.data\n",
        "    return pd.DataFrame(data)\n",
        "\n",
        "\n",
        "# Import additional metrics from scikit-learn\n",
        "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
        "\n",
        "# scripts/train_model.py\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "import tensorflow as tf\n",
        "\n",
        "def train_model():\n",
        "    # 1. Fetch data\n",
        "    df = fetch_housing_data()\n",
        "\n",
        "    # 2. Define features/targets\n",
        "    input_features = ['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']\n",
        "    target_features = ['QualityOfLifeTotalScore', 'Cost of Living', '2016 Crime Rate']\n",
        "\n",
        "    X = df[input_features].fillna(df[input_features].median())\n",
        "    y = df[target_features].fillna(df[target_features].median())\n",
        "\n",
        "    # 3. Scale\n",
        "    scaler = StandardScaler()\n",
        "    X_scaled = scaler.fit_transform(X)\n",
        "\n",
        "    # 4. Split\n",
        "    X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "    # 5. Build model\n",
        "    model = tf.keras.Sequential([\n",
        "        tf.keras.layers.Dense(len(target_features), input_shape=(X_train.shape[1],))\n",
        "    ])\n",
        "\n",
        "    # Change: Since there is only one Dense layer (output),\n",
        "    # use a single loss function and remove loss_weights\n",
        "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
        "\n",
        "    # 6. Train\n",
        "    model.fit(X_train, y_train, epochs=50, validation_split=0.1)\n",
        "\n",
        "    # 7. Evaluate\n",
        "    loss = model.evaluate(X_test, y_test)\n",
        "    print(\"Test loss:\", loss)\n",
        "\n",
        "    # 8. Save model & scaler if needed\n",
        "    model.save(\"trained_model.h5\")\n",
        "    # Optionally pickle the scaler for predictions\n",
        "    import joblib\n",
        "    joblib.dump(scaler, \"scaler.pkl\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    train_model()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ccc35a09-0bc7-40e8-bde6-1811f378f5c1",
        "id": "UohCPn7e6Snb",
        "collapsed": true
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/50\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/keras/src/layers/core/dense.py:87: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
            "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 12ms/step - loss: 2283911936.0000 - val_loss: 2272470016.0000\n",
            "Epoch 2/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2284296448.0000 - val_loss: 2272468736.0000\n",
            "Epoch 3/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2281052928.0000 - val_loss: 2272467456.0000\n",
            "Epoch 4/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2282250240.0000 - val_loss: 2272466432.0000\n",
            "Epoch 5/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2283545344.0000 - val_loss: 2272464896.0000\n",
            "Epoch 6/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2285220096.0000 - val_loss: 2272463872.0000\n",
            "Epoch 7/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2282976512.0000 - val_loss: 2272462592.0000\n",
            "Epoch 8/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2283671040.0000 - val_loss: 2272461312.0000\n",
            "Epoch 9/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2285305344.0000 - val_loss: 2272460032.0000\n",
            "Epoch 10/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2283525376.0000 - val_loss: 2272458752.0000\n",
            "Epoch 11/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2280108032.0000 - val_loss: 2272457472.0000\n",
            "Epoch 12/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2285374720.0000 - val_loss: 2272456192.0000\n",
            "Epoch 13/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2284717312.0000 - val_loss: 2272455168.0000\n",
            "Epoch 14/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2284583936.0000 - val_loss: 2272453888.0000\n",
            "Epoch 15/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2284564480.0000 - val_loss: 2272452864.0000\n",
            "Epoch 16/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2284074496.0000 - val_loss: 2272451328.0000\n",
            "Epoch 17/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2281880832.0000 - val_loss: 2272450048.0000\n",
            "Epoch 18/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2283115264.0000 - val_loss: 2272449024.0000\n",
            "Epoch 19/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2283387904.0000 - val_loss: 2272447744.0000\n",
            "Epoch 20/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2282793472.0000 - val_loss: 2272446464.0000\n",
            "Epoch 21/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2283825408.0000 - val_loss: 2272445184.0000\n",
            "Epoch 22/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2284058624.0000 - val_loss: 2272443904.0000\n",
            "Epoch 23/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2284195328.0000 - val_loss: 2272442880.0000\n",
            "Epoch 24/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2282483712.0000 - val_loss: 2272441344.0000\n",
            "Epoch 25/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2283397376.0000 - val_loss: 2272440320.0000\n",
            "Epoch 26/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2283389952.0000 - val_loss: 2272439040.0000\n",
            "Epoch 27/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2282471680.0000 - val_loss: 2272437760.0000\n",
            "Epoch 28/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2281078272.0000 - val_loss: 2272436736.0000\n",
            "Epoch 29/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2285846784.0000 - val_loss: 2272435456.0000\n",
            "Epoch 30/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2283875072.0000 - val_loss: 2272433920.0000\n",
            "Epoch 31/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2284592128.0000 - val_loss: 2272432896.0000\n",
            "Epoch 32/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2282972160.0000 - val_loss: 2272431616.0000\n",
            "Epoch 33/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2282981120.0000 - val_loss: 2272430336.0000\n",
            "Epoch 34/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2283268864.0000 - val_loss: 2272429056.0000\n",
            "Epoch 35/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2280325888.0000 - val_loss: 2272428032.0000\n",
            "Epoch 36/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2281965824.0000 - val_loss: 2272426752.0000\n",
            "Epoch 37/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2284344320.0000 - val_loss: 2272425472.0000\n",
            "Epoch 38/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 8ms/step - loss: 2284580352.0000 - val_loss: 2272424192.0000\n",
            "Epoch 39/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2277004544.0000 - val_loss: 2272423168.0000\n",
            "Epoch 40/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2285096704.0000 - val_loss: 2272421632.0000\n",
            "Epoch 41/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2284252928.0000 - val_loss: 2272420608.0000\n",
            "Epoch 42/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2281448192.0000 - val_loss: 2272419328.0000\n",
            "Epoch 43/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 6ms/step - loss: 2284017920.0000 - val_loss: 2272418048.0000\n",
            "Epoch 44/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2284061696.0000 - val_loss: 2272417024.0000\n",
            "Epoch 45/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 7ms/step - loss: 2285039616.0000 - val_loss: 2272415744.0000\n",
            "Epoch 46/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2284780800.0000 - val_loss: 2272414208.0000\n",
            "Epoch 47/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2281039872.0000 - val_loss: 2272413184.0000\n",
            "Epoch 48/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2282498560.0000 - val_loss: 2272412160.0000\n",
            "Epoch 49/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2281352448.0000 - val_loss: 2272410368.0000\n",
            "Epoch 50/50\n",
            "\u001b[1m23/23\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 4ms/step - loss: 2283582720.0000 - val_loss: 2272409344.0000\n",
            "\u001b[1m7/7\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 5ms/step - loss: 2287836416.0000 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test loss: 2287834624.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Weight Calc"
      ],
      "metadata": {
        "id": "cz5yagphKYW8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", message=\"X does not have valid feature names\")\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "def find_closest_match(user_input, df, scaler, weight_vector=None):\n",
        "    \"\"\"\n",
        "    Finds the closest matching record for a given user_input.\n",
        "\n",
        "    Parameters:\n",
        "      - user_input: dict with keys \"house_price\", \"income\", \"beds\",\n",
        "                    \"baths\", \"sq_ft\", \"population\"\n",
        "      - df: DataFrame of housing records that contains at least the following columns:\n",
        "            ['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']\n",
        "            plus location columns like \"State\" or \"City\".\n",
        "      - scaler: A fitted StandardScaler used to transform the numeric input features.\n",
        "      - weight_vector (optional): Array-like weights for each feature. If provided,\n",
        "                                  it multiplies the squared difference for that feature.\n",
        "\n",
        "    Returns:\n",
        "      - closest_match: A Pandas Series corresponding to the record with the smallest weighted distance.\n",
        "      - distance: The computed weighted distance.\n",
        "    \"\"\"\n",
        "    # Input features (same order as used in scaling/training)\n",
        "    input_features = ['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']\n",
        "\n",
        "    # Construct the user feature vector as a 2D array (one row)\n",
        "    user_vector = np.array([[user_input[\"house_price\"],\n",
        "                             user_input[\"income\"],\n",
        "                             user_input[\"beds\"],\n",
        "                             user_input[\"baths\"],\n",
        "                             user_input[\"sq_ft\"],\n",
        "                             user_input[\"population\"]]])\n",
        "\n",
        "    # Scale the user input using the same scaler\n",
        "    user_vector_scaled = scaler.transform(user_vector)\n",
        "\n",
        "    # Extract corresponding features from the dataset and fill missing values\n",
        "    data_features = df[input_features].fillna(df[input_features].median())\n",
        "    data_features_scaled = scaler.transform(data_features)\n",
        "\n",
        "    if weight_vector is not None:\n",
        "        weight_vector = np.array(weight_vector)  # Ensure it's a NumPy array\n",
        "        # Compute the weighted squared differences\n",
        "        differences = data_features_scaled - user_vector_scaled  # shape: (n_samples, n_features)\n",
        "        weighted_squared_diff = (differences ** 2) * weight_vector\n",
        "        distances = np.sqrt(np.sum(weighted_squared_diff, axis=1))\n",
        "    else:\n",
        "        # Standard Euclidean distance\n",
        "        distances = np.linalg.norm(data_features_scaled - user_vector_scaled, axis=1)\n",
        "\n",
        "    # Find the index of the closest match\n",
        "    idx = np.argmin(distances)\n",
        "    closest_match = df.iloc[idx]\n",
        "    return closest_match, distances[idx]\n",
        "\n",
        "def evaluate_weight_vector(weight_vector, df, scaler):\n",
        "    \"\"\"\n",
        "    Evaluates a given weight_vector by treating every row in df as a query.\n",
        "    For each query, find the closest matching record (excluding itself)\n",
        "    and compute the MSE between the target values of the query and the match.\n",
        "\n",
        "    Parameters:\n",
        "      - weight_vector: Array-like list of weights for the input features.\n",
        "      - df: Full DataFrame containing both input features and target features.\n",
        "      - scaler: The fitted StandardScaler used on the input features.\n",
        "\n",
        "    Returns:\n",
        "      - overall_mse: The average Mean Squared Error computed over all queries.\n",
        "    \"\"\"\n",
        "    input_features = ['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']\n",
        "    target_features = ['QualityOfLifeTotalScore', 'Cost of Living', '2016 Crime Rate']\n",
        "\n",
        "    errors = []\n",
        "    for idx, row in df.iterrows():\n",
        "        # Use the current row as user input\n",
        "        user_input = {\n",
        "            \"house_price\": row[\"ListedPrice\"],\n",
        "            \"income\": row[\"MeanIncome\"],\n",
        "            \"beds\": row[\"Bedroom\"],\n",
        "            \"baths\": row[\"Bathroom\"],\n",
        "            \"sq_ft\": row[\"Area\"],\n",
        "            \"population\": row[\"2022 Population\"]\n",
        "        }\n",
        "\n",
        "        # Find the closest match using the provided weight_vector\n",
        "        match, distance = find_closest_match(user_input, df, scaler, weight_vector=weight_vector)\n",
        "        # Skip if the best match is the same record\n",
        "        if match.name == idx:\n",
        "            continue\n",
        "\n",
        "        # Compute the MSE between the target features of the query and its match\n",
        "        true_target = row[target_features].values.astype(float)\n",
        "        matched_target = match[target_features].values.astype(float)\n",
        "        error = np.mean((true_target - matched_target) ** 2)\n",
        "        errors.append(error)\n",
        "\n",
        "    overall_mse = np.mean(errors)\n",
        "    return overall_mse\n",
        "\n",
        "#############################################\n",
        "# Example usage:\n",
        "#############################################\n",
        "\n",
        "# Assume you have already loaded your full dataset df and your fitted scaler.\n",
        "# For example:\n",
        "# df = fetch_housing_data()   <-- Your function to load the data\n",
        "# scaler = ...                <-- Your StandardScaler fitted on the input features\n",
        "\n",
        "# For demonstration, here is a dummy setup:\n",
        "# (Uncomment and replace with your actual data fetching logic)\n",
        "# df = pd.read_csv(\"your_housing_data.csv\")\n",
        "# from sklearn.preprocessing import StandardScaler\n",
        "# scaler = StandardScaler()\n",
        "# df_features = df[['ListedPrice', 'MeanIncome', 'Bedroom', 'Bathroom', 'Area', '2022 Population']].fillna(df.median())\n",
        "# scaler.fit(df_features)\n",
        "\n",
        "# Define an arbitrary weight vector for the 6 input features.\n",
        "# You can modify these numbers to adjust the relative importance.\n",
        "weight_vector = [1.8, 1.0, 1.0, 1.0, 1.0, 1.0]\n",
        "\n",
        "# Evaluate the matching MSE for this weight vector.\n",
        "overall_mse = evaluate_weight_vector(weight_vector, df, scaler)\n",
        "print(\"Overall Matching MSE for weight_vector {}: {:.2f}\".format(weight_vector, overall_mse))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHVfHlhfKaR9",
        "outputId": "f4071cf2-36d6-43c7-a738-97205f91c6f1"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Overall Matching MSE for weight_vector [1.8, 1.0, 1.0, 1.0, 1.0, 1.0]: 834832.34\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}